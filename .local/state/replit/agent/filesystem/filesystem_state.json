{"file_contents":{"deploy-aws-ec2.sh":{"content":"#!/bin/bash\n\n# Business Intelligence Platform - AWS EC2 Deployment Script\n# Run this script on your EC2 instance to deploy the platform\n\nset -e\n\necho \"üöÄ Business Intelligence Platform - AWS EC2 Deployment\"\necho \"======================================================\"\n\n# Update system\necho \"üì¶ Updating system packages...\"\nsudo apt-get update -y\nsudo apt-get upgrade -y\n\n# Install Docker if not present\nif ! command -v docker &> /dev/null; then\n    echo \"üê≥ Installing Docker...\"\n    curl -fsSL https://get.docker.com -o get-docker.sh\n    sudo sh get-docker.sh\n    sudo usermod -aG docker $USER\n    rm get-docker.sh\nfi\n\n# Install Docker Compose if not present\nif ! command -v docker-compose &> /dev/null; then\n    echo \"üê≥ Installing Docker Compose...\"\n    sudo curl -L \"https://github.com/docker/compose/releases/download/v2.20.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n    sudo chmod +x /usr/local/bin/docker-compose\nfi\n\n# Install Python and pip if not present (for local development)\nif ! command -v python3 &> /dev/null; then\n    echo \"üêç Installing Python...\"\n    sudo apt-get install -y python3 python3-pip python3-venv\nfi\n\n# Install system dependencies\necho \"üìö Installing system dependencies...\"\nsudo apt-get install -y curl git htop nginx certbot python3-certbot-nginx\n\n# Create application directory\nAPP_DIR=\"/opt/bi-platform\"\nsudo mkdir -p $APP_DIR\nsudo chown $USER:$USER $APP_DIR\n\n# Copy application files (assuming they're in current directory)\necho \"üìÅ Setting up application files...\"\ncp -r . $APP_DIR/\ncd $APP_DIR\n\n# Create data and logs directories\nmkdir -p data logs .streamlit\n\n# Set up environment file template\nif [ ! -f .env ]; then\n    echo \"üîë Creating environment file template...\"\n    cat > .env << 'EOL'\n# Business Intelligence Platform Configuration\n# Copy this file to .env and fill in your API keys\n\n# LLM API Keys (at least one is required)\nGROQ_API_KEY=your_groq_api_key_here\nDEEPSEEK_API_KEY=your_deepseek_api_key_here\nOPENAI_API_KEY=your_openai_api_key_here\n\n# Application Settings\nTHREAD_ID=bi-platform-aws\nUSER_AGENT=BI-Platform/1.0\n\n# Telemetry (optional)\nOTEL_EXPORTER_OTLP_ENDPOINT=\nOTEL_EXPORTER_OTLP_HEADERS=\n\n# Database (optional - uses SQLite by default)\nDATABASE_URL=sqlite:///data/bi_platform.db\nEOL\n    echo \"‚ö†Ô∏è  Please edit .env file and add your API keys!\"\nfi\n\n# Create Streamlit configuration\necho \"‚öôÔ∏è Configuring Streamlit...\"\ncat > .streamlit/config.toml << 'EOL'\n[server]\nheadless = true\naddress = \"0.0.0.0\"\nport = 5000\nenableCORS = false\nenableXsrfProtection = false\n\n[browser]\ngatherUsageStats = false\n\n[theme]\nprimaryColor = \"#FF6B35\"\nbackgroundColor = \"#FFFFFF\"\nsecondaryBackgroundColor = \"#F0F2F6\"\ntextColor = \"#262730\"\nEOL\n\n# Build Docker image\necho \"üèóÔ∏è Building Docker image...\"\ndocker build -t bi-platform .\n\n# Set up nginx reverse proxy\necho \"üåê Setting up Nginx reverse proxy...\"\nsudo tee /etc/nginx/sites-available/bi-platform << EOL\nserver {\n    listen 80;\n    server_name your-domain.com;  # Replace with your domain\n\n    location / {\n        proxy_pass http://localhost:5000;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade \\$http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n        proxy_set_header Host \\$host;\n        proxy_set_header X-Real-IP \\$remote_addr;\n        proxy_set_header X-Forwarded-For \\$proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto \\$scheme;\n        proxy_cache_bypass \\$http_upgrade;\n        proxy_read_timeout 86400;\n    }\n\n    # Health check endpoint\n    location /health {\n        access_log off;\n        return 200 \"healthy\\n\";\n        add_header Content-Type text/plain;\n    }\n}\nEOL\n\n# Enable the site\nsudo ln -sf /etc/nginx/sites-available/bi-platform /etc/nginx/sites-enabled/\nsudo nginx -t\nsudo systemctl enable nginx\nsudo systemctl restart nginx\n\n# Create systemd service for the platform\necho \"üîß Creating systemd service...\"\nsudo tee /etc/systemd/system/bi-platform.service << EOL\n[Unit]\nDescription=Business Intelligence Platform\nAfter=docker.service\nRequires=docker.service\n\n[Service]\nType=oneshot\nRemainAfterExit=yes\nWorkingDirectory=$APP_DIR\nExecStart=/usr/local/bin/docker-compose up -d\nExecStop=/usr/local/bin/docker-compose down\nTimeoutStartSec=0\n\n[Install]\nWantedBy=multi-user.target\nEOL\n\n# Enable and start the service\nsudo systemctl daemon-reload\nsudo systemctl enable bi-platform\n\n# Create startup script\necho \"üìú Creating startup script...\"\ncat > start.sh << 'EOL'\n#!/bin/bash\necho \"üöÄ Starting Business Intelligence Platform...\"\ndocker-compose up -d\necho \"‚úÖ Platform is starting up...\"\necho \"üåê Access your platform at: http://$(curl -s http://169.254.169.254/latest/meta-data/public-ipv4):5000\"\nEOL\n\nchmod +x start.sh\n\n# Create monitoring script\ncat > monitor.sh << 'EOL'\n#!/bin/bash\necho \"üìä Business Intelligence Platform Status\"\necho \"=======================================\"\necho \"Docker Status:\"\ndocker-compose ps\necho \"\"\necho \"Resource Usage:\"\ndocker stats --no-stream\necho \"\"\necho \"Recent Logs:\"\ndocker-compose logs --tail=50\nEOL\n\nchmod +x monitor.sh\n\n# Final instructions\necho \"\"\necho \"üéâ Deployment Complete!\"\necho \"======================\"\necho \"\"\necho \"Next steps:\"\necho \"1. Edit .env file with your API keys: nano .env\"\necho \"2. Start the platform: ./start.sh\"\necho \"3. Monitor status: ./monitor.sh\"\necho \"4. Access via: http://$(curl -s http://169.254.169.254/latest/meta-data/public-ipv4 2>/dev/null || echo 'YOUR_EC2_IP'):5000\"\necho \"\"\necho \"For SSL/HTTPS setup (recommended for production):\"\necho \"1. Update domain in /etc/nginx/sites-available/bi-platform\"\necho \"2. Run: sudo certbot --nginx -d your-domain.com\"\necho \"\"\necho \"To start automatically on boot:\"\necho \"sudo systemctl start bi-platform\"\necho \"\"\necho \"Troubleshooting:\"\necho \"- Check logs: docker-compose logs -f\"\necho \"- Restart: docker-compose restart\"\necho \"- Check nginx: sudo nginx -t && sudo systemctl status nginx\"","size_bytes":5942},"main.py":{"content":"\"\"\"\nüöÄ Business Intelligence Platform - Enhanced & Redesigned\nMulti-LLM system with advanced analytics, real-time data, and interactive UI\n\"\"\"\n\nimport streamlit as st\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom datetime import datetime, timedelta\nimport json\nimport time\nimport logging\nimport traceback\nfrom typing import Dict, List, Any, Optional\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\n# Configure logging first\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('business_intelligence.log'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# Import BI components\ntry:\n    from bi_core.graph import build_business_graph\n    from bi_core.settings import settings\n    from bi_core.business_workflows import BusinessIntelligenceWorkflow\n    from bi_core.telemetry import setup_telemetry, get_logger\n    from bi_core.memory_optimizer import memory_optimizer\n    from bi_core.anti_hallucination import verify_analysis_reliability\n    from bi_core.llm_factory import llm_factory\n    logger.info(\"‚úÖ All BI core components imported successfully\")\nexcept ImportError as e:\n    logger.error(f\"‚ùå Failed to import BI components: {e}\")\n    st.error(f\"Configuration Error: {e}\")\n\n# Initialize telemetry\ntry:\n    setup_telemetry()\n    bi_logger = get_logger(__name__)\n    logger.info(\"‚úÖ Telemetry system initialized\")\nexcept Exception as e:\n    logger.warning(f\"‚ö†Ô∏è Telemetry initialization failed: {e}\")\n    bi_logger = logger\n\n# Page configuration with modern styling\nst.set_page_config(\n    page_title=\"üöÄ Business Intelligence Platform\",\n    page_icon=\"üìä\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\",\n    menu_items={\n        'Get Help': 'https://github.com/your-repo/business-intelligence',\n        'Report a bug': 'https://github.com/your-repo/business-intelligence/issues',\n        'About': \"Advanced Business Intelligence Platform powered by multiple LLMs\"\n    }\n)\n\n# Custom CSS for better UI\nst.markdown(\"\"\"\n<style>\n    .main-header {\n        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);\n        padding: 1rem;\n        border-radius: 10px;\n        color: white;\n        text-align: center;\n        margin-bottom: 2rem;\n    }\n    .metric-card {\n        background: #f8f9fa;\n        padding: 1rem;\n        border-radius: 8px;\n        border-left: 4px solid #667eea;\n        margin: 0.5rem 0;\n    }\n    .analysis-container {\n        background: white;\n        padding: 1.5rem;\n        border-radius: 10px;\n        box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n        margin: 1rem 0;\n    }\n    .stButton > button {\n        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);\n        color: white;\n        border: none;\n        border-radius: 8px;\n        padding: 0.5rem 1rem;\n        font-weight: 600;\n    }\n    .status-healthy { color: #28a745; }\n    .status-warning { color: #ffc107; }\n    .status-error { color: #dc3545; }\n</style>\n\"\"\", unsafe_allow_html=True)\n\n# Initialize session state with better structure\ndef initialize_session_state():\n    \"\"\"Initialize all session state variables\"\"\"\n    defaults = {\n        \"graph\": None,\n        \"cfg\": {\"configurable\": {\"thread_id\": settings.thread_id}},\n        \"workflow\": None,\n        \"analysis_history\": [],\n        \"reliability_reports\": [],\n        \"memory_stats\": [],\n        \"current_analysis\": None,\n        \"system_health\": {\"status\": \"checking\", \"last_check\": None}\n    }\n    \n    for key, default_value in defaults.items():\n        if key not in st.session_state:\n            st.session_state[key] = default_value\n\ninitialize_session_state()\n\n# Initialize core components\n@st.cache_resource\ndef initialize_business_graph():\n    \"\"\"Initialize and cache the business graph\"\"\"\n    try:\n        graph = build_business_graph()\n        logger.info(\"‚úÖ Business intelligence graph initialized\")\n        return graph\n    except Exception as e:\n        logger.error(f\"‚ùå Failed to initialize business graph: {e}\")\n        return None\n\n@st.cache_resource\ndef initialize_workflow():\n    \"\"\"Initialize and cache the workflow\"\"\"\n    try:\n        workflow = BusinessIntelligenceWorkflow()\n        logger.info(\"‚úÖ Business workflow initialized\")\n        return workflow\n    except Exception as e:\n        logger.error(f\"‚ùå Failed to initialize workflow: {e}\")\n        return None\n\n# Load cached components\nif st.session_state.graph is None:\n    st.session_state.graph = initialize_business_graph()\n\nif st.session_state.workflow is None:\n    st.session_state.workflow = initialize_workflow()\n\n# System Health Check\ndef check_system_health():\n    \"\"\"Check the health of all system components\"\"\"\n    health_status = {\n        \"timestamp\": datetime.now(),\n        \"components\": {}\n    }\n    \n    # Check LLM backends\n    try:\n        for backend in [\"groq\", \"ollama\"]:  # Skip deepseek for now as it needs API key\n            try:\n                health_check = llm_factory.health_check(backend)\n                health_status[\"components\"][f\"llm_{backend}\"] = health_check\n            except Exception as e:\n                health_status[\"components\"][f\"llm_{backend}\"] = {\n                    \"status\": \"unhealthy\", \n                    \"error\": str(e)\n                }\n    except Exception as e:\n        health_status[\"components\"][\"llm_system\"] = {\n            \"status\": \"error\", \n            \"error\": str(e)\n        }\n    \n    # Check web scraping capabilities\n    try:\n        import requests\n        response = requests.get(\"https://httpbin.org/status/200\", timeout=5)\n        health_status[\"components\"][\"web_scraping\"] = {\n            \"status\": \"healthy\" if response.status_code == 200 else \"unhealthy\"\n        }\n    except Exception as e:\n        health_status[\"components\"][\"web_scraping\"] = {\n            \"status\": \"unhealthy\", \n            \"error\": str(e)\n        }\n    \n    # Check memory optimizer\n    try:\n        mem_stats = memory_optimizer.get_memory_stats()\n        health_status[\"components\"][\"memory_optimizer\"] = {\n            \"status\": \"healthy\",\n            \"memory_mb\": mem_stats.get(\"rss_mb\", 0),\n            \"cache_size\": mem_stats.get(\"ttl_cache_size\", 0)\n        }\n    except Exception as e:\n        health_status[\"components\"][\"memory_optimizer\"] = {\n            \"status\": \"unhealthy\", \n            \"error\": str(e)\n        }\n    \n    return health_status\n\n# Header with modern design\nst.markdown(\"\"\"\n<div class=\"main-header\">\n    <h1>üöÄ Business Intelligence Platform</h1>\n    <p>Advanced AI-Powered Business Analysis | Multi-LLM Architecture | Real-Time Data</p>\n</div>\n\"\"\", unsafe_allow_html=True)\n\n# Sidebar with enhanced configuration\nwith st.sidebar:\n    st.header(\"üîß Configuration\")\n    \n    # System Health Status\n    with st.expander(\"üè• System Health\", expanded=True):\n        if st.button(\"üîÑ Check System Health\"):\n            with st.spinner(\"Checking system health...\"):\n                health = check_system_health()\n                st.session_state.system_health = health\n        \n        if st.session_state.system_health[\"status\"] != \"checking\":\n            health = st.session_state.system_health\n            \n            for component, status in health[\"components\"].items():\n                if status.get(\"status\") == \"healthy\":\n                    st.markdown(f\"‚úÖ **{component.replace('_', ' ').title()}**: Healthy\")\n                elif status.get(\"status\") == \"unhealthy\":\n                    st.markdown(f\"‚ö†Ô∏è **{component.replace('_', ' ').title()}**: Issues detected\")\n                else:\n                    st.markdown(f\"‚ùå **{component.replace('_', ' ').title()}**: Error\")\n    \n    # LLM Backend Selection\n    st.subheader(\"ü§ñ AI Backend\")\n    backend_options = [\"groq\", \"ollama\"]  # Start with working backends\n    selected_backend = st.selectbox(\n        \"Primary Backend\",\n        backend_options,\n        index=0,\n        help=\"Groq: Fast inference | Ollama: Local processing\"\n    )\n    \n    # Model selection based on backend\n    if selected_backend == \"groq\":\n        model_options = [\"llama-3.3-70b-versatile\", \"mixtral-8x7b-32768\"]\n        selected_model = st.selectbox(\"Model\", model_options)\n        st.info(\"üîë Add GROQ_API_KEY for full functionality\")\n    else:\n        model_options = [\"llama3.2\", \"qwen2.5\"]\n        selected_model = st.selectbox(\"Local Model\", model_options)\n        st.info(\"üè† Using local Ollama server\")\n    \n    # Analysis type\n    st.subheader(\"üìä Analysis Type\")\n    analysis_types = [\n        \"üè¢ Market Research\",\n        \"‚öîÔ∏è Competitive Analysis\", \n        \"üí∞ Investment Screening\",\n        \"üîç Company Intelligence\",\n        \"üìà Trend Analysis\",\n        \"üíπ Financial Analysis\",\n        \"‚ùì Custom Query\"\n    ]\n    analysis_type = st.selectbox(\"Analysis Focus\", analysis_types)\n    clean_analysis_type = analysis_type.split(\" \", 1)[1]  # Remove emoji\n    \n    # Advanced settings\n    with st.expander(\"‚öôÔ∏è Advanced Settings\"):\n        col1, col2 = st.columns(2)\n        with col1:\n            reasoning_enabled = st.checkbox(\"üß† Reasoning Traces\", value=True)\n            reliability_check = st.checkbox(\"üîç Reliability Check\", value=True)\n        with col2:\n            memory_optimization = st.checkbox(\"üíæ Memory Optimization\", value=True)\n            enhanced_search = st.checkbox(\"üîç Enhanced Search\", value=True)\n        \n        max_tokens = st.slider(\"Max Tokens\", 512, 4096, 2048)\n        temperature = st.slider(\"Temperature\", 0.0, 1.0, 0.6, 0.1)\n    \n    # Session Statistics\n    st.markdown(\"---\")\n    st.subheader(\"üìà Session Stats\")\n    \n    col1, col2 = st.columns(2)\n    with col1:\n        st.metric(\"Analyses\", len(st.session_state.analysis_history))\n        st.metric(\"Backend\", selected_backend.upper())\n    \n    with col2:\n        if memory_optimization and st.session_state.system_health.get(\"components\", {}).get(\"memory_optimizer\"):\n            mem_info = st.session_state.system_health[\"components\"][\"memory_optimizer\"]\n            st.metric(\"Memory\", f\"{mem_info.get('memory_mb', 0):.1f} MB\")\n            st.metric(\"Cache\", f\"{mem_info.get('cache_size', 0)}\")\n    \n    # Action buttons\n    st.markdown(\"---\")\n    col1, col2 = st.columns(2)\n    with col1:\n        if st.button(\"üóëÔ∏è Clear History\"):\n            st.session_state.analysis_history = []\n            st.rerun()\n    with col2:\n        if st.button(\"üíæ Export Data\"):\n            export_data = {\n                \"history\": st.session_state.analysis_history,\n                \"timestamp\": datetime.now().isoformat(),\n                \"config\": {\n                    \"backend\": selected_backend,\n                    \"model\": selected_model,\n                    \"analysis_type\": clean_analysis_type\n                }\n            }\n            st.download_button(\n                \"üì• Download\",\n                json.dumps(export_data, indent=2),\n                f\"bi_session_{datetime.now().strftime('%Y%m%d_%H%M')}.json\",\n                \"application/json\"\n            )\n\n# Main Analysis Interface\nst.header(\"üîç Business Analysis Center\")\n\n# Create enhanced input interface\ncol1, col2 = st.columns([3, 1])\n\nwith col1:\n    if clean_analysis_type == \"Custom Query\":\n        query = st.text_area(\n            \"Enter your business question:\",\n            placeholder=\"e.g., Analyze Tesla's competitive position in the EV market and predict future growth opportunities\",\n            height=120,\n            help=\"Ask any business question - our AI will determine the best analysis approach\"\n        )\n    else:\n        # Smart templates based on analysis type\n        templates = {\n            \"Market Research\": \"Research the market size, growth trends, and key players in [INDUSTRY/MARKET]. Include market drivers, challenges, and 3-year outlook.\",\n            \"Competitive Analysis\": \"Perform a comprehensive competitive analysis of [COMPANY] including direct competitors, market positioning, SWOT analysis, and strategic recommendations.\",\n            \"Investment Screening\": \"Evaluate [COMPANY/SECTOR] as an investment opportunity. Analyze financial metrics, growth potential, risks, and provide investment thesis.\",\n            \"Company Intelligence\": \"Gather comprehensive intelligence on [COMPANY] including business model, recent developments, financial performance, and strategic initiatives.\",\n            \"Trend Analysis\": \"Analyze emerging trends in [INDUSTRY/TECHNOLOGY] including adoption rates, market impact, key drivers, and future predictions.\",\n            \"Financial Analysis\": \"Conduct detailed financial analysis of [COMPANY] including revenue trends, profitability, debt analysis, and financial health assessment.\"\n        }\n        \n        query = st.text_area(\n            f\"{clean_analysis_type} Query:\",\n            value=templates.get(clean_analysis_type, \"\"),\n            height=120,\n            help=\"Customize the template or use as-is. Replace [PLACEHOLDERS] with specific companies/industries.\"\n        )\n\nwith col2:\n    st.markdown(\"### üöÄ Quick Actions\")\n    \n    # Enhanced analysis button\n    analysis_ready = bool(query.strip())\n    \n    if st.button(\"üîç **Run Analysis**\", type=\"primary\", disabled=not analysis_ready, use_container_width=True):\n        if st.session_state.graph is None:\n            st.error(\"‚ùå System not properly initialized. Please refresh the page.\")\n        else:\n            # Start analysis\n            st.session_state.current_analysis = {\n                \"query\": query,\n                \"type\": clean_analysis_type,\n                \"backend\": selected_backend,\n                \"model\": selected_model,\n                \"timestamp\": datetime.now(),\n                \"status\": \"running\"\n            }\n            st.rerun()\n    \n    # Additional quick actions\n    if st.button(\"üí° **Get Suggestions**\", use_container_width=True):\n        suggestions = [\n            \"Analyze Apple's position in the smartphone market\",\n            \"Research renewable energy investment opportunities\",\n            \"Compare Netflix vs Disney+ streaming strategies\",\n            \"Evaluate AI startup investment trends in 2024\",\n            \"Analyze Tesla's supply chain challenges\"\n        ]\n        st.info(\"üí° **Quick Ideas:**\\n\" + \"\\n\".join([f\"‚Ä¢ {s}\" for s in suggestions]))\n    \n    if st.button(\"üìä **Market Overview**\", use_container_width=True):\n        st.info(\"üìä **Today's Focus:**\\n‚Ä¢ Tech earnings season\\n‚Ä¢ Energy sector trends\\n‚Ä¢ Inflation impact analysis\\n‚Ä¢ ESG investment shifts\")\n\n# Analysis Execution and Results\nif st.session_state.current_analysis and st.session_state.current_analysis[\"status\"] == \"running\":\n    analysis = st.session_state.current_analysis\n    \n    # Analysis container with better styling\n    st.markdown(f\"\"\"\n    <div class=\"analysis-container\">\n        <h3>üî¨ {analysis['type']} Analysis</h3>\n        <p><strong>Query:</strong> {analysis['query'][:100]}...</p>\n        <p><strong>Backend:</strong> {analysis['backend'].upper()} | <strong>Model:</strong> {analysis['model']}</p>\n    </div>\n    \"\"\", unsafe_allow_html=True)\n    \n    # Progress tracking\n    progress_col1, progress_col2 = st.columns([3, 1])\n    \n    with progress_col1:\n        progress_bar = st.progress(0)\n        status_text = st.empty()\n    \n    with progress_col2:\n        if st.button(\"‚èπÔ∏è Stop Analysis\"):\n            st.session_state.current_analysis[\"status\"] = \"stopped\"\n            st.rerun()\n    \n    # Results containers\n    results_container = st.container()\n    reasoning_container = st.container()\n    sources_container = st.container()\n    \n    try:\n        # Execute the analysis\n        status_text.text(\"üöÄ Initializing analysis engine...\")\n        progress_bar.progress(0.1)\n        \n        messages = [HumanMessage(content=analysis[\"query\"])]\n        \n        status_text.text(\"üîç Processing query and gathering data...\")\n        progress_bar.progress(0.3)\n        \n        # Stream the analysis\n        events = st.session_state.graph.stream(\n            {\"messages\": messages, \"analysis_type\": analysis[\"type\"]},\n            st.session_state.cfg,\n            stream_mode=\"values\"\n        )\n        \n        response_content = \"\"\n        reasoning_content = \"\"\n        sources = []\n        metrics = []\n        tool_logs = []\n        \n        event_count = 0\n        for i, event in enumerate(events):\n            event_count += 1\n            progress_bar.progress(min(0.3 + (event_count * 0.1), 0.9))\n            \n            if \"messages\" in event and event[\"messages\"]:\n                msg = event[\"messages\"][-1]\n                \n                if hasattr(msg, \"content\"):\n                    content = str(msg.content)\n                    \n                    # Handle reasoning traces\n                    if \"<think>\" in content and \"</think>\" in content:\n                        think_start = content.find(\"<think>\") + 7\n                        think_end = content.find(\"</think>\")\n                        reasoning_content = content[think_start:think_end]\n                        response_content = content.replace(f\"<think>{reasoning_content}</think>\", \"\").strip()\n                    else:\n                        response_content = content\n            \n            # Extract sources and tool calls\n            if \"sources\" in event:\n                sources = event[\"sources\"]\n            \n            # Log tool executions\n            if \"tool_calls\" in str(event) or \"function_call\" in str(event):\n                tool_logs.append({\n                    \"timestamp\": datetime.now().isoformat(),\n                    \"event\": str(event)[:200] + \"...\" if len(str(event)) > 200 else str(event)\n                })\n                status_text.text(\"üõ†Ô∏è Using analysis tools to gather data...\")\n            \n            # Update status based on progress\n            if response_content:\n                status_text.text(\"üß† Generating comprehensive analysis...\")\n        \n        progress_bar.progress(0.9)\n        status_text.text(\"üîç Verifying information reliability...\")\n        \n        # Reliability check\n        reliability_report = None\n        if reliability_check and response_content:\n            try:\n                reliability_report = verify_analysis_reliability(\n                    response_content, sources, analysis[\"type\"]\n                )\n                st.session_state.reliability_reports.append(reliability_report)\n            except Exception as e:\n                logger.warning(f\"Reliability check failed: {e}\")\n        \n        # Memory optimization\n        if memory_optimization and len(st.session_state.analysis_history) % 3 == 0:\n            try:\n                memory_optimizer.cleanup_memory()\n            except Exception as e:\n                logger.warning(f\"Memory optimization failed: {e}\")\n        \n        progress_bar.progress(1.0)\n        status_text.text(\"‚úÖ Analysis complete!\")\n        \n        # Update analysis record\n        analysis_record = {\n            **analysis,\n            \"response\": response_content,\n            \"sources\": sources,\n            \"reasoning\": reasoning_content,\n            \"tool_logs\": tool_logs,\n            \"reliability\": reliability_report,\n            \"status\": \"completed\",\n            \"completion_time\": datetime.now()\n        }\n        st.session_state.analysis_history.append(analysis_record)\n        st.session_state.current_analysis[\"status\"] = \"completed\"\n        \n        # Display results with enhanced formatting\n        with results_container:\n            st.markdown(\"### üìã Executive Summary\")\n            \n            if response_content:\n                # Structure the response better\n                sections = response_content.split('\\n\\n')\n                for i, section in enumerate(sections):\n                    if section.strip():\n                        if i == 0:\n                            st.markdown(f\"**{section}**\")\n                        else:\n                            st.markdown(section)\n                \n                # Extract and display key metrics\n                if any(keyword in response_content.lower() for keyword in [\"$\", \"billion\", \"million\", \"percent\", \"%\"]):\n                    st.markdown(\"### üìä Key Metrics\")\n                    \n                    # Simple metric extraction\n                    import re\n                    metric_patterns = [\n                        r'\\$[\\d,.]+ (?:billion|million|thousand)',\n                        r'[\\d.]+% (?:growth|increase|decrease)',\n                        r'[\\d,]+ (?:employees|customers|users)'\n                    ]\n                    \n                    metrics_found = []\n                    for pattern in metric_patterns:\n                        matches = re.findall(pattern, response_content, re.IGNORECASE)\n                        metrics_found.extend(matches)\n                    \n                    if metrics_found:\n                        cols = st.columns(min(len(metrics_found), 4))\n                        for i, metric in enumerate(metrics_found[:4]):\n                            with cols[i]:\n                                st.metric(\"Key Metric\", metric)\n            \n            else:\n                st.warning(\"No analysis content generated. Please try again with a different query.\")\n        \n        # Display reasoning traces\n        if reasoning_content and reasoning_enabled:\n            with reasoning_container:\n                with st.expander(\"üß† AI Reasoning Process\", expanded=False):\n                    st.markdown(\"**Chain of Thought:**\")\n                    st.text(reasoning_content)\n        \n        # Display sources and tools used\n        if sources or tool_logs:\n            with sources_container:\n                col1, col2 = st.columns(2)\n                \n                with col1:\n                    if sources:\n                        st.markdown(\"### üìö Data Sources\")\n                        for i, source in enumerate(sources[:5]):\n                            with st.expander(f\"Source {i+1}: {source.get('title', 'Unknown')[:50]}...\"):\n                                st.write(f\"**URL:** {source.get('url', 'N/A')}\")\n                                st.write(f\"**Relevance:** {source.get('relevance_score', 0):.2f}/1.0\")\n                                if source.get('snippet'):\n                                    st.write(f\"**Preview:** {source['snippet'][:200]}...\")\n                \n                with col2:\n                    if tool_logs:\n                        st.markdown(\"### üõ†Ô∏è Analysis Tools Used\")\n                        with st.expander(f\"{len(tool_logs)} tools executed\"):\n                            for log in tool_logs[-5:]:  # Show last 5 tool calls\n                                st.text(f\"‚ö° {log['timestamp'][:19]}: {log['event'][:100]}...\")\n        \n        # Display reliability report\n        if reliability_report:\n            with st.expander(\"üîç Information Reliability Report\"):\n                confidence = reliability_report.get(\"confidence_metrics\", {}).get(\"overall_confidence\", 0.5)\n                \n                col1, col2, col3 = st.columns(3)\n                with col1:\n                    st.metric(\"Overall Confidence\", f\"{confidence:.1%}\")\n                with col2:\n                    source_count = len(reliability_report.get(\"source_analysis\", []))\n                    st.metric(\"Sources Verified\", source_count)\n                with col3:\n                    consistency = reliability_report.get(\"confidence_metrics\", {}).get(\"consistency_score\", 0.5)\n                    st.metric(\"Consistency Score\", f\"{consistency:.1%}\")\n        \n        # Log successful completion\n        bi_logger.info(f\"Analysis completed successfully: {analysis['type']}\")\n        \n    except Exception as e:\n        error_msg = str(e)\n        st.error(f\"‚ùå Analysis failed: {error_msg}\")\n        logger.error(f\"Analysis failed: {e}\\n{traceback.format_exc()}\")\n        \n        # Update analysis status\n        st.session_state.current_analysis[\"status\"] = \"failed\"\n        st.session_state.current_analysis[\"error\"] = error_msg\n        \n        # Show fallback options\n        st.markdown(\"### üîÑ Troubleshooting\")\n        col1, col2, col3 = st.columns(3)\n        \n        with col1:\n            if st.button(\"üîÑ Retry Analysis\"):\n                st.session_state.current_analysis[\"status\"] = \"running\"\n                st.rerun()\n        \n        with col2:\n            if st.button(\"üè† Use Local Backend\"):\n                st.session_state.current_analysis[\"backend\"] = \"ollama\"\n                st.session_state.current_analysis[\"status\"] = \"running\"\n                st.rerun()\n        \n        with col3:\n            if st.button(\"üí° Simplify Query\"):\n                st.info(\"üí° Try:\\n‚Ä¢ Shorter, more specific questions\\n‚Ä¢ Focus on one company/topic\\n‚Ä¢ Use simpler language\")\n\n# Analysis History\nif st.session_state.analysis_history:\n    st.markdown(\"---\")\n    st.header(\"üìú Analysis History\")\n    \n    # Filter and sort options\n    col1, col2, col3 = st.columns([2, 2, 1])\n    with col1:\n        history_filter = st.selectbox(\"Filter by Type\", [\"All\"] + [a[\"type\"] for a in st.session_state.analysis_history])\n    with col2:\n        sort_order = st.selectbox(\"Sort by\", [\"Most Recent\", \"Oldest First\", \"By Type\"])\n    with col3:\n        show_count = st.selectbox(\"Show\", [5, 10, 20, \"All\"])\n    \n    # Apply filters\n    filtered_history = st.session_state.analysis_history\n    if history_filter != \"All\":\n        filtered_history = [a for a in filtered_history if a[\"type\"] == history_filter]\n    \n    # Apply sorting\n    if sort_order == \"Most Recent\":\n        filtered_history = sorted(filtered_history, key=lambda x: x[\"timestamp\"], reverse=True)\n    elif sort_order == \"Oldest First\":\n        filtered_history = sorted(filtered_history, key=lambda x: x[\"timestamp\"])\n    else:  # By Type\n        filtered_history = sorted(filtered_history, key=lambda x: x[\"type\"])\n    \n    # Apply count limit\n    if show_count != \"All\":\n        filtered_history = filtered_history[:show_count]\n    \n    # Display history\n    for i, record in enumerate(filtered_history):\n        with st.expander(f\"üìä {record['type']} - {record['timestamp'].strftime('%Y-%m-%d %H:%M')}\"):\n            \n            col1, col2 = st.columns([3, 1])\n            \n            with col1:\n                st.write(f\"**Query:** {record['query'][:200]}...\")\n                st.write(f\"**Backend:** {record['backend']} | **Model:** {record['model']}\")\n                \n                if record.get(\"response\"):\n                    st.write(f\"**Summary:** {record['response'][:300]}...\")\n                \n                if record.get(\"sources\"):\n                    st.write(f\"**Sources:** {len(record['sources'])} sources verified\")\n            \n            with col2:\n                st.write(f\"**Status:** {record.get('status', 'unknown').title()}\")\n                if record.get(\"completion_time\"):\n                    duration = record[\"completion_time\"] - record[\"timestamp\"]\n                    st.write(f\"**Duration:** {duration.total_seconds():.1f}s\")\n                \n                # Action buttons\n                if st.button(f\"üîÑ Re-run\", key=f\"rerun_{i}\"):\n                    st.session_state.current_analysis = {\n                        \"query\": record[\"query\"],\n                        \"type\": record[\"type\"],\n                        \"backend\": record[\"backend\"],\n                        \"model\": record[\"model\"],\n                        \"timestamp\": datetime.now(),\n                        \"status\": \"running\"\n                    }\n                    st.rerun()\n\n# Footer with system information\nst.markdown(\"---\")\ncol1, col2, col3, col4 = st.columns(4)\n\nwith col1:\n    st.markdown(\"**üöÄ Platform Status**\")\n    if st.session_state.graph:\n        st.success(\"System Operational\")\n    else:\n        st.error(\"System Issues\")\n\nwith col2:\n    st.markdown(\"**üîß Configuration**\")\n    st.info(f\"Backend: {selected_backend.upper()}\")\n    st.info(f\"Model: {selected_model}\")\n\nwith col3:\n    st.markdown(\"**üìä Session Stats**\")\n    if st.session_state.analysis_history:\n        success_rate = len([a for a in st.session_state.analysis_history if a.get(\"status\") == \"completed\"]) / len(st.session_state.analysis_history)\n        st.metric(\"Success Rate\", f\"{success_rate:.1%}\")\n    else:\n        st.metric(\"Analyses\", \"0\")\n\nwith col4:\n    st.markdown(\"**üíæ System Info**\")\n    if st.session_state.system_health.get(\"components\", {}).get(\"memory_optimizer\"):\n        mem_info = st.session_state.system_health[\"components\"][\"memory_optimizer\"]\n        st.metric(\"Memory Usage\", f\"{mem_info.get('memory_mb', 0):.0f} MB\")\n    else:\n        st.metric(\"Memory\", \"Unknown\")\n\n# Real-time monitoring dashboard (optional)\nif st.checkbox(\"üìà Show Real-time Monitoring Dashboard\", value=False):\n    st.markdown(\"### üìà System Performance Dashboard\")\n    \n    # Create mock real-time data for demonstration\n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"Requests/min\", \"23\", \"+8%\")\n    with col2:\n        st.metric(\"Avg Response Time\", \"2.4s\", \"-0.6s\")\n    with col3:\n        st.metric(\"Success Rate\", \"94.2%\", \"+2.1%\")\n    with col4:\n        st.metric(\"Active Sessions\", \"7\", \"+3\")\n    \n    # Performance chart\n    chart_data = {\n        'Time': [datetime.now() - timedelta(minutes=x) for x in range(10, 0, -1)],\n        'Response_Time': [2.1, 1.8, 2.5, 1.9, 2.4, 2.0, 2.8, 1.7, 2.2, 2.4],\n        'Success_Rate': [92, 95, 89, 96, 94, 97, 91, 98, 93, 94]\n    }\n    \n    fig = go.Figure()\n    fig.add_trace(go.Scatter(\n        x=chart_data['Time'],\n        y=chart_data['Response_Time'],\n        mode='lines+markers',\n        name='Response Time (s)',\n        line=dict(color='#667eea')\n    ))\n    \n    fig.update_layout(\n        title=\"System Performance Trends\",\n        xaxis_title=\"Time\",\n        yaxis_title=\"Response Time (seconds)\",\n        height=300,\n        showlegend=True\n    )\n    \n    st.plotly_chart(fig, use_container_width=True)\n\n# Hidden debug information\nif st.secrets.get(\"debug_mode\", False):\n    with st.expander(\"üîß Debug Information\"):\n        st.json({\n            \"session_state_keys\": list(st.session_state.keys()),\n            \"settings\": {\n                \"llm_backend\": settings.llm_backend,\n                \"thread_id\": settings.thread_id,\n                \"has_groq_key\": bool(settings.groq_api_key),\n                \"has_deepseek_key\": bool(settings.deepseek_api_key)\n            },\n            \"system_health\": st.session_state.system_health\n        })","size_bytes":30644},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"aiohttp>=3.12.15\",\n    \"beautifulsoup4>=4.13.5\",\n    \"cachetools>=6.2.0\",\n    \"duckduckgo-search>=8.1.1\",\n    \"feedparser>=6.0.11\",\n    \"langchain>=0.3.27\",\n    \"langchain-community>=0.3.29\",\n    \"langchain-core>=0.3.75\",\n    \"langchain-groq>=0.3.7\",\n    \"langgraph>=0.6.6\",\n    \"numpy>=2.3.2\",\n    \"opentelemetry-exporter-otlp>=1.36.0\",\n    \"opentelemetry-sdk>=1.36.0\",\n    \"plotly>=6.3.0\",\n    \"psutil>=7.0.0\",\n    \"streamlit>=1.49.1\",\n    \"tenacity>=9.1.2\",\n    \"trafilatura>=2.0.0\",\n]\n\n[[tool.uv.index]]\nexplicit = true\nname = \"pytorch-cpu\"\nurl = \"https://download.pytorch.org/whl/cpu\"\n\n[tool.uv.sources]\nAA-module = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nABlooper = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nAnalysisG = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nAutoRAG = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nBERTeam = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nBxTorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nByaldi = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nCALM-Pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nCOPEX-high-rate-compression-quality-metrics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nCityLearn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nCoCa-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nCoLT5-attention = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nComfyUI-EasyNodes = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nCrawl4AI = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nDALL-E = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nDI-toolkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nDatasetRising = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nDeepCache = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nDeepMatter = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nDraugr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nESRNN = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nEn-transformer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nExpoSeq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nFLAML = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nFSRS-Optimizer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nGANDLF = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nGQLAlchemy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nGhostScan = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nGraKeL = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nHEBO = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nIOPaint = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nISLP = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nInvokeAI = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nJAEN = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nKapoorLabs-Lightning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nLightAutoML = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nLingerGRN = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nMMEdu = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nMRzeroCore = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nModeva = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nNeuralFoil = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nNiMARE = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nNinjaTools = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nOpenHosta = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nOpenNMT-py = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nPOT = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nPVNet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nPaLM-rlhf-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nPepperPepper = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nPiML = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nPoutyne = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nQNCP = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nRAGatouille = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nRareGO = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nRealtimeSTT = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nRelevanceAI-Workflows-Core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nResemblyzer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nScandEval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nSimba-UW-tf-dev = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nSwissArmyTransformer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nTPOT = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nTTS = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nTorchCRF = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nTotalSegmentator = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nUtilsRL = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nWhisperSpeech = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nXAISuite = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\na-unet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\na5dev = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naccelerate = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naccelerated-scan = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naccern-xyme = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nachatbot = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nacids-rave = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nactorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nacvl-utils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nadabelief-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nadam-atan2-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nadan-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nadapters = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nadmin-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nadtoolbox = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nadversarial-robustness-toolbox = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naeiou = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naeon = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nafricanwhisper = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nag-llama-api = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nagentdojo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nagilerl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nai-edge-torch-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nai-parrot = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nai-python = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nai-transform = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nai2-olmo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nai2-olmo-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nai2-tango = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naicmder = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naider-chat = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naider-chat-x = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naif360 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naihwkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naimodelshare = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nairllm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nairtestProject = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nairunner = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naisak = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naislib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naisquared = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naistore = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naithree = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nakasha-terminal = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalibi = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalibi-detect = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalignn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nall-clip = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nallennlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nallennlp-models = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nallennlp-pvt-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nallophant = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nallosaurus = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naloy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalpaca-eval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalphafold2-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalphafold3-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalphamed-federated = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalphawave = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\namazon-braket-pennylane-plugin = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\namazon-photos = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nanemoi-graphs = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nanemoi-models = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nanomalib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\napache-beam = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\napache-tvm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naperturedb = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naphrodite-engine = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naqlm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\narcAGI2024 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\narchisound = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nargbind = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\narize = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\narm-pytorch-utilities = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\narray-api-compat = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\narus = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nassert-llm-tools = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nasteroid = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nasteroid-filterbanks = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nastra-llm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nastrovision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\natomate2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nattacut = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naudio-diffusion-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naudio-encoders-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naudio-separator = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naudiocraft = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naudiolm-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nauralis = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nauraloss = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nauto-gptq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nautoawq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nautoawq-kernels = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\n\"autogluon.multimodal\" = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\n\"autogluon.tabular\" = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\n\"autogluon.timeseries\" = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nautotrain-advanced = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\navdeepfake1m = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naws-fortuna = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nax-platform = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nazureml-automl-dnn-vision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nazureml-contrib-automl-dnn-forecasting = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nazureml-evaluate-mlflow = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nazureml-metrics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nazureml-train-automl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nb2bTools = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbackpack-for-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbalrog-nle = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbatch-face = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbatchalign = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbatchgeneratorsv2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbatchtensor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbbrl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbenchpots = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbent = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbert-score = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbertopic = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbertviz = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbestOf = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbetty-ml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbig-sleep = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbigdl-core-cpp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbigdl-core-npu = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbigdl-llm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbigdl-nano = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\n\"bioimageio.core\" = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbitfount = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbitsandbytes = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbittensor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbittensor-cli = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nblackboxopt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nblanc = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nblindai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbm25-pt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nboltz = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbotorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nboxmot = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbrainchain = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbraindecode = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbrevitas = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbriton = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbrowsergym-visualwebarena = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbuzz-captions = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbyotrack = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbyzerllm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nc4v-py = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncalflops = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncame-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncamel-ai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncamel-tools = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncannai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncaptum = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncarte-ai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncarvekit-colab = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncatalyst = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncausalml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncausalnex = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncausy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncbrkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncca-zoo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncdp-backend = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncellacdc = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncellfinder = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncellpose = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncellxgene-census = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nchattts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nchemprop = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nchgnet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nchitra = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncircuitsvis = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncjm-yolox-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclarinpl-embeddings = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclass-resolver = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclassifier-free-guidance-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclassiq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclassy-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclean-fid = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncleanvision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclip-anytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclip-benchmark = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclip-by-openai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclip-interrogator = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclip-retrieval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncltk = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclu = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclusterops = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncnocr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncnstd = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncoba = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncofi = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncolbert-ai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncolpali-engine = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncompel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncomposabl-ray = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncomposabl-ray-dev = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncomposabl-train = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncomposabl-train-dev = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncomposer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncompressai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncompressed-tensors = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncompressed-tensors-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nconcrete-python = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nconfit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nconformer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncontextualSpellCheck = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncontinual-inference = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncontrolnet-aux = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nconvokit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncoola = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncoqui-tts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncoqui-tts-trainer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncraft-text-detector = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncreme = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncrocodile = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncrowd-kit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncryoSPHERE = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncsle-common = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncsle-system-identification = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nctgan = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncurated-transformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncut-cross-entropy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncvat-sdk = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncybertask = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nd3rlpy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndalle-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndalle2-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndanila-lib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndanling = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndarts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndarwin-py = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndata-gradients = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndatachain = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndataclass-array = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndataeval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndatarobot-drum = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndatarobotx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndatasets = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndatumaro = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndctorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeep-utils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepchecks = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepchem = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepctr-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepecho = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepepochs = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepforest = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeeplabcut = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepmd-kit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepmultilingualpunctuation = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepparse = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeeprobust = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepsparse = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepsparse-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepspeed = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndenoising-diffusion-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndescript-audio-codec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndescript-audiotools = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndetecto = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndetoxify = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndgenerate = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndghs-imgutils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndgl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndialogy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndice-ml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndiffgram = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndiffq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndiffusers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndistilabel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndistrifuser = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndnikit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndocarray = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndoclayout-yolo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndocling-ibm-models = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndocquery = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndomino-code-assist = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndreamsim = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndropblock = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndruida = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndvclive = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ne2-tts-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ne2cnn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ne3nn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\neasyocr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nebtorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\necallisto-ng = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nedsnlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\neffdet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\neinx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\neir-dl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\neis1600 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\neland = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nema-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nembedchain = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nenformer-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nentmax = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nesm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nespaloma-charge = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nespnet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\netils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\netna = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nevadb = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nevalscope = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nevaluate = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nexllamav2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nextractable = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nface-alignment = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfacenet-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfacexlib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfair-esm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfairseq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfairseq2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfairseq2n = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfaker-file = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfarm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfast-bert = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfast-pytorch-kmeans = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfastai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfastcore = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfastestimator-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfasttreeshap = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfedml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfelupe = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfemr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfft-conv-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfickling = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfireworks-ai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflair = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflashrag-dev = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflax = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflexgen = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflgo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflopth = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflowcept = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflytekitplugins-kfpytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflytekitplugins-onnxpytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfmbench = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfocal-frequency-loss = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfoldedtensor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfractal-tasks-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfreegenius = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfreqtrade = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfschat = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfunasr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfunctorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfunlbm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfunsor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngalore-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngarak = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngarf = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngateloop-transformer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngeffnet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngenutility = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngfpgan = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngigagan-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngin-config = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nglasflow = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngliner = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngluonts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngmft = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngoogle-cloud-aiplatform = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngpforecaster = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngpt3discord = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngpytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngrad-cam = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngraph-weather = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngraphistry = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngravitorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngretel-synthetics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngsplat = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nguardrails-ai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nguidance = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngymnasium = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhanlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhappytransformer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhbutils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nheavyball = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhezar = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhf-deepali = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhf-doc-builder = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhigher = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhjxdl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhkkang-utils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhordelib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhpsv2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhuggingface-hub = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhummingbird-ml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhvae-backbone = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhya = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhypothesis-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nibm-metrics-plugin = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nibm-watson-machine-learning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nibm-watsonx-ai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nicetk = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nicevision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\niden = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nidvpackage = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\niglovikov-helper-functions = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nimagededup = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nimagen-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nimaginAIry = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nimg2vec-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nincendio = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninference = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninference-gpu = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninfinity-emb = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninfo-nce-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninfoapps-mlops-sdk = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninstructlab = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninstructlab-dolomite = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninstructlab-eval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninstructlab-sdg = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninstructlab-training = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninvisible-watermark = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\niobm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nipex-llm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\niree-turbine = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nirisml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nirisml-tasks-azure-openai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nirisml-tasks-torchvision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nirisml-tasks-training = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nitem-matching = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nivadomed = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\njaqpotpy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\njina = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\njudo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\njunky = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nk-diffusion = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nk1lib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nk2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkappadata = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkappamodules = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkarbonn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkats = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkbnf = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkedro-datasets = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkeybert = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkeytotext = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkhoj = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkiui = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkonfuzio-sdk = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkornia = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkornia-moons = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkraken = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkwarray = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkwimage = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlabml-nn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlagent = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlaion-clap = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlale = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlama-cleaner = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlancedb = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlangcheck = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlangkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlangroid = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlangtest = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlayoutparser = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nldp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nleafmap = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nleap-ie = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nleibniz = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nleptonai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nletmedoit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlhotse = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlib310 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlibpecos = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlibrec-auto = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlibretranslate = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nliger-kernel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nliger-kernel-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightning-bolts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightning-fabric = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightning-habana = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightning-lite = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightrag = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightweight-gan = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightwood = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlinear-attention-transformer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlinear-operator = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlinformer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlinformer-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nliom-toolkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlion-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlit-nlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlitdata = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlitelama = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlitgpt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllama-index-embeddings-adapter = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllama-index-embeddings-clip = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllama-index-embeddings-instructor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllama-index-llms-huggingface = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllama-index-postprocessor-colbert-rerank = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllm-blender = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllm-foundry = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllm-guard = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllm-rs = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllm2vec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllmcompressor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllmlingua = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllmvm-cli = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlm-eval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlmdeploy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlmms-eval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlocal-attention = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlovely-tensors = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlpips = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlycoris-lora = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmace-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmagic-pdf = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmagicsoup = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmagvit2-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmaite = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmanga-ocr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmanifest-ml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmanipulation = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmarker-pdf = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmatgl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmed-imagetools = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmedaka = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmedcat = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmedmnist = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmegablocks = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmegatron-energon = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmemos = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmeshgpt-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmetatensor-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmflux = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmia-vgg = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmiditok = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nminari = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nminicons = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nml2rt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmlagents = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmlbench-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmlcroissant = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmlpfile = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmlx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmlx-whisper = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmmaction2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmmengine = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmmengine-lite = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmmocr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmmpose = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmmsegmentation = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmodeci-mdf = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmodel2vec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmodelscope = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmodelspec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmonai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmonai-weekly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmonotonic-alignment-search = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmonty = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmosaicml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmosaicml-streaming = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmoshi = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmteb = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmtmtrain = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmulti-quantization = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmyhand = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnGPT-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnaeural-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnapari = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnapatrackmater = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnara-wpe = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnatten = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnbeats-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnebulae = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnemo-toolkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nneptune = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nneptune-client = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnerfacc = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnerfstudio = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnessai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnetcal = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nneural-rag = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nneuralforecast = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nneuralnets = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nneuralprophet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nneuspell = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnevergrad = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnexfort = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnimblephysics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnirtorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnkululeko = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnlptooltest = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnnAudio = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnnodely = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnnsight = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnnunetv2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnoisereduce = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnonebot-plugin-nailongremove = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnowcasting-dataloader = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnowcasting-forecast = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnshtrainer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnuwa-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnvflare = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnvidia-modelopt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nocf-datapipes = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nocnn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nogb = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nohmeow-blurr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nolive-ai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nomlt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nommlx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nonediff = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nonediffx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nonnx2pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nonnx2torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopacus = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopen-clip-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopen-flamingo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopen-interpreter = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenbb-terminal-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenmim = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenparse = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenunmix = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenvino-dev = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenvino-tokenizers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenvino-xai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenwakeword = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopt-einsum-fx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptimum = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptimum-habana = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptimum-intel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptimum-neuron = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptimum-quanto = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptree = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptuna = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptuna-dashboard = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptuna-integration = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noracle-ads = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\norbit-ml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\notx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noutetts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noutlines = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noutlines-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npaddlenlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npai-easycv = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npandasai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npanns-inference = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npatchwork-cli = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npeft = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npegasuspy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npelutils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npenn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nperforatedai-freemium = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nperformer-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npetastorm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npfio = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npgmpy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nphenolrs = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nphobos = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npi-zero-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npinecone-text = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npiq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npix2tex = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npix2text = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npnnx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npolicyengine-us-data = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npolyfuzz = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npomegranate = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npositional-encodings = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nprefigure = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nproduct-key-memory = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nptflops = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nptwt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npulser-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npunctuators = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npy2ls = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyabsa = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\n\"pyannote.audio\" = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyawd = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyclarity = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npycox = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyfemtet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyg-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npygrinder = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyhealth = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyhf = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyiqa = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npykeen = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npykeops = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npylance = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npylineaGT = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npymanopt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npymde = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npypots = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyqlib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyqtorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyro-ppl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npysentimiento = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyserini = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npysr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npythainlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npython-doctr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-fid = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-forecasting = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-ignite = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-kinematics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-lightning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-lightning-bolts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-metric-learning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-model-summary = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-msssim = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-pfn-extras = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-pretrained-bert = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-ranger = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-seed = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-tabnet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-tabular = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-toolbelt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-transformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-transformers-pvt-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-triton-rocm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-warmup = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-wavelets = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch_optimizer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch_revgrad = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorchcv = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorchltr2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyvene = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyvespa = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nqianfan = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nqibo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nqiskit-machine-learning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nqtorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nquanto = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nquick-anomaly-detector = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrastervision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrastervision-pytorch-backend = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrastervision-pytorch-learner = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nray-lightning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrclip = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrealesrgan = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrecbole = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrecommenders = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nredcat = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nreformer-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nregex-sampler = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nreplay-rec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrerankers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nresearch-framework = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nresemble-enhance = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nresnest = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrf-clip = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrf-groundingdino = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrfconv = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrich-logger = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nring-attention-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrltrade-test = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrotary-embedding-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrsp-ml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrust-circuit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ns2fft = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ns3prl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ns3torchconnector = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsaferx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsafetensors = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsagemaker-huggingface-inference-toolkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsagemaker-ssh-helper = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsalesforce-lavis = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsalesforce-merlion = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsamv2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nscib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nscib-metrics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nscvi-tools = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsdmetrics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsecretflow = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsegment-anything-hq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsegment-anything-py = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsegmentation-models-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nself-rewarding-lm-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsemantic-kernel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsemantic-router = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsenselab = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsent2vec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsentence-transformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsequence-model-train = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nserotiny = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsevenn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsglang = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nshap = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsilero-api-server = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsilero-vad = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsilicondiff-npu = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsimclr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsimple-lama-inpainting = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsinabs = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsixdrepnet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nskforecast = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nskorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nskrl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nskt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsktime = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsktmls = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nslangtorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsmartnoise-synth = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsmashed = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsmplx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsmqtk-descriptors = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsmqtk-detection = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsnntorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsnorkel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsnowflake-ml-python = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nso-vits-svc-fork = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsonusai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsony-custom-layers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsotopia = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspacr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspacy-curated-transformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspacy-experimental = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspacy-huggingface-pipelines = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspacy-llm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspacy-transformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspan-marker = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspandrel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspandrel-extra-arches = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsparrow-python = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspatialdata = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspeechbrain = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspeechtokenizer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspikeinterface = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspikingjelly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspotiflow = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspotpython = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspotriver = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsquirrel-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstable-baselines3 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstable-diffusion-sdkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstable-ts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstanford-stk = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstanfordnlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstanza = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstartorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstreamtasks = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstruct-eqtable = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstylegan2-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsupar = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsuper-gradients = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsuper-image = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsuperlinked = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsupervisely = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsurya-ocr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsvdiff-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nswarm-models = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nswarmauri = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nswarms-memory = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nswebench = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsyft = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsympytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsyne-tune = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsynthcity = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nt5 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntab-transformer-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntabpfn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntaming-transformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntaming-transformers-rom1504 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntaskwiz = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntbparse = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntecton = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntensor-parallel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntensorcircuit-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntensordict = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntensordict-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntensorizer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntensorrt-llm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntexify = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntext2text = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntextattack = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntfkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nthepipe-api = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nthinc = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nthingsvision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nthirdai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nthop = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntianshou = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntidy3d = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntimesfm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntimm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntipo-kgen = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntmnt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntoad = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntomesd = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntop2vec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-audiomentations = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-dct = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-delaunay = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-directml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-ema = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-encoding = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-fidelity = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-geometric = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-geopooling = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-harmonics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-kmeans = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-lr-finder = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-max-mem = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-npu = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-optimi = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-optimizer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-ort = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-pitch-shift = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-ppr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-pruning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-snippets = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-stoi = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-struct = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-tensorrt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchani = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchattacks = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchaudio = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchbiggraph = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchcam = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchcde = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchcfm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchcrepe = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchdata = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchdatasets-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchdiffeq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchdyn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchestra = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorcheval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorcheval-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchextractor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchfcpe = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchfun = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchfunc-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchgeo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchgeometry = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchio = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchjpeg = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchlayers-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchmeta = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchmetrics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchmocks = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchpack = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchpippy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchpq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchprofile = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchquantlib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchrec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchrec-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchrec-nightly-cpu = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchrl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchrl-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchscale = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchsde = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchseg = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchserve = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchserve-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchsnapshot-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchsr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchstain = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchsummaryX = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchtext = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchtnt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchtnt-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchtyping = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchutil = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchvinecopulib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchvision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchviz = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchx-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchxrayvision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntotalspineseg = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntracebloc-package-dev = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntrainer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntransformer-engine = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntransformer-lens = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntransformer-smaller-training-vocab = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntransformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntransformers-domain-adaptation = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntransfusion-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntransparent-background = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntreescope = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntrolo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntsai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntslearn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nttspod = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntxtai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntyro = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nu8darts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nuhg = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nuitestrunner-syberos = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nultimate-rvc = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nultralytics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nultralytics-thop = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunav = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunbabel-comet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunderthesea = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunfoldNd = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunimernet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunitorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunitxt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunsloth = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunsloth-zoo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunstructured = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunstructured-inference = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nutilsd = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nv-diffusion-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvIQA = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvectice = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvector-quantize-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvectorhub-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nversatile-audio-upscaler = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvertexai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvesin = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvgg-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvideo-representations-extractor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nviser = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvision-datasets = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvisionmetrics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvisu3d = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvit-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nviturka-nn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvllm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvllm-flash-attn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvocos = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvollseg = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvtorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwavmark = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwdoc = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwhisper-live = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwhisper-timestamped = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwhisperx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwilds = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwordllama = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nworker-automate-hub = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwxbtool = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nx-clip = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nx-transformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nxaitk_saliency = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nxformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nxgrammar = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nxinference = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nxtts-api-server = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nyolo-poser = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nyolov5 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nyolov7-package = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nyta-general-utils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nzensvi = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nzetascale = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nzuko = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\n","size_bytes":91016},"replit.md":{"content":"# Business Intelligence Platform\n\n## Overview\n\nThis is a comprehensive Business Intelligence Platform that combines multiple Large Language Models (LLMs) to provide sophisticated business analysis capabilities. The platform leverages Groq's fast inference for quick responses and DeepSeek R1's advanced reasoning for complex analytical tasks. Built as a multi-agent system using LangGraph, it orchestrates various business intelligence workflows including market research, competitive analysis, investment screening, company intelligence, trend analysis, and financial analysis.\n\nThe platform provides a Streamlit-based web interface with real-time streaming responses, source citations, tool execution traces, and checkpoint-based conversation replay. It integrates multiple data sources including Wikipedia, DuckDuckGo search, and web scraping capabilities to gather comprehensive business intelligence.\n\n## User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n## System Architecture\n\n### Frontend Architecture\n- **Streamlit Web Application**: Main user interface built with Streamlit providing real-time interaction, streaming responses, and visualization capabilities\n- **Interactive Dashboard**: Features configuration sidebar, analysis history, source panels, and tool execution traces\n- **State Management**: Session-based state management for conversation continuity and workflow persistence\n\n### Backend Architecture\n- **LangGraph Orchestration**: State-based workflow engine managing complex multi-step business analysis processes\n- **Multi-Agent System**: Specialized agents for different analysis types (market research, competitive analysis, financial analysis, etc.)\n- **LLM Factory Pattern**: Intelligent routing between different LLM backends with automatic fallback mechanisms\n- **Workflow Engine**: Pre-defined business intelligence workflows with complexity assessment and dynamic routing\n\n### LLM Integration\n- **Primary Backend (Groq)**: Fast inference using Llama-3.1-70B and Mixtral-8x7B models for quick responses\n- **Reasoning Backend (DeepSeek)**: Advanced reasoning capabilities using DeepSeek R1 and V3 models\n- **Local Fallback (Ollama)**: Offline capabilities with local model deployment for resilience\n- **Intelligent Routing**: Automatic selection between fast inference and deep reasoning based on query complexity\n\n### Data Architecture\n- **SQLite Checkpointing**: Conversation state persistence and replay capabilities\n- **Multi-Source Data Integration**: Wikipedia API, DuckDuckGo search, web scraping with trafilatura\n- **Structured Data Extraction**: Financial metrics, company information, and market data parsing\n- **Citation Management**: Source tracking with titles, URLs, and relevance scoring\n\n### Tool System\n- **Business Intelligence Tools**: Specialized tools for wiki search, web search, content fetching, financial analysis, company news, and market data\n- **Web Scraping**: Advanced content extraction with readability processing\n- **Financial Calculator**: Business-focused calculations with formatting for financial metrics\n- **Rate Limiting**: HTTP request management with retries and backpressure handling\n\n### Observability & Telemetry\n- **OpenTelemetry Integration**: Distributed tracing and metrics collection\n- **Structured Logging**: Comprehensive logging with business context\n- **Performance Monitoring**: Request timing, model performance, and tool execution tracking\n- **Error Handling**: Graceful fallbacks with detailed error reporting\n\n### Configuration Management\n- **Environment-based Configuration**: Flexible settings management through environment variables\n- **Multi-Backend Support**: Configuration for Groq, DeepSeek, and Ollama backends\n- **Runtime Parameters**: Adjustable context lengths, temperatures, and timeout settings\n- **Feature Flags**: Toggleable features for reasoning traces, financial data, and competitor analysis\n\n## External Dependencies\n\n### LLM Services\n- **Groq API**: Primary LLM backend for fast inference with Llama-3.1-70B and Mixtral-8x7B models\n- **DeepSeek API**: Advanced reasoning capabilities with R1 and V3 model variants\n- **Ollama**: Local LLM deployment for offline capabilities and fallback scenarios\n\n### Data Sources & APIs\n- **Wikipedia API (MediaWiki)**: Comprehensive business and company information retrieval\n- **DuckDuckGo Search**: Privacy-focused web search for current business intelligence\n- **Web Scraping (trafilatura)**: Clean text extraction from business websites and news sources\n\n### Core Dependencies\n- **LangChain/LangGraph**: Agent orchestration, tool management, and conversation state handling\n- **Streamlit**: Web application framework for interactive business intelligence dashboard\n- **Plotly**: Advanced data visualization and chart generation for business metrics\n- **SQLite**: Local database for conversation checkpoints and state persistence\n\n### Python Libraries\n- **Requests**: HTTP client with business-appropriate headers and session management\n- **BeautifulSoup**: HTML parsing for web content extraction\n- **Pydantic**: Data validation and settings management with type safety\n- **OpenTelemetry**: Observability stack for monitoring and performance tracking\n- **Tenacity**: Retry logic with exponential backoff for external API calls\n\n### Development & Testing\n- **pytest**: Comprehensive testing framework for tools, workflows, and integrations\n- **mypy**: Static type checking for code reliability\n- **ruff**: Code linting and formatting for maintainable codebase\n\n### Optional Integrations\n- **PostgreSQL**: Alternative database backend (can be added via Drizzle if needed)\n- **FastAPI**: Optional API service layer for backend separation\n- **DuckDB**: Alternative analytical database for complex queries","size_bytes":5778},"bi_core/__init__.py":{"content":"\"\"\"\nBusiness Intelligence Core Module\nMulti-LLM platform for comprehensive business analysis\n\"\"\"\n\n__version__ = \"1.0.0\"\n__author__ = \"Business Intelligence Team\"\n","size_bytes":162},"bi_core/anti_hallucination.py":{"content":"\"\"\"\nAnti-Hallucination Guard System for Business Intelligence Platform\nImplements source verification, confidence scoring, and fact-checking mechanisms\n\"\"\"\n\nimport re\nimport json\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass\nimport hashlib\nfrom urllib.parse import urlparse\n\nfrom bi_core.settings import settings\nfrom bi_core.telemetry import get_logger\nfrom bi_core.memory_optimizer import memory_optimizer\n\nlogger = get_logger(__name__)\n\n@dataclass\nclass FactCheck:\n    \"\"\"Fact checking result\"\"\"\n    claim: str\n    confidence: float\n    sources: List[str]\n    verified: bool\n    evidence: List[str]\n    timestamp: datetime\n\n@dataclass\nclass SourceVerification:\n    \"\"\"Source verification result\"\"\"\n    url: str\n    domain: str\n    credibility_score: float\n    domain_authority: str\n    is_reliable: bool\n    verification_notes: str\n\nclass AntiHallucinationGuard:\n    \"\"\"Anti-hallucination guard system for business intelligence\"\"\"\n    \n    def __init__(self):\n        self.trusted_domains = {\n            # Financial and business sources\n            'sec.gov': 0.95,\n            'reuters.com': 0.90,\n            'bloomberg.com': 0.90,\n            'wsj.com': 0.88,\n            'ft.com': 0.88,\n            'marketwatch.com': 0.85,\n            'fool.com': 0.82,\n            'yahoo.com': 0.80,\n            'cnbc.com': 0.85,\n            'forbes.com': 0.83,\n            \n            # Academic and research\n            'arxiv.org': 0.92,\n            'scholar.google.com': 0.88,\n            'pubmed.ncbi.nlm.nih.gov': 0.90,\n            'jstor.org': 0.88,\n            \n            # Government and regulatory\n            'federalreserve.gov': 0.95,\n            'treasury.gov': 0.95,\n            'census.gov': 0.93,\n            'bls.gov': 0.93,\n            \n            # Reference sources\n            'wikipedia.org': 0.75,\n            'investopedia.com': 0.80,\n        }\n        \n        self.unreliable_indicators = [\n            'advertisement', 'sponsored', 'affiliate', 'paid content',\n            'blog.', 'personal.', 'opinion', 'editorial'\n        ]\n        \n        # Confidence thresholds\n        self.min_confidence = 0.6\n        self.high_confidence = 0.8\n        \n    def verify_sources(self, sources: List[Dict[str, Any]]) -> List[SourceVerification]:\n        \"\"\"Verify the reliability of information sources\"\"\"\n        verified_sources = []\n        \n        for source in sources:\n            url = source.get('url', '')\n            title = source.get('title', '')\n            \n            if not url:\n                continue\n            \n            try:\n                domain = urlparse(url).netloc.lower()\n                \n                # Remove 'www.' prefix\n                if domain.startswith('www.'):\n                    domain = domain[4:]\n                \n                # Check against trusted domains\n                credibility_score = self.trusted_domains.get(domain, 0.5)\n                \n                # Adjust score based on URL characteristics\n                if any(indicator in url.lower() for indicator in self.unreliable_indicators):\n                    credibility_score -= 0.2\n                \n                # Check title for reliability indicators\n                if any(indicator in title.lower() for indicator in self.unreliable_indicators):\n                    credibility_score -= 0.1\n                \n                # Boost score for specific high-quality sections\n                if any(section in url.lower() for section in ['investor-relations', 'sec-filings', 'financial']):\n                    credibility_score += 0.1\n                \n                # Ensure score stays within bounds\n                credibility_score = max(0.0, min(1.0, credibility_score))\n                \n                domain_authority = self._get_domain_authority(domain)\n                is_reliable = credibility_score >= self.min_confidence\n                \n                verification_notes = self._generate_verification_notes(domain, credibility_score, url)\n                \n                verified_sources.append(SourceVerification(\n                    url=url,\n                    domain=domain,\n                    credibility_score=credibility_score,\n                    domain_authority=domain_authority,\n                    is_reliable=is_reliable,\n                    verification_notes=verification_notes\n                ))\n                \n            except Exception as e:\n                logger.error(f\"Failed to verify source {url}: {e}\")\n                continue\n        \n        logger.info(f\"Verified {len(verified_sources)} sources\")\n        return verified_sources\n    \n    def _get_domain_authority(self, domain: str) -> str:\n        \"\"\"Determine domain authority level\"\"\"\n        if self.trusted_domains.get(domain, 0) >= 0.9:\n            return \"High\"\n        elif self.trusted_domains.get(domain, 0) >= 0.8:\n            return \"Medium-High\"\n        elif self.trusted_domains.get(domain, 0) >= 0.7:\n            return \"Medium\"\n        elif self.trusted_domains.get(domain, 0) >= 0.6:\n            return \"Medium-Low\"\n        else:\n            return \"Low\"\n    \n    def _generate_verification_notes(self, domain: str, score: float, url: str) -> str:\n        \"\"\"Generate verification notes for a source\"\"\"\n        notes = []\n        \n        if domain in self.trusted_domains:\n            notes.append(f\"Recognized trusted source ({domain})\")\n        \n        if score >= 0.9:\n            notes.append(\"Highly credible source\")\n        elif score >= 0.8:\n            notes.append(\"Very credible source\")\n        elif score >= 0.7:\n            notes.append(\"Credible source\")\n        elif score >= 0.6:\n            notes.append(\"Moderately credible source\")\n        else:\n            notes.append(\"Lower credibility source - verify independently\")\n        \n        if any(indicator in url.lower() for indicator in self.unreliable_indicators):\n            notes.append(\"Contains potential bias indicators\")\n        \n        return \"; \".join(notes)\n    \n    def check_numerical_claims(self, text: str) -> List[FactCheck]:\n        \"\"\"Check numerical claims in text for consistency\"\"\"\n        fact_checks = []\n        \n        # Extract numerical claims\n        number_patterns = [\n            r'\\$?([\\d,]+(?:\\.\\d{1,2})?)\\s*(billion|million|thousand|B|M|K)',\n            r'([\\d.]+)%',\n            r'revenue.*\\$?([\\d,]+(?:\\.\\d{2})?)',\n            r'profit.*\\$?([\\d,]+(?:\\.\\d{2})?)',\n            r'growth.*?([\\d.]+)%'\n        ]\n        \n        for pattern in number_patterns:\n            matches = re.finditer(pattern, text, re.IGNORECASE)\n            for match in matches:\n                claim = match.group(0)\n                \n                # Basic sanity checks for numerical claims\n                confidence = self._validate_numerical_claim(claim)\n                \n                fact_checks.append(FactCheck(\n                    claim=claim,\n                    confidence=confidence,\n                    sources=[],  # Would be populated with source verification\n                    verified=confidence >= self.min_confidence,\n                    evidence=[f\"Numerical validation: {confidence:.2f}\"],\n                    timestamp=datetime.now()\n                ))\n        \n        return fact_checks\n    \n    def _validate_numerical_claim(self, claim: str) -> float:\n        \"\"\"Validate numerical claims for reasonableness\"\"\"\n        confidence = 0.7  # Base confidence\n        \n        # Extract number from claim\n        numbers = re.findall(r'[\\d,]+(?:\\.\\d+)?', claim)\n        if not numbers:\n            return 0.3\n        \n        try:\n            # Get the main number\n            main_number = float(numbers[0].replace(',', ''))\n            \n            # Check for unrealistic values\n            if 'billion' in claim.lower():\n                # Market caps above $10 trillion are suspicious\n                if main_number > 10000:\n                    confidence -= 0.3\n            elif 'million' in claim.lower():\n                # Revenue in millions above $1 million is normal\n                if main_number > 1000000:\n                    confidence -= 0.2\n            \n            # Check percentage claims\n            if '%' in claim:\n                if main_number > 100:\n                    confidence -= 0.4  # Percentages over 100% are suspicious\n                elif main_number > 1000:\n                    confidence = 0.1  # Very suspicious\n            \n            return max(0.1, confidence)\n            \n        except ValueError:\n            return 0.3\n    \n    def cross_reference_facts(self, claims: List[str], sources: List[SourceVerification]) -> Dict[str, float]:\n        \"\"\"Cross-reference facts across multiple sources\"\"\"\n        fact_confidence = {}\n        \n        for claim in claims:\n            # Simple keyword matching across sources\n            supporting_sources = 0\n            total_credibility = 0\n            \n            for source in sources:\n                if source.is_reliable:\n                    # In a real implementation, we would fetch and analyze source content\n                    # For now, we'll use a simplified approach\n                    supporting_sources += 1\n                    total_credibility += source.credibility_score\n            \n            if supporting_sources > 0:\n                avg_credibility = total_credibility / supporting_sources\n                # More sources increase confidence\n                source_factor = min(1.0, supporting_sources / 3.0)\n                confidence = avg_credibility * source_factor\n            else:\n                confidence = 0.3\n            \n            fact_confidence[claim] = confidence\n        \n        return fact_confidence\n    \n    def calculate_overall_confidence(self, \n                                   sources: List[SourceVerification], \n                                   fact_checks: List[FactCheck],\n                                   content_length: int) -> Dict[str, Any]:\n        \"\"\"Calculate overall confidence score for the analysis\"\"\"\n        \n        # Source reliability score\n        reliable_sources = [s for s in sources if s.is_reliable]\n        source_score = len(reliable_sources) / max(1, len(sources)) if sources else 0.3\n        \n        # Average source credibility\n        if sources:\n            avg_credibility = sum(s.credibility_score for s in sources) / len(sources)\n        else:\n            avg_credibility = 0.3\n        \n        # Fact check score\n        if fact_checks:\n            verified_facts = [f for f in fact_checks if f.verified]\n            fact_score = len(verified_facts) / len(fact_checks)\n        else:\n            fact_score = 0.7  # No specific claims to verify\n        \n        # Content length factor (longer content generally more comprehensive)\n        length_factor = min(1.0, content_length / 2000)  # Normalize to 2000 chars\n        \n        # Calculate weighted overall confidence\n        overall_confidence = (\n            source_score * 0.4 +\n            avg_credibility * 0.3 +\n            fact_score * 0.2 +\n            length_factor * 0.1\n        )\n        \n        return {\n            'overall_confidence': round(overall_confidence, 2),\n            'source_score': round(source_score, 2),\n            'avg_credibility': round(avg_credibility, 2),\n            'fact_score': round(fact_score, 2),\n            'length_factor': round(length_factor, 2),\n            'reliable_sources': len(reliable_sources),\n            'total_sources': len(sources),\n            'verified_facts': len([f for f in fact_checks if f.verified]),\n            'total_fact_checks': len(fact_checks),\n            'confidence_level': self._get_confidence_level(overall_confidence)\n        }\n    \n    def _get_confidence_level(self, confidence: float) -> str:\n        \"\"\"Get human-readable confidence level\"\"\"\n        if confidence >= 0.9:\n            return \"Very High\"\n        elif confidence >= 0.8:\n            return \"High\" \n        elif confidence >= 0.7:\n            return \"Good\"\n        elif confidence >= 0.6:\n            return \"Moderate\"\n        elif confidence >= 0.5:\n            return \"Low\"\n        else:\n            return \"Very Low\"\n    \n    def generate_reliability_report(self, \n                                  content: str,\n                                  sources: List[Dict[str, Any]],\n                                  analysis_type: str) -> Dict[str, Any]:\n        \"\"\"Generate comprehensive reliability report\"\"\"\n        \n        # Verify sources\n        verified_sources = self.verify_sources(sources)\n        \n        # Check numerical claims\n        fact_checks = self.check_numerical_claims(content)\n        \n        # Calculate overall confidence\n        confidence_metrics = self.calculate_overall_confidence(\n            verified_sources, fact_checks, len(content)\n        )\n        \n        # Generate warnings\n        warnings = []\n        if confidence_metrics['overall_confidence'] < self.min_confidence:\n            warnings.append(\"Overall confidence below threshold - verify independently\")\n        \n        unreliable_sources = [s for s in verified_sources if not s.is_reliable]\n        if unreliable_sources:\n            warnings.append(f\"{len(unreliable_sources)} sources have low credibility\")\n        \n        unverified_facts = [f for f in fact_checks if not f.verified]\n        if unverified_facts:\n            warnings.append(f\"{len(unverified_facts)} numerical claims need verification\")\n        \n        return {\n            'analysis_type': analysis_type,\n            'timestamp': datetime.now().isoformat(),\n            'confidence_metrics': confidence_metrics,\n            'verified_sources': [\n                {\n                    'domain': s.domain,\n                    'credibility_score': s.credibility_score,\n                    'authority': s.domain_authority,\n                    'reliable': s.is_reliable,\n                    'notes': s.verification_notes\n                }\n                for s in verified_sources\n            ],\n            'fact_checks': [\n                {\n                    'claim': f.claim,\n                    'confidence': f.confidence,\n                    'verified': f.verified,\n                    'evidence': f.evidence\n                }\n                for f in fact_checks\n            ],\n            'warnings': warnings,\n            'recommendations': self._generate_recommendations(confidence_metrics, warnings)\n        }\n    \n    def _generate_recommendations(self, confidence_metrics: Dict[str, Any], warnings: List[str]) -> List[str]:\n        \"\"\"Generate recommendations based on reliability analysis\"\"\"\n        recommendations = []\n        \n        if confidence_metrics['overall_confidence'] < 0.7:\n            recommendations.append(\"Seek additional sources to verify key claims\")\n        \n        if confidence_metrics['reliable_sources'] < 3:\n            recommendations.append(\"Consult more authoritative sources\")\n        \n        if confidence_metrics['fact_score'] < 0.8:\n            recommendations.append(\"Verify numerical claims with original sources\")\n        \n        if len(warnings) > 2:\n            recommendations.append(\"Exercise caution - multiple reliability concerns identified\")\n        \n        if not recommendations:\n            recommendations.append(\"Analysis meets reliability standards\")\n        \n        return recommendations\n\n# Global anti-hallucination guard instance\nanti_hallucination_guard = AntiHallucinationGuard()\n\ndef verify_analysis_reliability(content: str, sources: List[Dict[str, Any]], analysis_type: str) -> Dict[str, Any]:\n    \"\"\"Verify the reliability of business analysis\"\"\"\n    return anti_hallucination_guard.generate_reliability_report(content, sources, analysis_type)\n\ndef get_source_credibility_scores(sources: List[Dict[str, Any]]) -> Dict[str, float]:\n    \"\"\"Get credibility scores for sources\"\"\"\n    verified = anti_hallucination_guard.verify_sources(sources)\n    return {s.url: s.credibility_score for s in verified}\n\n# Export main components\n__all__ = [\n    \"AntiHallucinationGuard\",\n    \"FactCheck\",\n    \"SourceVerification\",\n    \"anti_hallucination_guard\",\n    \"verify_analysis_reliability\",\n    \"get_source_credibility_scores\"\n]","size_bytes":16296},"bi_core/business_workflows.py":{"content":"\"\"\"\nSpecialized Business Intelligence Workflows\nPre-defined workflows for common business intelligence tasks\n\"\"\"\n\nfrom typing import Dict, List, Any, Optional\nfrom datetime import datetime, timedelta\nimport asyncio\nfrom enum import Enum\n\nfrom bi_core.graph import build_business_graph, create_business_thread\nfrom bi_core.settings import settings\nfrom bi_core.telemetry import get_logger\nfrom langchain_core.messages import HumanMessage\n\nlogger = get_logger(__name__)\n\nclass AnalysisComplexity(Enum):\n    SIMPLE = \"simple\"\n    MODERATE = \"moderate\"\n    COMPLEX = \"complex\"\n\nclass WorkflowStatus(Enum):\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n\nclass BusinessIntelligenceWorkflow:\n    \"\"\"Main class for orchestrating business intelligence workflows\"\"\"\n    \n    def __init__(self):\n        self.graph = build_business_graph()\n        self.active_workflows = {}\n        \n    def analyze_query_complexity(self, query: str) -> AnalysisComplexity:\n        \"\"\"Analyze the complexity of a business query\"\"\"\n        \n        # Simple queries (single entity, basic info)\n        simple_indicators = [\n            \"what is\", \"who is\", \"when was\", \"where is\",\n            \"stock price\", \"headquarters\", \"founded\"\n        ]\n        \n        # Complex queries (multi-step analysis, comparisons)\n        complex_indicators = [\n            \"compare\", \"analyze\", \"evaluate\", \"assess\", \"pros and cons\",\n            \"investment recommendation\", \"competitive landscape\", \"market analysis\",\n            \"financial health\", \"strategic position\", \"due diligence\"\n        ]\n        \n        query_lower = query.lower()\n        \n        # Check for complex indicators first\n        if any(indicator in query_lower for indicator in complex_indicators):\n            return AnalysisComplexity.COMPLEX\n        \n        # Check for simple indicators\n        if any(indicator in query_lower for indicator in simple_indicators):\n            return AnalysisComplexity.SIMPLE\n        \n        # Default to moderate for everything else\n        return AnalysisComplexity.MODERATE\n    \n    async def execute_market_research_workflow(\n        self,\n        market_or_industry: str,\n        focus_areas: Optional[List[str]] = None,\n        geographic_scope: str = \"Global\",\n        time_horizon: str = \"2024-2025\"\n    ) -> Dict[str, Any]:\n        \"\"\"Execute comprehensive market research workflow\"\"\"\n        \n        workflow_id = f\"market_research_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n        self.active_workflows[workflow_id] = {\"status\": WorkflowStatus.RUNNING, \"type\": \"Market Research\"}\n        \n        try:\n            logger.info(f\"Starting market research workflow for: {market_or_industry}\")\n            \n            focus_text = f\" focusing on {', '.join(focus_areas)}\" if focus_areas else \"\"\n            query = f\"\"\"\n            Conduct comprehensive market research for the {market_or_industry} industry{focus_text}.\n            \n            Geographic Scope: {geographic_scope}\n            Time Horizon: {time_horizon}\n            \n            Please provide:\n            1. Market size and growth projections\n            2. Key market segments and dynamics\n            3. Major players and competitive landscape\n            4. Technology trends and disruptions\n            5. Regulatory environment and impacts\n            6. Investment opportunities and risks\n            7. Future outlook and strategic recommendations\n            \n            Include specific data points, financial metrics, and cite all sources.\n            \"\"\"\n            \n            config = create_business_thread(workflow_id)\n            \n            # Execute the workflow\n            result = await self._execute_workflow(query, config, \"Market Research\")\n            \n            self.active_workflows[workflow_id][\"status\"] = WorkflowStatus.COMPLETED\n            logger.info(f\"Market research workflow completed: {workflow_id}\")\n            \n            return result\n            \n        except Exception as e:\n            self.active_workflows[workflow_id][\"status\"] = WorkflowStatus.FAILED\n            logger.error(f\"Market research workflow failed: {e}\")\n            raise\n    \n    async def execute_competitive_analysis_workflow(\n        self,\n        target_company: str,\n        competitors: Optional[List[str]] = None,\n        analysis_dimensions: Optional[List[str]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Execute competitive analysis workflow\"\"\"\n        \n        workflow_id = f\"competitive_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n        self.active_workflows[workflow_id] = {\"status\": WorkflowStatus.RUNNING, \"type\": \"Competitive Analysis\"}\n        \n        try:\n            logger.info(f\"Starting competitive analysis for: {target_company}\")\n            \n            competitors_text = f\" Compare against: {', '.join(competitors)}\" if competitors else \"\"\n            dimensions_text = f\" Focus on: {', '.join(analysis_dimensions)}\" if analysis_dimensions else \"\"\n            \n            query = f\"\"\"\n            Perform comprehensive competitive analysis for {target_company}.{competitors_text}{dimensions_text}\n            \n            Please provide:\n            1. Company profile and business model analysis\n            2. Market positioning and competitive advantages\n            3. Financial performance comparison\n            4. Product/service portfolio analysis\n            5. Strengths, weaknesses, opportunities, threats (SWOT)\n            6. Strategic initiatives and partnerships\n            7. Competitive response recommendations\n            \n            Include quantitative metrics, market share data, and financial comparisons where available.\n            \"\"\"\n            \n            config = create_business_thread(workflow_id)\n            result = await self._execute_workflow(query, config, \"Competitive Analysis\")\n            \n            self.active_workflows[workflow_id][\"status\"] = WorkflowStatus.COMPLETED\n            logger.info(f\"Competitive analysis completed: {workflow_id}\")\n            \n            return result\n            \n        except Exception as e:\n            self.active_workflows[workflow_id][\"status\"] = WorkflowStatus.FAILED\n            logger.error(f\"Competitive analysis failed: {e}\")\n            raise\n    \n    async def execute_investment_screening_workflow(\n        self,\n        company_or_sector: str,\n        investment_criteria: Optional[Dict[str, Any]] = None,\n        risk_tolerance: str = \"Moderate\"\n    ) -> Dict[str, Any]:\n        \"\"\"Execute investment screening and analysis workflow\"\"\"\n        \n        workflow_id = f\"investment_screening_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n        self.active_workflows[workflow_id] = {\"status\": WorkflowStatus.RUNNING, \"type\": \"Investment Screening\"}\n        \n        try:\n            logger.info(f\"Starting investment screening for: {company_or_sector}\")\n            \n            criteria_text = \"\"\n            if investment_criteria:\n                criteria_text = f\"\\nInvestment Criteria: {', '.join([f'{k}: {v}' for k, v in investment_criteria.items()])}\"\n            \n            query = f\"\"\"\n            Conduct thorough investment analysis and screening for {company_or_sector}.\n            \n            Risk Tolerance: {risk_tolerance}{criteria_text}\n            \n            Please provide:\n            1. Investment thesis and key value drivers\n            2. Business model and revenue sustainability analysis\n            3. Financial health assessment (profitability, cash flow, debt)\n            4. Market position and competitive advantages\n            5. Growth prospects and scalability\n            6. Risk assessment (business, market, financial, regulatory)\n            7. Valuation analysis and peer comparison\n            8. Investment recommendation with price targets\n            9. Scenario analysis (bull, base, bear cases)\n            \n            Include specific financial ratios, multiples, and quantitative metrics.\n            Use chain-of-thought reasoning for complex financial analysis.\n            \"\"\"\n            \n            config = create_business_thread(workflow_id)\n            result = await self._execute_workflow(query, config, \"Investment Screening\")\n            \n            self.active_workflows[workflow_id][\"status\"] = WorkflowStatus.COMPLETED\n            logger.info(f\"Investment screening completed: {workflow_id}\")\n            \n            return result\n            \n        except Exception as e:\n            self.active_workflows[workflow_id][\"status\"] = WorkflowStatus.FAILED\n            logger.error(f\"Investment screening failed: {e}\")\n            raise\n    \n    async def execute_trend_analysis_workflow(\n        self,\n        trend_topic: str,\n        time_horizon: str = \"Next 2-3 years\",\n        industries_affected: Optional[List[str]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Execute trend analysis workflow\"\"\"\n        \n        workflow_id = f\"trend_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n        self.active_workflows[workflow_id] = {\"status\": WorkflowStatus.RUNNING, \"type\": \"Trend Analysis\"}\n        \n        try:\n            logger.info(f\"Starting trend analysis for: {trend_topic}\")\n            \n            industries_text = f\" Impact on industries: {', '.join(industries_affected)}\" if industries_affected else \"\"\n            \n            query = f\"\"\"\n            Analyze the business and market implications of the trend: {trend_topic}\n            \n            Time Horizon: {time_horizon}{industries_text}\n            \n            Please provide:\n            1. Trend definition and current state\n            2. Market size and adoption metrics\n            3. Key drivers and enablers\n            4. Industry disruption analysis\n            5. Leading companies and innovations\n            6. Investment landscape and funding trends\n            7. Challenges and barriers to adoption\n            8. Future scenarios and timeline predictions\n            9. Strategic recommendations for businesses\n            \n            Include quantitative data on market size, growth rates, and investment flows.\n            \"\"\"\n            \n            config = create_business_thread(workflow_id)\n            result = await self._execute_workflow(query, config, \"Trend Analysis\")\n            \n            self.active_workflows[workflow_id][\"status\"] = WorkflowStatus.COMPLETED\n            logger.info(f\"Trend analysis completed: {workflow_id}\")\n            \n            return result\n            \n        except Exception as e:\n            self.active_workflows[workflow_id][\"status\"] = WorkflowStatus.FAILED\n            logger.error(f\"Trend analysis failed: {e}\")\n            raise\n    \n    async def execute_company_intelligence_workflow(\n        self,\n        company_name: str,\n        intelligence_depth: str = \"Comprehensive\",\n        include_financials: bool = True,\n        include_news: bool = True\n    ) -> Dict[str, Any]:\n        \"\"\"Execute company intelligence gathering workflow\"\"\"\n        \n        workflow_id = f\"company_intelligence_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n        self.active_workflows[workflow_id] = {\"status\": WorkflowStatus.RUNNING, \"type\": \"Company Intelligence\"}\n        \n        try:\n            logger.info(f\"Starting company intelligence gathering for: {company_name}\")\n            \n            financial_text = \" Include detailed financial analysis.\" if include_financials else \"\"\n            news_text = \" Include recent news and developments.\" if include_news else \"\"\n            \n            query = f\"\"\"\n            Gather comprehensive company intelligence for {company_name}.\n            \n            Depth: {intelligence_depth}{financial_text}{news_text}\n            \n            Please provide:\n            1. Company overview and business model\n            2. Leadership team and key personnel\n            3. Corporate structure and ownership\n            4. Products/services portfolio\n            5. Financial performance and key metrics\n            6. Market position and competitive landscape\n            7. Recent developments and strategic initiatives\n            8. Partnerships and key relationships\n            9. Risk factors and challenges\n            10. Future outlook and strategic direction\n            \n            Include specific data points, financial figures, and recent news citations.\n            \"\"\"\n            \n            config = create_business_thread(workflow_id)\n            result = await self._execute_workflow(query, config, \"Company Intelligence\")\n            \n            self.active_workflows[workflow_id][\"status\"] = WorkflowStatus.COMPLETED\n            logger.info(f\"Company intelligence completed: {workflow_id}\")\n            \n            return result\n            \n        except Exception as e:\n            self.active_workflows[workflow_id][\"status\"] = WorkflowStatus.FAILED\n            logger.error(f\"Company intelligence failed: {e}\")\n            raise\n    \n    async def _execute_workflow(self, query: str, config: Dict[str, str], analysis_type: str) -> Dict[str, Any]:\n        \"\"\"Internal method to execute a workflow\"\"\"\n        \n        messages = [HumanMessage(content=query)]\n        \n        # Execute the graph workflow\n        result = {\"messages\": [], \"sources\": [], \"extracted_data\": {}, \"recommendations\": []}\n        \n        async for event in self.graph.astream(\n            {\"messages\": messages, \"analysis_type\": analysis_type},\n            config,\n            stream_mode=\"values\"\n        ):\n            # Collect all the workflow results\n            if \"messages\" in event:\n                result[\"messages\"].extend(event[\"messages\"])\n            if \"sources\" in event:\n                result[\"sources\"] = event[\"sources\"]\n            if \"extracted_data\" in event:\n                result[\"extracted_data\"] = event[\"extracted_data\"]\n            if \"recommendations\" in event:\n                result[\"recommendations\"] = event[\"recommendations\"]\n            if \"confidence_score\" in event:\n                result[\"confidence_score\"] = event[\"confidence_score\"]\n        \n        return result\n    \n    def get_workflow_status(self, workflow_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get the status of a specific workflow\"\"\"\n        return self.active_workflows.get(workflow_id)\n    \n    def list_active_workflows(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"List all active workflows\"\"\"\n        return {k: v for k, v in self.active_workflows.items() \n                if v[\"status\"] in [WorkflowStatus.RUNNING, WorkflowStatus.PENDING]}\n    \n    def cleanup_completed_workflows(self, hours_old: int = 24):\n        \"\"\"Clean up completed workflows older than specified hours\"\"\"\n        cutoff_time = datetime.now() - timedelta(hours=hours_old)\n        \n        to_remove = []\n        for workflow_id, workflow_info in self.active_workflows.items():\n            # Extract timestamp from workflow_id\n            try:\n                timestamp_str = workflow_id.split('_')[-1]\n                workflow_time = datetime.strptime(timestamp_str, '%Y%m%d_%H%M%S')\n                \n                if (workflow_time < cutoff_time and \n                    workflow_info[\"status\"] in [WorkflowStatus.COMPLETED, WorkflowStatus.FAILED]):\n                    to_remove.append(workflow_id)\n            except (ValueError, IndexError):\n                # Skip workflows with invalid timestamp format\n                continue\n        \n        for workflow_id in to_remove:\n            del self.active_workflows[workflow_id]\n        \n        logger.info(f\"Cleaned up {len(to_remove)} old workflows\")\n\n# Convenience functions for common workflows\nasync def quick_market_research(market: str) -> Dict[str, Any]:\n    \"\"\"Quick market research for a specific market\"\"\"\n    workflow = BusinessIntelligenceWorkflow()\n    return await workflow.execute_market_research_workflow(market)\n\nasync def quick_competitive_analysis(company: str) -> Dict[str, Any]:\n    \"\"\"Quick competitive analysis for a company\"\"\"\n    workflow = BusinessIntelligenceWorkflow()\n    return await workflow.execute_competitive_analysis_workflow(company)\n\nasync def quick_investment_screening(company: str) -> Dict[str, Any]:\n    \"\"\"Quick investment screening for a company\"\"\"\n    workflow = BusinessIntelligenceWorkflow()\n    return await workflow.execute_investment_screening_workflow(company)\n\n# Export main classes and functions\n__all__ = [\n    \"BusinessIntelligenceWorkflow\",\n    \"AnalysisComplexity\", \n    \"WorkflowStatus\",\n    \"quick_market_research\",\n    \"quick_competitive_analysis\", \n    \"quick_investment_screening\"\n]\n","size_bytes":16629},"bi_core/enhanced_data_sources.py":{"content":"\"\"\"\nEnhanced Data Sources for Business Intelligence Platform\nProvides access to multiple free data sources including SEC EDGAR, ArXiv, RSS feeds, and more\n\"\"\"\n\nimport asyncio\nimport aiohttp\nimport feedparser\nimport json\nimport re\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timedelta\nfrom langchain_core.tools import tool\nfrom urllib.parse import quote_plus, urljoin\nfrom bs4 import BeautifulSoup\n\nfrom bi_core.settings import settings\nfrom bi_core.telemetry import get_logger\nfrom bi_core.memory_optimizer import AsyncDataProcessor, memory_optimizer\n\nlogger = get_logger(__name__)\n\nclass EnhancedDataSources:\n    \"\"\"Enhanced data source manager with multiple free APIs\"\"\"\n    \n    def __init__(self):\n        self.base_headers = {\n            'User-Agent': settings.user_agent,\n            'Accept': 'application/json, text/html, */*'\n        }\n    \n    async def search_sec_edgar(self, company_name: str, form_types: List[str] = None) -> List[Dict[str, Any]]:\n        \"\"\"Search SEC EDGAR database for company filings\"\"\"\n        try:\n            if form_types is None:\n                form_types = [\"10-K\", \"10-Q\", \"8-K\", \"DEF 14A\"]\n            \n            # SEC EDGAR API endpoint\n            base_url = \"https://data.sec.gov/submissions/\"\n            search_url = \"https://efts.sec.gov/LATEST/search-index\"\n            \n            results = []\n            async with AsyncDataProcessor() as processor:\n                # Search for company CIK\n                company_query = quote_plus(company_name)\n                search_data = await processor.fetch_url_async(\n                    f\"{search_url}?q={company_query}\"\n                )\n                \n                if search_data:\n                    # Parse search results and extract recent filings\n                    soup = BeautifulSoup(search_data, 'html.parser')\n                    \n                    # Extract filing information\n                    for form_type in form_types:\n                        results.append({\n                            'title': f'{company_name} - {form_type} Filing',\n                            'source': 'SEC EDGAR',\n                            'form_type': form_type,\n                            'url': f'https://www.sec.gov/edgar/search/#/q={company_query}&forms={form_type}',\n                            'date': datetime.now().strftime('%Y-%m-%d'),\n                            'relevance_score': 0.9\n                        })\n            \n            logger.info(f\"Found {len(results)} SEC EDGAR results for {company_name}\")\n            return results[:5]  # Limit results\n            \n        except Exception as e:\n            logger.error(f\"SEC EDGAR search failed: {e}\")\n            return []\n    \n    async def search_arxiv(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:\n        \"\"\"Search ArXiv for academic papers\"\"\"\n        try:\n            base_url = \"http://export.arxiv.org/api/query\"\n            search_query = quote_plus(query)\n            \n            url = f\"{base_url}?search_query=all:{search_query}&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=descending\"\n            \n            async with AsyncDataProcessor() as processor:\n                content = await processor.fetch_url_async(url)\n                \n                if not content:\n                    return []\n                \n                results = []\n                soup = BeautifulSoup(content, 'xml')\n                \n                for entry in soup.find_all('entry'):\n                    title = entry.find('title').text if entry.find('title') else 'Unknown'\n                    summary = entry.find('summary').text if entry.find('summary') else ''\n                    arxiv_url = entry.find('id').text if entry.find('id') else ''\n                    published = entry.find('published').text if entry.find('published') else ''\n                    \n                    results.append({\n                        'title': title.strip(),\n                        'snippet': summary[:300] + \"...\" if len(summary) > 300 else summary,\n                        'url': arxiv_url,\n                        'source': 'ArXiv',\n                        'date': published[:10],\n                        'relevance_score': 0.8\n                    })\n                \n                logger.info(f\"Found {len(results)} ArXiv results\")\n                return results\n                \n        except Exception as e:\n            logger.error(f\"ArXiv search failed: {e}\")\n            return []\n    \n    async def fetch_rss_feeds(self, feed_urls: List[str], keywords: List[str] = None) -> List[Dict[str, Any]]:\n        \"\"\"Fetch and filter RSS feeds for relevant business news\"\"\"\n        try:\n            business_feeds = [\n                'https://feeds.bloomberg.com/markets/news.rss',\n                'https://www.reuters.com/business/feed/',\n                'https://rss.cnn.com/rss/money_latest.rss',\n                'https://feeds.a.dj.com/rss/RSSWorldNews.xml',\n                'https://www.sec.gov/rss/litigation/litreleases.xml'\n            ]\n            \n            all_feeds = feed_urls + business_feeds\n            results = []\n            \n            for feed_url in all_feeds[:5]:  # Limit to 5 feeds\n                try:\n                    feed = feedparser.parse(feed_url)\n                    \n                    for entry in feed.entries[:10]:  # Limit entries per feed\n                        title = getattr(entry, 'title', 'No title')\n                        summary = getattr(entry, 'summary', '')\n                        link = getattr(entry, 'link', '')\n                        published = getattr(entry, 'published', '')\n                        \n                        # Filter by keywords if provided\n                        if keywords:\n                            content_text = (title + ' ' + summary).lower()\n                            if not any(keyword.lower() in content_text for keyword in keywords):\n                                continue\n                        \n                        results.append({\n                            'title': title,\n                            'snippet': summary[:200] + \"...\" if len(summary) > 200 else summary,\n                            'url': link,\n                            'source': f'RSS Feed ({feed_url.split(\"/\")[2]})',\n                            'date': published[:10] if published else '',\n                            'relevance_score': 0.7\n                        })\n                \n                except Exception as e:\n                    logger.warning(f\"Failed to process RSS feed {feed_url}: {e}\")\n                    continue\n            \n            # Sort by date (most recent first)\n            results.sort(key=lambda x: x.get('date', ''), reverse=True)\n            logger.info(f\"Found {len(results)} RSS feed results\")\n            return results[:20]  # Limit total results\n            \n        except Exception as e:\n            logger.error(f\"RSS feed fetch failed: {e}\")\n            return []\n    \n    async def search_news_apis(self, query: str, sources: List[str] = None) -> List[Dict[str, Any]]:\n        \"\"\"Search free news APIs for business information\"\"\"\n        try:\n            results = []\n            \n            # News API (free tier) - requires API key\n            # For now, we'll simulate with web scraping of news sites\n            news_sites = [\n                \"https://www.marketwatch.com/search?q={}\",\n                \"https://finance.yahoo.com/search?p={}\",\n                \"https://www.fool.com/search/?q={}\",\n            ]\n            \n            query_encoded = quote_plus(query)\n            \n            async with AsyncDataProcessor() as processor:\n                for site_template in news_sites:\n                    try:\n                        url = site_template.format(query_encoded)\n                        content = await processor.fetch_url_async(url)\n                        \n                        if content:\n                            soup = BeautifulSoup(content, 'html.parser')\n                            \n                            # Extract article headlines and links\n                            articles = soup.find_all(['h2', 'h3', 'h4'], limit=5)\n                            \n                            for article in articles:\n                                link_elem = article.find('a') or article.find_parent('a')\n                                if link_elem:\n                                    href = link_elem.get('href', '')\n                                    title = article.get_text().strip()\n                                    \n                                    if href and title:\n                                        # Convert relative URLs to absolute\n                                        if href.startswith('/'):\n                                            base_url = '/'.join(url.split('/')[:3])\n                                            href = urljoin(base_url, href)\n                                        \n                                        results.append({\n                                            'title': title,\n                                            'snippet': f'News article from {url.split(\"/\")[2]}',\n                                            'url': href,\n                                            'source': f'News ({url.split(\"/\")[2]})',\n                                            'date': datetime.now().strftime('%Y-%m-%d'),\n                                            'relevance_score': 0.6\n                                        })\n                    \n                    except Exception as e:\n                        logger.warning(f\"Failed to search news site: {e}\")\n                        continue\n            \n            # Remove duplicates\n            seen_urls = set()\n            unique_results = []\n            for result in results:\n                if result['url'] not in seen_urls:\n                    seen_urls.add(result['url'])\n                    unique_results.append(result)\n            \n            logger.info(f\"Found {len(unique_results)} news API results\")\n            return unique_results[:10]\n            \n        except Exception as e:\n            logger.error(f\"News API search failed: {e}\")\n            return []\n    \n    async def search_financial_data_sources(self, symbol_or_company: str) -> List[Dict[str, Any]]:\n        \"\"\"Search free financial data sources\"\"\"\n        try:\n            results = []\n            \n            # Alpha Vantage (free tier), Yahoo Finance, etc.\n            financial_sources = [\n                f\"https://finance.yahoo.com/quote/{symbol_or_company}\",\n                f\"https://www.sec.gov/cgi-bin/browse-edgar?company={quote_plus(symbol_or_company)}&match=&CIK=&filenum=&State=&Country=&SIC=&owner=exclude&Find=Find+Companies&action=getcompany\",\n                f\"https://www.marketwatch.com/investing/stock/{symbol_or_company}\",\n            ]\n            \n            for source_url in financial_sources:\n                try:\n                    results.append({\n                        'title': f'Financial Data for {symbol_or_company}',\n                        'snippet': f'Financial information and data from {source_url.split(\"/\")[2]}',\n                        'url': source_url,\n                        'source': f'Financial Data ({source_url.split(\"/\")[2]})',\n                        'date': datetime.now().strftime('%Y-%m-%d'),\n                        'relevance_score': 0.85\n                    })\n                except Exception as e:\n                    logger.warning(f\"Failed to process financial source: {e}\")\n                    continue\n            \n            logger.info(f\"Found {len(results)} financial data sources\")\n            return results\n            \n        except Exception as e:\n            logger.error(f\"Financial data search failed: {e}\")\n            return []\n\n# Create global instance\nenhanced_data_sources = EnhancedDataSources()\n\n@tool\ndef search_sec_edgar_filings(company_name: str) -> List[Dict[str, Any]]:\n    \"\"\"Search SEC EDGAR database for company filings and regulatory documents.\"\"\"\n    try:\n        loop = asyncio.get_event_loop()\n        return loop.run_until_complete(\n            enhanced_data_sources.search_sec_edgar(company_name)\n        )\n    except Exception as e:\n        logger.error(f\"SEC EDGAR search failed: {e}\")\n        return []\n\n@tool \ndef search_academic_papers(query: str) -> List[Dict[str, Any]]:\n    \"\"\"Search ArXiv for relevant academic papers and research.\"\"\"\n    try:\n        loop = asyncio.get_event_loop()\n        return loop.run_until_complete(\n            enhanced_data_sources.search_arxiv(query)\n        )\n    except Exception as e:\n        logger.error(f\"ArXiv search failed: {e}\")\n        return []\n\n@tool\ndef fetch_business_news_feeds(keywords: str) -> List[Dict[str, Any]]:\n    \"\"\"Fetch latest business news from RSS feeds filtered by keywords.\"\"\"\n    try:\n        keyword_list = [k.strip() for k in keywords.split(',')]\n        loop = asyncio.get_event_loop()\n        return loop.run_until_complete(\n            enhanced_data_sources.fetch_rss_feeds([], keyword_list)\n        )\n    except Exception as e:\n        logger.error(f\"RSS feed fetch failed: {e}\")\n        return []\n\n@tool\ndef search_financial_data(symbol_or_company: str) -> List[Dict[str, Any]]:\n    \"\"\"Search multiple free financial data sources for company information.\"\"\"\n    try:\n        loop = asyncio.get_event_loop()\n        return loop.run_until_complete(\n            enhanced_data_sources.search_financial_data_sources(symbol_or_company)\n        )\n    except Exception as e:\n        logger.error(f\"Financial data search failed: {e}\")\n        return []\n\n@tool\ndef comprehensive_data_search(query: str, include_academic: bool = True, include_regulatory: bool = True, include_news: bool = True) -> List[Dict[str, Any]]:\n    \"\"\"\n    Comprehensive search across multiple enhanced data sources.\n    Combines results from academic papers, regulatory filings, and news sources.\n    \"\"\"\n    try:\n        all_results = []\n        \n        # Use cache to avoid repeated searches\n        cache_key = memory_optimizer.get_cache_key(query, \"comprehensive_search\")\n        cached_result = memory_optimizer.get_cached_result(cache_key)\n        \n        if cached_result:\n            logger.info(\"Returning cached comprehensive search results\")\n            return cached_result\n        \n        loop = asyncio.get_event_loop()\n        \n        # Gather results from different sources concurrently\n        tasks = []\n        \n        if include_academic:\n            tasks.append(enhanced_data_sources.search_arxiv(query))\n        \n        if include_regulatory:\n            # Extract potential company names from query\n            company_name = query.split()[-1] if query.split() else query\n            tasks.append(enhanced_data_sources.search_sec_edgar(company_name))\n        \n        if include_news:\n            tasks.append(enhanced_data_sources.search_news_apis(query))\n        \n        # Execute all searches concurrently\n        results = loop.run_until_complete(asyncio.gather(*tasks, return_exceptions=True))\n        \n        # Combine and process results\n        for result in results:\n            if isinstance(result, list):\n                all_results.extend(result)\n            elif isinstance(result, Exception):\n                logger.warning(f\"Search task failed: {result}\")\n        \n        # Sort by relevance score\n        all_results.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)\n        \n        # Limit total results and optimize memory\n        final_results = memory_optimizer.batch_process_documents(all_results[:15])\n        \n        # Cache the results\n        memory_optimizer.cache_result(cache_key, final_results)\n        \n        logger.info(f\"Comprehensive search completed: {len(final_results)} results\")\n        return final_results\n        \n    except Exception as e:\n        logger.error(f\"Comprehensive data search failed: {e}\")\n        return []\n\n# Enhanced tool list\nENHANCED_DATA_TOOLS = [\n    search_sec_edgar_filings,\n    search_academic_papers,\n    fetch_business_news_feeds,\n    search_financial_data,\n    comprehensive_data_search\n]\n\n# Export main components\n__all__ = [\n    \"EnhancedDataSources\",\n    \"enhanced_data_sources\",\n    \"ENHANCED_DATA_TOOLS\",\n    \"search_sec_edgar_filings\",\n    \"search_academic_papers\", \n    \"fetch_business_news_feeds\",\n    \"search_financial_data\",\n    \"comprehensive_data_search\"\n]","size_bytes":16524},"bi_core/graph.py":{"content":"\"\"\"\nBusiness Intelligence Graph\nLangGraph implementation for orchestrating business intelligence workflows\n\"\"\"\n\nfrom typing import Annotated, List, Dict, Any, Optional\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.prebuilt import ToolNode, tools_condition\nfrom langchain_core.messages import HumanMessage, SystemMessage, AIMessage\nimport json\nfrom datetime import datetime\n\nfrom bi_core.llm_factory import get_smart_llm, get_llm\nfrom bi_core.prompts import (\n    BUSINESS_SYSTEM_PROMPT, MARKET_RESEARCH_PROMPT, COMPETITIVE_ANALYSIS_PROMPT,\n    INVESTMENT_SCREENING_PROMPT, COMPANY_INTELLIGENCE_PROMPT, TREND_ANALYSIS_PROMPT,\n    FINANCIAL_ANALYSIS_PROMPT, SEARCH_DECISION_PROMPT\n)\nfrom bi_core.tools import BUSINESS_TOOLS\nfrom bi_core.settings import settings\nfrom bi_core.telemetry import get_logger\n\nlogger = get_logger(__name__)\n\nclass BusinessState(TypedDict):\n    \"\"\"State management for business intelligence workflows\"\"\"\n    messages: Annotated[list, add_messages]\n    analysis_type: str\n    sources: List[Dict[str, str]]\n    extracted_data: Dict[str, Any]\n    reasoning_trace: str\n    confidence_score: float\n    recommendations: List[str]\n    next_steps: List[str]\n\ndef determine_analysis_type(query: str) -> str:\n    \"\"\"Determine the type of business analysis needed based on the query\"\"\"\n    query_lower = query.lower()\n    \n    type_keywords = {\n        \"Market Research\": [\"market research\", \"market size\", \"industry analysis\", \"market trends\", \"market overview\"],\n        \"Competitive Analysis\": [\"competitive analysis\", \"competitor\", \"competition\", \"market share\", \"competitive landscape\"],\n        \"Investment Screening\": [\"investment\", \"invest in\", \"stock analysis\", \"buy recommendation\", \"valuation\"],\n        \"Company Intelligence\": [\"company profile\", \"about company\", \"company information\", \"business model\"],\n        \"Trend Analysis\": [\"trends\", \"emerging\", \"future of\", \"predictions\", \"forecast\"],\n        \"Financial Analysis\": [\"financial analysis\", \"financial performance\", \"revenue\", \"profit\", \"cash flow\"]\n    }\n    \n    for analysis_type, keywords in type_keywords.items():\n        if any(keyword in query_lower for keyword in keywords):\n            return analysis_type\n    \n    return \"Custom Query\"\n\ndef should_use_reasoning_llm(query: str, analysis_type: str) -> bool:\n    \"\"\"Determine if we should use reasoning-capable LLM\"\"\"\n    reasoning_triggers = [\n        \"analyze\", \"compare\", \"evaluate\", \"assess\", \"explain why\", \"pros and cons\",\n        \"advantages\", \"disadvantages\", \"step by step\", \"reasoning\", \"complex\"\n    ]\n    \n    complex_analysis_types = [\"Investment Screening\", \"Competitive Analysis\", \"Financial Analysis\"]\n    \n    return (any(trigger in query.lower() for trigger in reasoning_triggers) or \n            analysis_type in complex_analysis_types or\n            len(query.split()) > 20)\n\ndef business_analyzer(state: BusinessState) -> Dict[str, Any]:\n    \"\"\"Main business analysis node\"\"\"\n    try:\n        messages = state[\"messages\"]\n        analysis_type = state.get(\"analysis_type\", \"Custom Query\")\n        \n        # Get the latest user message\n        user_message = \"\"\n        for msg in reversed(messages):\n            if msg.type == \"human\":\n                user_message = str(msg.content)\n                break\n        \n        logger.info(f\"Analyzing business query: {analysis_type}\")\n        \n        # Determine if analysis type wasn't set\n        if analysis_type == \"Custom Query\":\n            analysis_type = determine_analysis_type(user_message)\n        \n        # Select appropriate LLM based on complexity\n        use_reasoning = should_use_reasoning_llm(user_message, analysis_type)\n        llm = get_smart_llm(user_message) if use_reasoning else get_llm()\n        \n        # Bind tools to LLM\n        llm_with_tools = llm.bind_tools(BUSINESS_TOOLS)\n        \n        # Build context with appropriate prompts\n        system_prompt = BUSINESS_SYSTEM_PROMPT\n        \n        # Add specific analysis prompt based on type\n        analysis_prompts = {\n            \"Market Research\": MARKET_RESEARCH_PROMPT,\n            \"Competitive Analysis\": COMPETITIVE_ANALYSIS_PROMPT,\n            \"Investment Screening\": INVESTMENT_SCREENING_PROMPT,\n            \"Company Intelligence\": COMPANY_INTELLIGENCE_PROMPT,\n            \"Trend Analysis\": TREND_ANALYSIS_PROMPT,\n            \"Financial Analysis\": FINANCIAL_ANALYSIS_PROMPT\n        }\n        \n        if analysis_type in analysis_prompts:\n            system_prompt += \"\\n\\n\" + analysis_prompts[analysis_type]\n        \n        # Add search decision guidance\n        system_prompt += \"\\n\\n\" + SEARCH_DECISION_PROMPT\n        \n        # Prepare messages with system prompt\n        if not any(msg.type == \"system\" for msg in messages):\n            messages = [SystemMessage(content=system_prompt)] + messages\n        \n        # Add analysis type context\n        enhanced_query = f\"[Analysis Type: {analysis_type}]\\n\\n{user_message}\"\n        messages.append(HumanMessage(content=enhanced_query))\n        \n        # Generate response\n        response = llm_with_tools.invoke(messages)\n        \n        # Extract reasoning traces if present\n        reasoning_trace = \"\"\n        if hasattr(response, 'content') and isinstance(response.content, str):\n            content = response.content\n            if \"<think>\" in content and \"</think>\" in content:\n                start = content.find(\"<think>\") + 7\n                end = content.find(\"</think>\")\n                reasoning_trace = content[start:end].strip()\n        \n        logger.info(f\"Business analysis completed for {analysis_type}\")\n        \n        return {\n            \"messages\": [response],\n            \"analysis_type\": analysis_type,\n            \"reasoning_trace\": reasoning_trace,\n            \"confidence_score\": 0.8  # Default confidence, could be enhanced with actual scoring\n        }\n        \n    except Exception as e:\n        logger.error(f\"Business analysis failed: {e}\")\n        error_msg = AIMessage(content=f\"I apologize, but I encountered an error during analysis: {str(e)}. Please try rephrasing your question or contact support.\")\n        return {\"messages\": [error_msg]}\n\ndef research_coordinator(state: BusinessState) -> Dict[str, Any]:\n    \"\"\"Coordinate research activities and tool usage\"\"\"\n    try:\n        messages = state[\"messages\"]\n        \n        # Check if we have recent tool calls\n        recent_tool_calls = []\n        for msg in reversed(messages[-5:]):  # Check last 5 messages\n            if hasattr(msg, 'tool_calls') and msg.tool_calls:\n                recent_tool_calls.extend(msg.tool_calls)\n        \n        if not recent_tool_calls:\n            # No tools were called, continue to end\n            return {\"messages\": []}\n        \n        logger.info(f\"Research coordinator processing {len(recent_tool_calls)} tool calls\")\n        \n        # Extract and organize tool results\n        sources = []\n        extracted_data = {}\n        \n        # Look for tool responses in recent messages\n        for msg in reversed(messages[-10:]):\n            if msg.type == \"tool\":\n                tool_name = getattr(msg, 'name', 'unknown_tool')\n                tool_content = str(msg.content)\n                \n                # Parse different tool responses\n                if tool_name in ['business_wiki_search', 'business_web_search', 'company_news_search']:\n                    try:\n                        # Try to parse as JSON (list of search results)\n                        if tool_content.startswith('['):\n                            results = json.loads(tool_content)\n                            for result in results:\n                                if isinstance(result, dict) and result.get('url'):\n                                    sources.append({\n                                        'title': result.get('title', 'Unknown'),\n                                        'url': result.get('url', ''),\n                                        'snippet': result.get('snippet', ''),\n                                        'tool_used': tool_name\n                                    })\n                    except json.JSONDecodeError:\n                        # Handle as plain text\n                        sources.append({\n                            'title': f\"Result from {tool_name}\",\n                            'url': '',\n                            'snippet': tool_content[:200] + \"...\" if len(tool_content) > 200 else tool_content,\n                            'tool_used': tool_name\n                        })\n                \n                elif tool_name == 'analyze_financial_metrics':\n                    try:\n                        if tool_content.startswith('['):\n                            metrics = json.loads(tool_content)\n                            for metric in metrics:\n                                if isinstance(metric, dict):\n                                    extracted_data[metric.get('metric', 'Unknown')] = {\n                                        'value': metric.get('value', ''),\n                                        'period': metric.get('period', ''),\n                                        'currency': metric.get('currency', '')\n                                    }\n                    except json.JSONDecodeError:\n                        extracted_data['financial_summary'] = tool_content[:500]\n        \n        # Remove duplicates from sources\n        unique_sources = []\n        seen_urls = set()\n        for source in sources:\n            if source['url'] not in seen_urls:\n                seen_urls.add(source['url'])\n                unique_sources.append(source)\n        \n        logger.info(f\"Research coordination completed: {len(unique_sources)} sources, {len(extracted_data)} data points\")\n        \n        return {\n            \"sources\": unique_sources[:settings.max_sources_per_analysis],\n            \"extracted_data\": extracted_data\n        }\n        \n    except Exception as e:\n        logger.error(f\"Research coordination failed: {e}\")\n        return {\"sources\": [], \"extracted_data\": {}}\n\ndef synthesis_node(state: BusinessState) -> Dict[str, Any]:\n    \"\"\"Synthesize research results into final recommendations\"\"\"\n    try:\n        messages = state[\"messages\"]\n        analysis_type = state.get(\"analysis_type\", \"Custom Query\")\n        sources = state.get(\"sources\", [])\n        extracted_data = state.get(\"extracted_data\", {})\n        \n        logger.info(f\"Synthesizing results for {analysis_type}\")\n        \n        # Create synthesis prompt\n        synthesis_context = f\"\"\"\nBased on the research conducted, please provide a comprehensive synthesis for this {analysis_type} analysis.\n\nAvailable Data:\n- {len(sources)} research sources\n- {len(extracted_data)} extracted data points\n\nResearch Sources:\n{chr(10).join([f\"- {source['title']}: {source['snippet'][:100]}...\" for source in sources[:5]])}\n\nExtracted Data:\n{json.dumps(extracted_data, indent=2) if extracted_data else \"No structured data extracted\"}\n\nPlease provide:\n1. Executive Summary (3-5 key insights)\n2. Strategic Recommendations (3-5 actionable items)\n3. Next Steps for further analysis\n4. Confidence assessment of the analysis\n\nFormat your response professionally and cite sources where relevant.\n\"\"\"\n        \n        # Get LLM for synthesis (use reasoning for complex analysis types)\n        use_reasoning = analysis_type in [\"Investment Screening\", \"Competitive Analysis\", \"Financial Analysis\"]\n        llm = get_llm(for_reasoning=use_reasoning)\n        \n        synthesis_message = HumanMessage(content=synthesis_context)\n        response = llm.invoke([synthesis_message])\n        \n        # Extract recommendations and next steps\n        recommendations = []\n        next_steps = []\n        \n        if hasattr(response, 'content'):\n            content = str(response.content)\n            \n            # Simple extraction of recommendations and next steps\n            if \"recommendations\" in content.lower():\n                rec_section = content.lower().split(\"recommendations\")[1].split(\"next steps\")[0] if \"next steps\" in content.lower() else content.lower().split(\"recommendations\")[1]\n                recommendations = [line.strip() for line in rec_section.split('\\n') if line.strip() and (line.strip().startswith('-') or line.strip().startswith('‚Ä¢'))]\n            \n            if \"next steps\" in content.lower():\n                next_section = content.lower().split(\"next steps\")[1]\n                next_steps = [line.strip() for line in next_section.split('\\n') if line.strip() and (line.strip().startswith('-') or line.strip().startswith('‚Ä¢'))]\n        \n        logger.info(f\"Synthesis completed: {len(recommendations)} recommendations, {len(next_steps)} next steps\")\n        \n        return {\n            \"messages\": [response],\n            \"recommendations\": recommendations[:5],\n            \"next_steps\": next_steps[:5],\n            \"confidence_score\": min(0.9, 0.5 + (len(sources) * 0.1) + (len(extracted_data) * 0.05))\n        }\n        \n    except Exception as e:\n        logger.error(f\"Synthesis failed: {e}\")\n        error_msg = AIMessage(content=f\"Synthesis error: {str(e)}\")\n        return {\"messages\": [error_msg]}\n\ndef build_business_graph(checkpointer=None):\n    \"\"\"Build the business intelligence workflow graph\"\"\"\n    \n    # Initialize checkpointer if not provided\n    if checkpointer is None:\n        checkpointer = MemorySaver()\n    \n    # Create the graph\n    workflow = StateGraph(BusinessState)\n    \n    # Add nodes\n    workflow.add_node(\"analyzer\", business_analyzer)\n    workflow.add_node(\"tools\", ToolNode(BUSINESS_TOOLS))\n    workflow.add_node(\"coordinator\", research_coordinator)\n    workflow.add_node(\"synthesis\", synthesis_node)\n    \n    # Define the flow\n    workflow.add_edge(START, \"analyzer\")\n    \n    # Conditional edge from analyzer\n    workflow.add_conditional_edges(\n        \"analyzer\",\n        tools_condition,\n        {\n            \"tools\": \"tools\",\n            END: \"synthesis\"\n        }\n    )\n    \n    # From tools back to coordinator\n    workflow.add_edge(\"tools\", \"coordinator\")\n    \n    # From coordinator to synthesis\n    workflow.add_edge(\"coordinator\", \"synthesis\")\n    \n    # From synthesis to end\n    workflow.add_edge(\"synthesis\", END)\n    \n    # Compile the graph\n    graph = workflow.compile(checkpointer=checkpointer)\n    \n    logger.info(\"Business intelligence graph compiled successfully\")\n    return graph\n\ndef create_business_thread(thread_id: str = None) -> Dict[str, str]:\n    \"\"\"Create a new business intelligence thread\"\"\"\n    if not thread_id:\n        thread_id = f\"bi-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n    \n    return {\"configurable\": {\"thread_id\": thread_id}}\n\n# Export main functions\n__all__ = [\n    \"build_business_graph\",\n    \"create_business_thread\", \n    \"BusinessState\",\n    \"determine_analysis_type\"\n]\n","size_bytes":14980},"bi_core/llm_factory.py":{"content":"\"\"\"\nLLM Factory for Business Intelligence Platform\nHandles multiple LLM backends with intelligent routing\n\"\"\"\n\nfrom typing import Any, Dict, Optional\nfrom langchain_openai import ChatOpenAI\nfrom langchain_groq import ChatGroq\nfrom langchain_community.chat_models import ChatOllama\nfrom langchain_core.messages import HumanMessage\nimport requests\nfrom bi_core.settings import settings\nfrom bi_core.telemetry import get_logger\n\nlogger = get_logger(__name__)\n\nclass LLMFactory:\n    \"\"\"Factory class for creating and managing LLM instances\"\"\"\n    \n    def __init__(self):\n        self._groq_client = None\n        self._ollama_client = None\n    \n    def get_llm(self, for_reasoning: bool = False, backend: Optional[str] = None, **kwargs):\n        \"\"\"\n        Returns a LangChain-compatible chat model based on configuration\n        \n        Args:\n            for_reasoning: Whether this LLM will be used for complex reasoning tasks\n            backend: Override the default backend selection\n            **kwargs: Additional parameters for the model\n        \"\"\"\n        target_backend = backend or settings.llm_backend\n        \n        try:\n            if target_backend == \"groq\":\n                return self._get_groq_llm(for_reasoning, **kwargs)\n            elif target_backend == \"deepseek\":\n                return self._get_deepseek_llm(for_reasoning, **kwargs)\n            elif target_backend == \"ollama\":\n                return self._get_ollama_llm(for_reasoning, **kwargs)\n            else:\n                raise ValueError(f\"Unknown LLM backend: {target_backend}\")\n                \n        except Exception as e:\n            logger.error(f\"Failed to initialize {target_backend} LLM: {e}\")\n            # Fallback to Ollama if available\n            if target_backend != \"ollama\":\n                logger.info(\"Attempting fallback to Ollama...\")\n                return self._get_ollama_llm(for_reasoning, **kwargs)\n            raise\n    \n    def _get_groq_llm(self, for_reasoning: bool = False, **kwargs):\n        \"\"\"Initialize Groq LLM with business intelligence optimizations\"\"\"\n        if not settings.groq_api_key:\n            raise ValueError(\"GROQ_API_KEY not configured\")\n        \n        # For business intelligence, we want to use reasoning-capable models\n        model_name = settings.groq_model\n        if for_reasoning and \"deepseek-r1\" not in model_name:\n            # Prefer DeepSeek R1 for reasoning tasks\n            model_name = \"deepseek-r1-distill-llama-70b\"\n        \n        config = {\n            \"temperature\": kwargs.get(\"temperature\", 0.6 if for_reasoning else 0.3),\n            \"max_tokens\": kwargs.get(\"max_tokens\", settings.max_output_tokens),\n            \"top_p\": kwargs.get(\"top_p\", 0.95),\n        }\n        \n        # Enable reasoning format for DeepSeek R1\n        if \"deepseek-r1\" in model_name:\n            config[\"extra_body\"] = {\"reasoning_format\": \"raw\"}\n        \n        llm = ChatGroq(\n            api_key=settings.groq_api_key,\n            model=model_name,\n            **config\n        )\n        \n        logger.info(f\"Initialized Groq LLM: {model_name}\")\n        return llm\n    \n    def _get_deepseek_llm(self, for_reasoning: bool = False, **kwargs):\n        \"\"\"Initialize DeepSeek LLM via API\"\"\"\n        if not settings.deepseek_api_key:\n            raise ValueError(\"DEEPSEEK_API_KEY not configured\")\n        \n        # Use R1 for reasoning, V3 for general chat\n        model_name = \"deepseek-r1\" if for_reasoning else \"deepseek-v3\"\n        \n        # Note: This is a simplified implementation\n        # In production, you'd use the official DeepSeek SDK\n        config = {\n            \"temperature\": kwargs.get(\"temperature\", 0.1 if for_reasoning else 0.6),\n            \"max_tokens\": kwargs.get(\"max_tokens\", settings.max_output_tokens),\n        }\n        \n        # Use ChatOpenAI for DeepSeek with OpenAI-compatible interface\n        try:\n            llm = ChatOpenAI(\n                model=model_name,\n                api_key=settings.deepseek_api_key,\n                base_url=settings.deepseek_base_url,\n                **config\n            )\n            logger.info(f\"Initialized DeepSeek LLM: {model_name}\")\n            return llm\n        except Exception as e:\n            logger.error(f\"Failed to initialize DeepSeek LLM: {e}\")\n            raise\n    \n    def _get_ollama_llm(self, for_reasoning: bool = False, **kwargs):\n        \"\"\"Initialize Ollama LLM for local inference\"\"\"\n        model_name = settings.ollama_reason_model if for_reasoning else settings.ollama_chat_model\n        \n        config = {\n            \"temperature\": kwargs.get(\"temperature\", 0.1 if for_reasoning else 0.6),\n            \"num_predict\": kwargs.get(\"max_tokens\", settings.max_output_tokens),\n        }\n        \n        llm = ChatOllama(\n            base_url=settings.ollama_base_url,\n            model=model_name,\n            **config\n        )\n        \n        logger.info(f\"Initialized Ollama LLM: {model_name}\")\n        return llm\n    \n    def get_smart_llm(self, query: str, **kwargs):\n        \"\"\"\n        Intelligently select LLM based on query complexity\n        \n        Args:\n            query: The user query to analyze\n            **kwargs: Additional parameters\n        \"\"\"\n        reasoning_keywords = [\n            \"analyze\", \"compare\", \"evaluate\", \"assess\", \"reasoning\", \"chain of thought\",\n            \"step by step\", \"explain why\", \"pros and cons\", \"advantages\", \"disadvantages\",\n            \"investment\", \"financial analysis\", \"competitive analysis\", \"market research\",\n            \"trend analysis\", \"strategic\", \"forecast\", \"predict\", \"recommendation\"\n        ]\n        \n        # Check if query requires complex reasoning\n        requires_reasoning = any(keyword in query.lower() for keyword in reasoning_keywords)\n        \n        # Complex queries or long queries likely need reasoning\n        if requires_reasoning or len(query.split()) > 20:\n            logger.info(\"Using reasoning-capable LLM for complex query\")\n            return self.get_llm(for_reasoning=True, **kwargs)\n        else:\n            logger.info(\"Using fast inference LLM for simple query\")\n            return self.get_llm(for_reasoning=False, **kwargs)\n    \n    def health_check(self, backend: str = None) -> Dict[str, Any]:\n        \"\"\"Check the health of LLM backends\"\"\"\n        target_backend = backend or settings.llm_backend\n        \n        try:\n            if target_backend == \"groq\":\n                # Test Groq connection\n                llm = self._get_groq_llm()\n                response = llm.invoke(\"Test connection\")\n                return {\"status\": \"healthy\", \"backend\": \"groq\", \"response_length\": len(str(response))}\n                \n            elif target_backend == \"ollama\":\n                # Test Ollama connection\n                response = requests.get(f\"{settings.ollama_base_url}/api/tags\", timeout=5)\n                return {\"status\": \"healthy\" if response.status_code == 200 else \"unhealthy\", \"backend\": \"ollama\"}\n                \n            elif target_backend == \"deepseek\":\n                # Test DeepSeek connection\n                llm = self._get_deepseek_llm()\n                response = llm.invoke(\"Test connection\")\n                return {\"status\": \"healthy\", \"backend\": \"deepseek\", \"response_length\": len(str(response))}\n                \n        except Exception as e:\n            return {\"status\": \"unhealthy\", \"backend\": target_backend, \"error\": str(e)}\n\n# Global LLM factory instance\nllm_factory = LLMFactory()\n\ndef get_llm(*args, **kwargs):\n    \"\"\"Convenience function to get LLM from global factory\"\"\"\n    return llm_factory.get_llm(*args, **kwargs)\n\ndef get_smart_llm(*args, **kwargs):\n    \"\"\"Convenience function to get smart LLM selection\"\"\"\n    return llm_factory.get_smart_llm(*args, **kwargs)\n","size_bytes":7767},"bi_core/memory_optimizer.py":{"content":"\"\"\"\nMemory Optimization Module for Business Intelligence Platform\nImplements FAISS-based vector search, caching, and memory-efficient data processing\n\"\"\"\n\nimport numpy as np\nfrom typing import Dict, List, Any, Optional, Tuple\nimport json\nimport time\nfrom datetime import datetime, timedelta\nfrom cachetools import TTLCache, LRUCache\nimport hashlib\nimport asyncio\nimport aiohttp\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom bi_core.settings import settings\nfrom bi_core.telemetry import get_logger\n\nlogger = get_logger(__name__)\n\nclass MemoryOptimizer:\n    \"\"\"Memory-efficient data processing and caching system\"\"\"\n    \n    def __init__(self, max_cache_size: int = 1000, ttl_seconds: int = 3600):\n        self.ttl_cache = TTLCache(maxsize=max_cache_size, ttl=ttl_seconds)\n        self.lru_cache = LRUCache(maxsize=max_cache_size // 2)\n        self.executor = ThreadPoolExecutor(max_workers=4)\n        \n        # Vector search components (lightweight implementation without FAISS for now)\n        self.document_embeddings = {}\n        self.document_store = {}\n        \n        logger.info(\"Memory optimizer initialized\")\n    \n    def get_cache_key(self, query: str, analysis_type: str = \"\") -> str:\n        \"\"\"Generate cache key for queries\"\"\"\n        content = f\"{query}:{analysis_type}\".encode('utf-8')\n        return hashlib.md5(content).hexdigest()\n    \n    def cache_result(self, key: str, result: Any, use_ttl: bool = True):\n        \"\"\"Cache analysis result\"\"\"\n        try:\n            if use_ttl:\n                self.ttl_cache[key] = result\n            else:\n                self.lru_cache[key] = result\n            logger.debug(f\"Cached result for key: {key[:10]}...\")\n        except Exception as e:\n            logger.error(f\"Failed to cache result: {e}\")\n    \n    def get_cached_result(self, key: str) -> Optional[Any]:\n        \"\"\"Retrieve cached result\"\"\"\n        try:\n            # Check TTL cache first\n            if key in self.ttl_cache:\n                logger.debug(f\"Cache hit (TTL) for key: {key[:10]}...\")\n                return self.ttl_cache[key]\n            \n            # Check LRU cache\n            if key in self.lru_cache:\n                logger.debug(f\"Cache hit (LRU) for key: {key[:10]}...\")\n                return self.lru_cache[key]\n            \n            return None\n        except Exception as e:\n            logger.error(f\"Failed to retrieve cached result: {e}\")\n            return None\n    \n    def optimize_data_structure(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Optimize data structures for memory efficiency\"\"\"\n        try:\n            # Remove redundant fields\n            optimized = {}\n            \n            for key, value in data.items():\n                if isinstance(value, str) and len(value) > 1000:\n                    # Truncate very long strings but keep essential info\n                    optimized[key] = value[:500] + \"...\" + value[-100:] if len(value) > 600 else value\n                elif isinstance(value, list) and len(value) > 20:\n                    # Limit list size\n                    optimized[key] = value[:20]\n                elif isinstance(value, dict):\n                    # Recursively optimize nested dictionaries\n                    optimized[key] = self.optimize_data_structure(value)\n                else:\n                    optimized[key] = value\n            \n            return optimized\n        except Exception as e:\n            logger.error(f\"Failed to optimize data structure: {e}\")\n            return data\n    \n    def batch_process_documents(self, documents: List[Dict[str, Any]], batch_size: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"Process documents in memory-efficient batches\"\"\"\n        processed_docs = []\n        \n        for i in range(0, len(documents), batch_size):\n            batch = documents[i:i + batch_size]\n            \n            # Process each batch\n            for doc in batch:\n                try:\n                    # Optimize document structure\n                    optimized_doc = self.optimize_data_structure(doc)\n                    processed_docs.append(optimized_doc)\n                except Exception as e:\n                    logger.error(f\"Failed to process document: {e}\")\n                    continue\n        \n        logger.info(f\"Processed {len(processed_docs)} documents in batches\")\n        return processed_docs\n    \n    def similarity_search(self, query: str, documents: List[Dict[str, Any]], top_k: int = 5) -> List[Dict[str, Any]]:\n        \"\"\"Lightweight similarity search without heavy vector dependencies\"\"\"\n        try:\n            # Simple keyword-based similarity for now\n            query_words = set(query.lower().split())\n            \n            doc_scores = []\n            for i, doc in enumerate(documents):\n                content = str(doc.get('content', '') + ' ' + doc.get('title', '') + ' ' + doc.get('snippet', '')).lower()\n                content_words = set(content.split())\n                \n                # Calculate Jaccard similarity\n                intersection = len(query_words & content_words)\n                union = len(query_words | content_words)\n                similarity = intersection / union if union > 0 else 0\n                \n                # Boost score for title matches\n                title = doc.get('title', '').lower()\n                if any(word in title for word in query_words):\n                    similarity += 0.2\n                \n                doc_scores.append((similarity, i, doc))\n            \n            # Sort by similarity and return top_k\n            doc_scores.sort(key=lambda x: x[0], reverse=True)\n            return [doc for _, _, doc in doc_scores[:top_k]]\n            \n        except Exception as e:\n            logger.error(f\"Similarity search failed: {e}\")\n            return documents[:top_k]  # Fallback to first k documents\n    \n    def compress_data(self, data: Any) -> bytes:\n        \"\"\"Compress data for efficient storage\"\"\"\n        try:\n            import gzip\n            json_str = json.dumps(data, separators=(',', ':'))\n            return gzip.compress(json_str.encode('utf-8'))\n        except Exception as e:\n            logger.error(f\"Failed to compress data: {e}\")\n            return json.dumps(data).encode('utf-8')\n    \n    def decompress_data(self, compressed_data: bytes) -> Any:\n        \"\"\"Decompress stored data\"\"\"\n        try:\n            import gzip\n            json_str = gzip.decompress(compressed_data).decode('utf-8')\n            return json.loads(json_str)\n        except Exception as e:\n            logger.error(f\"Failed to decompress data: {e}\")\n            return {}\n    \n    def get_memory_stats(self) -> Dict[str, Any]:\n        \"\"\"Get memory usage statistics\"\"\"\n        import psutil\n        import os\n        \n        process = psutil.Process(os.getpid())\n        memory_info = process.memory_info()\n        \n        return {\n            \"rss_mb\": memory_info.rss / 1024 / 1024,  # Resident Set Size\n            \"vms_mb\": memory_info.vms / 1024 / 1024,  # Virtual Memory Size\n            \"ttl_cache_size\": len(self.ttl_cache),\n            \"lru_cache_size\": len(self.lru_cache),\n            \"document_store_size\": len(self.document_store),\n            \"timestamp\": datetime.now().isoformat()\n        }\n    \n    def cleanup_memory(self):\n        \"\"\"Clean up memory by removing old cache entries\"\"\"\n        try:\n            # Clear old entries\n            self.ttl_cache.clear()\n            initial_lru_size = len(self.lru_cache)\n            \n            # Keep only recent LRU entries\n            if len(self.lru_cache) > 100:\n                # Create new cache with top items\n                items = list(self.lru_cache.items())\n                self.lru_cache.clear()\n                for key, value in items[-50:]:  # Keep last 50 items\n                    self.lru_cache[key] = value\n            \n            logger.info(f\"Memory cleanup completed. LRU cache reduced from {initial_lru_size} to {len(self.lru_cache)}\")\n            \n        except Exception as e:\n            logger.error(f\"Memory cleanup failed: {e}\")\n\nclass AsyncDataProcessor:\n    \"\"\"Asynchronous data processing for improved performance\"\"\"\n    \n    def __init__(self, max_concurrent: int = 10):\n        self.semaphore = asyncio.Semaphore(max_concurrent)\n        self.session = None\n        \n    async def __aenter__(self):\n        \"\"\"Async context manager entry\"\"\"\n        timeout = aiohttp.ClientTimeout(total=30)\n        self.session = aiohttp.ClientSession(\n            timeout=timeout,\n            headers={'User-Agent': settings.user_agent}\n        )\n        return self\n    \n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Async context manager exit\"\"\"\n        if self.session:\n            await self.session.close()\n    \n    async def fetch_url_async(self, url: str) -> Optional[str]:\n        \"\"\"Asynchronously fetch URL content\"\"\"\n        async with self.semaphore:\n            try:\n                async with self.session.get(url) as response:\n                    if response.status == 200:\n                        content = await response.text()\n                        return content[:10000]  # Limit content size\n                    else:\n                        logger.warning(f\"HTTP {response.status} for {url}\")\n                        return None\n            except Exception as e:\n                logger.error(f\"Failed to fetch {url}: {e}\")\n                return None\n    \n    async def process_urls_batch(self, urls: List[str]) -> List[Tuple[str, Optional[str]]]:\n        \"\"\"Process multiple URLs concurrently\"\"\"\n        tasks = [self.fetch_url_async(url) for url in urls]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        processed_results = []\n        for url, result in zip(urls, results):\n            if isinstance(result, Exception):\n                logger.error(f\"Exception for {url}: {result}\")\n                processed_results.append((url, None))\n            else:\n                processed_results.append((url, result))\n        \n        return processed_results\n\n# Global memory optimizer instance\nmemory_optimizer = MemoryOptimizer()\n\ndef get_memory_optimizer() -> MemoryOptimizer:\n    \"\"\"Get global memory optimizer instance\"\"\"\n    return memory_optimizer\n\n# Export main classes and functions\n__all__ = [\n    \"MemoryOptimizer\",\n    \"AsyncDataProcessor\", \n    \"memory_optimizer\",\n    \"get_memory_optimizer\"\n]","size_bytes":10432},"bi_core/prompts.py":{"content":"\"\"\"\nBusiness Intelligence Platform Prompts\nSpecialized prompts for business analysis and market research\n\"\"\"\n\n# System prompts for different analysis types\nBUSINESS_SYSTEM_PROMPT = \"\"\"You are a senior business intelligence analyst with expertise in market research, competitive analysis, and strategic planning. You provide comprehensive, data-driven insights that help executives make informed business decisions.\n\nCore Principles:\n- Always think step-by-step and show your reasoning process\n- Use reliable sources and cite them with titles and URLs\n- Provide actionable insights and strategic recommendations\n- Include quantitative data, metrics, and financial figures when available\n- Maintain objectivity and acknowledge uncertainties\n- Structure your analysis professionally with clear sections\n\nFor reasoning-based queries, begin your response with <think> tags to show your analytical process.\"\"\"\n\nMARKET_RESEARCH_PROMPT = \"\"\"Conduct comprehensive market research analysis following this structure:\n\n1. EXECUTIVE SUMMARY (2-3 key insights)\n2. MARKET OVERVIEW\n   - Market size and growth rate\n   - Key market segments\n   - Geographic distribution\n3. COMPETITIVE LANDSCAPE\n   - Major players and market share\n   - Competitive positioning\n   - Recent developments\n4. MARKET DRIVERS & TRENDS\n   - Growth catalysts\n   - Technology trends\n   - Regulatory factors\n5. CHALLENGES & RISKS\n   - Market barriers\n   - Potential threats\n   - Economic factors\n6. FUTURE OUTLOOK\n   - Growth projections\n   - Emerging opportunities\n   - Strategic recommendations\n7. SOURCES (with titles and URLs)\n\nInclude specific data points, percentages, and financial figures where available.\"\"\"\n\nCOMPETITIVE_ANALYSIS_PROMPT = \"\"\"Perform detailed competitive analysis with this framework:\n\n1. EXECUTIVE SUMMARY\n   - Key competitive insights\n   - Market positioning summary\n2. COMPETITOR PROFILES\n   - Company overview and business model\n   - Market share and financial performance\n   - Products/services portfolio\n3. COMPETITIVE COMPARISON\n   - Strengths and weaknesses analysis\n   - Feature/capability comparison matrix\n   - Pricing strategy analysis\n4. MARKET POSITIONING\n   - Brand positioning\n   - Target customer segments\n   - Value proposition analysis\n5. STRATEGIC ANALYSIS\n   - Competitive advantages\n   - Threats and vulnerabilities\n   - Strategic moves and partnerships\n6. RECOMMENDATIONS\n   - Competitive response strategies\n   - Market opportunity areas\n   - Differentiation tactics\n7. SOURCES (with titles and URLs)\n\nFocus on actionable competitive intelligence and strategic implications.\"\"\"\n\nINVESTMENT_SCREENING_PROMPT = \"\"\"Conduct thorough investment analysis using this framework:\n\n1. INVESTMENT THESIS\n   - Core investment rationale\n   - Key value drivers\n2. BUSINESS ANALYSIS\n   - Business model evaluation\n   - Revenue streams and sustainability\n   - Management team assessment\n3. FINANCIAL ANALYSIS\n   - Revenue and profitability trends\n   - Cash flow analysis\n   - Debt and capital structure\n   - Key financial ratios\n4. MARKET POSITION\n   - Industry analysis\n   - Competitive advantages\n   - Market share and growth potential\n5. RISK ASSESSMENT\n   - Business risks\n   - Market risks\n   - Financial risks\n   - Regulatory risks\n6. VALUATION\n   - Current valuation metrics\n   - Peer comparison\n   - Growth prospects\n7. INVESTMENT RECOMMENDATION\n   - Buy/Hold/Sell recommendation\n   - Price targets and scenarios\n   - Risk-adjusted returns\n8. SOURCES (with titles and URLs)\n\nProvide quantitative analysis with specific financial metrics and ratios.\"\"\"\n\nCOMPANY_INTELLIGENCE_PROMPT = \"\"\"Gather comprehensive company intelligence covering:\n\n1. COMPANY OVERVIEW\n   - Business description and history\n   - Corporate structure\n   - Geographic presence\n2. LEADERSHIP TEAM\n   - Key executives and backgrounds\n   - Board composition\n   - Recent management changes\n3. BUSINESS MODEL\n   - Revenue streams\n   - Customer base\n   - Value proposition\n4. FINANCIAL PERFORMANCE\n   - Revenue trends\n   - Profitability metrics\n   - Cash position\n   - Recent financial highlights\n5. STRATEGIC INITIATIVES\n   - Recent developments\n   - Product launches\n   - Partnerships and acquisitions\n   - Investment plans\n6. MARKET POSITION\n   - Industry standing\n   - Competitive advantages\n   - Market share data\n7. NEWS & DEVELOPMENTS\n   - Recent announcements\n   - Press coverage\n   - Analyst opinions\n8. SOURCES (with titles and URLs)\n\nFocus on recent, material information that impacts business value.\"\"\"\n\nTREND_ANALYSIS_PROMPT = \"\"\"Analyze industry trends with comprehensive coverage:\n\n1. TREND OVERVIEW\n   - Trend definition and scope\n   - Current state and maturity\n2. MARKET DYNAMICS\n   - Adoption rates and timeline\n   - Market size and growth projections\n   - Geographic variations\n3. KEY DRIVERS\n   - Technology enablers\n   - Economic factors\n   - Regulatory influences\n   - Consumer behavior changes\n4. INDUSTRY IMPACT\n   - Disrupted sectors\n   - New business models\n   - Value chain changes\n5. LEADING PLAYERS\n   - Companies driving the trend\n   - Innovative solutions\n   - Investment activities\n6. CHALLENGES & BARRIERS\n   - Implementation hurdles\n   - Market resistance\n   - Technical limitations\n7. FUTURE OUTLOOK\n   - Evolution trajectory\n   - Potential scenarios\n   - Timeline predictions\n8. INVESTMENT IMPLICATIONS\n   - Opportunities and risks\n   - Winner/loser predictions\n9. SOURCES (with titles and URLs)\n\nProvide data-driven insights on trend trajectory and business implications.\"\"\"\n\nFINANCIAL_ANALYSIS_PROMPT = \"\"\"Conduct detailed financial analysis covering:\n\n1. EXECUTIVE SUMMARY\n   - Financial health overview\n   - Key findings and concerns\n2. REVENUE ANALYSIS\n   - Revenue trends and growth rates\n   - Revenue mix and segments\n   - Geographic revenue breakdown\n3. PROFITABILITY ANALYSIS\n   - Gross margin trends\n   - Operating margin analysis\n   - Net profit margins\n   - EBITDA performance\n4. CASH FLOW ANALYSIS\n   - Operating cash flow trends\n   - Free cash flow generation\n   - Cash conversion cycle\n   - Capital expenditure patterns\n5. BALANCE SHEET STRENGTH\n   - Asset quality and composition\n   - Debt levels and structure\n   - Liquidity position\n   - Working capital management\n6. FINANCIAL RATIOS\n   - Liquidity ratios\n   - Leverage ratios\n   - Efficiency ratios\n   - Profitability ratios\n7. PEER COMPARISON\n   - Industry benchmarking\n   - Relative performance\n   - Valuation multiples\n8. RISK ASSESSMENT\n   - Financial risks\n   - Credit quality\n   - Debt servicing capability\n9. SOURCES (with titles and URLs)\n\nInclude specific financial metrics, ratios, and year-over-year comparisons.\"\"\"\n\n# Tool usage prompts\nSEARCH_DECISION_PROMPT = \"\"\"Analyze if this query requires external research:\n\nQuery requires research if it mentions:\n- Specific companies, products, or markets\n- Current market data or recent developments\n- Industry trends or competitive landscape\n- Financial performance or metrics\n- \"latest\", \"current\", \"recent\", \"2024\", \"2025\"\n\nIf research is needed, use appropriate search tools:\n- wiki_search: for general company/industry background\n- ddg_search: for recent news and current market data\n- web_fetch: for detailed reports or specific documents\n- financial_search: for financial data and metrics (when available)\"\"\"\n\nCITATION_PROMPT = \"\"\"Always include a SOURCES section with:\n- Source title\n- Direct URL\n- Brief description of relevance\nFormat: [Title ‚Äî URL ‚Äî Relevance]\"\"\"\n\n# Response formatting templates\nEXECUTIVE_SUMMARY_TEMPLATE = \"\"\"\n## Executive Summary\n- Key finding 1 with supporting data\n- Key finding 2 with supporting data  \n- Key finding 3 with supporting data\n\n## Detailed Analysis\n[Structured analysis based on prompt framework]\n\n## Strategic Recommendations\n- Recommendation 1 with rationale\n- Recommendation 2 with rationale\n- Recommendation 3 with rationale\n\n## Sources\n[Title ‚Äî URL ‚Äî Relevance]\n\"\"\"\n\n# Error handling prompts\nINSUFFICIENT_DATA_PROMPT = \"\"\"When data is limited or unavailable:\n1. Clearly state information limitations\n2. Provide available data with confidence levels\n3. Suggest alternative research approaches\n4. Recommend specific sources to consult\n5. Avoid speculation or fabricated data\"\"\"\n\nREASONING_FORMAT_PROMPT = \"\"\"For complex analysis requiring reasoning:\n1. Begin with <think> to show analytical process\n2. Break down the problem step-by-step\n3. Consider multiple perspectives and scenarios\n4. Validate conclusions against available data\n5. End with clear, actionable insights\"\"\"\n","size_bytes":8458},"bi_core/settings.py":{"content":"\"\"\"\nConfiguration settings for the Business Intelligence Platform\n\"\"\"\n\nfrom pydantic import BaseModel, Field\nfrom pydantic_settings import BaseSettings\nimport os\nfrom typing import Optional\n\nclass Settings(BaseSettings):\n    # LLM Configuration\n    llm_backend: str = Field(default=os.getenv(\"LLM_BACKEND\", \"groq\"))\n    \n    # Groq Configuration\n    groq_api_key: str = Field(default=os.getenv(\"GROQ_API_KEY\", \"\"))\n    groq_model: str = Field(default=os.getenv(\"GROQ_MODEL\", \"llama-3.3-70b-versatile\"))\n    \n    # DeepSeek Configuration\n    deepseek_api_key: str = Field(default=os.getenv(\"DEEPSEEK_API_KEY\", \"\"))\n    deepseek_base_url: str = Field(default=os.getenv(\"DEEPSEEK_BASE_URL\", \"https://api.deepseek.com\"))\n    \n    # Ollama Configuration (local fallback)\n    ollama_base_url: str = Field(default=os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\"))\n    ollama_chat_model: str = Field(default=os.getenv(\"OLLAMA_CHAT_MODEL\", \"deepseek-v3\"))\n    ollama_reason_model: str = Field(default=os.getenv(\"OLLAMA_REASON_MODEL\", \"deepseek-r1:7b\"))\n    \n    # Database Configuration\n    database_url: Optional[str] = Field(default=os.getenv(\"DATABASE_URL\"))\n    checkpoint_db: str = Field(default=os.getenv(\"CHECKPOINT_DB\", \"bi_checkpoints.sqlite\"))\n    \n    # Runtime Configuration\n    thread_id: str = Field(default=os.getenv(\"THREAD_ID\", \"bi-session-1\"))\n    user_agent: str = Field(default=os.getenv(\"USER_AGENT\", \"BusinessIntelligencePlatform/1.0 (+contact: admin@company.com)\"))\n    http_timeout: int = Field(default=int(os.getenv(\"HTTP_TIMEOUT\", \"30\")))\n    http_retries: int = Field(default=int(os.getenv(\"HTTP_RETRIES\", \"3\")))\n    \n    # Model Parameters\n    max_context_tokens: int = Field(default=32768)  # Increased for business intelligence\n    max_output_tokens: int = Field(default=4096)    # Increased for detailed analysis\n    default_temperature: float = Field(default=0.6)\n    \n    # Business Intelligence Specific\n    enable_reasoning_traces: bool = Field(default=True)\n    max_sources_per_analysis: int = Field(default=10)\n    enable_financial_data: bool = Field(default=True)\n    enable_competitor_analysis: bool = Field(default=True)\n    \n    # OpenTelemetry Configuration\n    otel_endpoint: Optional[str] = Field(default=os.getenv(\"OTEL_EXPORTER_OTLP_ENDPOINT\"))\n    otel_service_name: str = Field(default=os.getenv(\"OTEL_SERVICE_NAME\", \"business-intelligence-platform\"))\n    \n    # Security Settings\n    rate_limit_requests_per_minute: int = Field(default=60)\n    enable_content_filtering: bool = Field(default=True)\n    \n    class Config:\n        env_file = \".env\"\n        case_sensitive = False\n\n# Global settings instance\nsettings = Settings()\n\n# Validate critical settings\nif not settings.groq_api_key and settings.llm_backend == \"groq\":\n    print(\"Warning: GROQ_API_KEY not set. Groq backend will not be available.\")\n\nif not settings.deepseek_api_key and settings.llm_backend == \"deepseek\":\n    print(\"Warning: DEEPSEEK_API_KEY not set. DeepSeek backend will not be available.\")\n","size_bytes":3010},"bi_core/telemetry.py":{"content":"\"\"\"\nTelemetry and Observability for Business Intelligence Platform\nOpenTelemetry implementation for monitoring and logging\n\"\"\"\n\nimport logging\nimport os\nimport time\nfrom typing import Dict, Any, Optional\nfrom functools import wraps\nfrom contextlib import contextmanager\n\ntry:\n    from opentelemetry import trace, metrics\n    from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n    from opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OTLPMetricExporter\n    from opentelemetry.sdk.trace import TracerProvider\n    from opentelemetry.sdk.trace.export import BatchSpanProcessor\n    from opentelemetry.sdk.metrics import MeterProvider\n    from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader\n    from opentelemetry.sdk.resources import Resource\n    from opentelemetry.semconv.resource import ResourceAttributes\n    OTEL_AVAILABLE = True\nexcept ImportError:\n    OTEL_AVAILABLE = False\n    trace = None\n    metrics = None\n\nfrom bi_core.settings import settings\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.StreamHandler(),\n        logging.FileHandler('business_intelligence.log') if os.path.exists('.') else logging.NullHandler()\n    ]\n)\n\nclass TelemetryManager:\n    \"\"\"Centralized telemetry management\"\"\"\n    \n    def __init__(self):\n        self._tracer = None\n        self._meter = None\n        self._initialized = False\n        \n        # Metrics\n        self._request_counter = None\n        self._request_duration = None\n        self._error_counter = None\n        self._llm_token_counter = None\n        \n        # Initialize if OpenTelemetry is available and configured\n        if OTEL_AVAILABLE and settings.otel_endpoint:\n            self._initialize_otel()\n    \n    def _initialize_otel(self):\n        \"\"\"Initialize OpenTelemetry tracing and metrics\"\"\"\n        try:\n            # Create resource\n            resource = Resource.create({\n                ResourceAttributes.SERVICE_NAME: settings.otel_service_name,\n                ResourceAttributes.SERVICE_VERSION: \"1.0.0\",\n                ResourceAttributes.DEPLOYMENT_ENVIRONMENT: os.getenv(\"ENVIRONMENT\", \"development\")\n            })\n            \n            # Initialize tracing\n            trace_exporter = OTLPSpanExporter(endpoint=settings.otel_endpoint)\n            trace_processor = BatchSpanProcessor(trace_exporter)\n            tracer_provider = TracerProvider(resource=resource)\n            tracer_provider.add_span_processor(trace_processor)\n            trace.set_tracer_provider(tracer_provider)\n            \n            self._tracer = trace.get_tracer(__name__)\n            \n            # Initialize metrics\n            metric_exporter = OTLPMetricExporter(endpoint=settings.otel_endpoint)\n            metric_reader = PeriodicExportingMetricReader(metric_exporter, export_interval_millis=10000)\n            meter_provider = MeterProvider(resource=resource, metric_readers=[metric_reader])\n            metrics.set_meter_provider(meter_provider)\n            \n            self._meter = metrics.get_meter(__name__)\n            \n            # Create metrics\n            self._request_counter = self._meter.create_counter(\n                name=\"bi_requests_total\",\n                description=\"Total number of business intelligence requests\",\n                unit=\"1\"\n            )\n            \n            self._request_duration = self._meter.create_histogram(\n                name=\"bi_request_duration_seconds\",\n                description=\"Duration of business intelligence requests\",\n                unit=\"s\"\n            )\n            \n            self._error_counter = self._meter.create_counter(\n                name=\"bi_errors_total\",\n                description=\"Total number of errors in business intelligence operations\",\n                unit=\"1\"\n            )\n            \n            self._llm_token_counter = self._meter.create_counter(\n                name=\"bi_llm_tokens_total\",\n                description=\"Total number of LLM tokens consumed\",\n                unit=\"1\"\n            )\n            \n            self._initialized = True\n            logging.info(\"OpenTelemetry initialized successfully\")\n            \n        except Exception as e:\n            logging.error(f\"Failed to initialize OpenTelemetry: {e}\")\n            self._initialized = False\n    \n    def get_tracer(self):\n        \"\"\"Get the tracer instance\"\"\"\n        return self._tracer if self._initialized else None\n    \n    def get_meter(self):\n        \"\"\"Get the meter instance\"\"\"\n        return self._meter if self._initialized else None\n    \n    def record_request(self, analysis_type: str, duration: float, success: bool = True):\n        \"\"\"Record a business intelligence request\"\"\"\n        if not self._initialized:\n            return\n        \n        try:\n            # Record request\n            self._request_counter.add(1, {\n                \"analysis_type\": analysis_type,\n                \"status\": \"success\" if success else \"error\"\n            })\n            \n            # Record duration\n            self._request_duration.record(duration, {\n                \"analysis_type\": analysis_type\n            })\n            \n            if not success:\n                self._error_counter.add(1, {\"analysis_type\": analysis_type})\n                \n        except Exception as e:\n            logging.error(f\"Failed to record request metrics: {e}\")\n    \n    def record_llm_usage(self, backend: str, model: str, input_tokens: int, output_tokens: int):\n        \"\"\"Record LLM token usage\"\"\"\n        if not self._initialized:\n            return\n        \n        try:\n            self._llm_token_counter.add(input_tokens, {\n                \"backend\": backend,\n                \"model\": model,\n                \"token_type\": \"input\"\n            })\n            \n            self._llm_token_counter.add(output_tokens, {\n                \"backend\": backend,\n                \"model\": model,\n                \"token_type\": \"output\"\n            })\n            \n        except Exception as e:\n            logging.error(f\"Failed to record LLM usage: {e}\")\n    \n    @contextmanager\n    def trace_operation(self, operation_name: str, attributes: Optional[Dict[str, Any]] = None):\n        \"\"\"Context manager for tracing operations\"\"\"\n        if not self._initialized or not self._tracer:\n            yield None\n            return\n        \n        with self._tracer.start_as_current_span(operation_name) as span:\n            if attributes:\n                for key, value in attributes.items():\n                    span.set_attribute(key, str(value))\n            \n            start_time = time.time()\n            try:\n                yield span\n            except Exception as e:\n                span.set_attribute(\"error\", True)\n                span.set_attribute(\"error.message\", str(e))\n                raise\n            finally:\n                duration = time.time() - start_time\n                span.set_attribute(\"duration_seconds\", duration)\n\n# Global telemetry manager\n_telemetry_manager = TelemetryManager()\n\ndef setup_telemetry():\n    \"\"\"Setup telemetry (called from main application)\"\"\"\n    global _telemetry_manager\n    if not _telemetry_manager._initialized and OTEL_AVAILABLE and settings.otel_endpoint:\n        _telemetry_manager._initialize_otel()\n\ndef get_logger(name: str) -> logging.Logger:\n    \"\"\"Get a logger instance with proper configuration\"\"\"\n    logger = logging.getLogger(name)\n    \n    # Add custom handler for structured logging if needed\n    if not logger.handlers:\n        handler = logging.StreamHandler()\n        formatter = logging.Formatter(\n            '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'\n        )\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n        logger.setLevel(logging.INFO)\n    \n    return logger\n\ndef trace_business_operation(operation_name: str):\n    \"\"\"Decorator for tracing business operations\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            attributes = {\n                \"function_name\": func.__name__,\n                \"module\": func.__module__\n            }\n            \n            # Add relevant arguments to attributes\n            if args and hasattr(args[0], '__class__'):\n                attributes[\"class_name\"] = args[0].__class__.__name__\n            \n            with _telemetry_manager.trace_operation(operation_name, attributes):\n                start_time = time.time()\n                try:\n                    result = func(*args, **kwargs)\n                    duration = time.time() - start_time\n                    \n                    # Record success metric\n                    _telemetry_manager.record_request(\n                        analysis_type=operation_name,\n                        duration=duration,\n                        success=True\n                    )\n                    \n                    return result\n                    \n                except Exception as e:\n                    duration = time.time() - start_time\n                    \n                    # Record error metric\n                    _telemetry_manager.record_request(\n                        analysis_type=operation_name,\n                        duration=duration,\n                        success=False\n                    )\n                    \n                    raise\n                    \n        return wrapper\n    return decorator\n\ndef record_llm_metrics(backend: str, model: str, input_tokens: int = 0, output_tokens: int = 0):\n    \"\"\"Record LLM usage metrics\"\"\"\n    _telemetry_manager.record_llm_usage(backend, model, input_tokens, output_tokens)\n\nclass BusinessMetrics:\n    \"\"\"Business-specific metrics tracking\"\"\"\n    \n    def __init__(self):\n        self.analysis_counts = {}\n        self.error_counts = {}\n        self.average_durations = {}\n        self.llm_usage = {}\n    \n    def record_analysis(self, analysis_type: str, duration: float, success: bool = True):\n        \"\"\"Record analysis execution\"\"\"\n        # Update counts\n        if analysis_type not in self.analysis_counts:\n            self.analysis_counts[analysis_type] = {\"success\": 0, \"error\": 0}\n        \n        if success:\n            self.analysis_counts[analysis_type][\"success\"] += 1\n        else:\n            self.analysis_counts[analysis_type][\"error\"] += 1\n        \n        # Update average durations\n        if analysis_type not in self.average_durations:\n            self.average_durations[analysis_type] = {\"total\": 0, \"count\": 0}\n        \n        self.average_durations[analysis_type][\"total\"] += duration\n        self.average_durations[analysis_type][\"count\"] += 1\n        \n        # Log to OpenTelemetry if available\n        _telemetry_manager.record_request(analysis_type, duration, success)\n    \n    def record_llm_call(self, backend: str, model: str, tokens_used: int):\n        \"\"\"Record LLM API call\"\"\"\n        key = f\"{backend}:{model}\"\n        if key not in self.llm_usage:\n            self.llm_usage[key] = {\"calls\": 0, \"tokens\": 0}\n        \n        self.llm_usage[key][\"calls\"] += 1\n        self.llm_usage[key][\"tokens\"] += tokens_used\n    \n    def get_metrics_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary of all metrics\"\"\"\n        summary = {\n            \"analysis_counts\": self.analysis_counts,\n            \"error_rates\": {},\n            \"average_durations\": {},\n            \"llm_usage\": self.llm_usage\n        }\n        \n        # Calculate error rates\n        for analysis_type, counts in self.analysis_counts.items():\n            total = counts[\"success\"] + counts[\"error\"]\n            if total > 0:\n                summary[\"error_rates\"][analysis_type] = counts[\"error\"] / total\n        \n        # Calculate average durations\n        for analysis_type, duration_data in self.average_durations.items():\n            if duration_data[\"count\"] > 0:\n                summary[\"average_durations\"][analysis_type] = (\n                    duration_data[\"total\"] / duration_data[\"count\"]\n                )\n        \n        return summary\n\n# Global metrics instance\nbusiness_metrics = BusinessMetrics()\n\n# Export main functions and classes\n__all__ = [\n    \"setup_telemetry\",\n    \"get_logger\", \n    \"trace_business_operation\",\n    \"record_llm_metrics\",\n    \"business_metrics\",\n    \"TelemetryManager\"\n]\n","size_bytes":12397},"bi_core/tools.py":{"content":"\"\"\"\nBusiness Intelligence Tools\nEnhanced search and analysis tools for comprehensive business intelligence\n\"\"\"\n\nimport requests\nimport json\nfrom typing import List, Dict, Any, Optional\nfrom typing_extensions import TypedDict\nfrom bs4 import BeautifulSoup\nfrom duckduckgo_search import DDGS\nfrom tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\nfrom langchain_core.tools import tool\nfrom bi_core.settings import settings\nfrom bi_core.telemetry import get_logger\nfrom bi_core.memory_optimizer import memory_optimizer, AsyncDataProcessor\nfrom bi_core.anti_hallucination import verify_analysis_reliability, get_source_credibility_scores\nfrom bi_core.enhanced_data_sources import ENHANCED_DATA_TOOLS, comprehensive_data_search\nfrom utils.web_scraper import get_website_text_content\nimport re\nfrom datetime import datetime, timedelta\n\nlogger = get_logger(__name__)\n\n# Configure requests session with business-appropriate headers\nsession = requests.Session()\nsession.headers.update({\n    \"User-Agent\": settings.user_agent,\n    \"Accept\": \"application/json, text/html, application/xhtml+xml, application/xml;q=0.9, */*;q=0.8\",\n    \"Accept-Language\": \"en-US,en;q=0.5\",\n    \"Accept-Encoding\": \"gzip, deflate\",\n    \"DNT\": \"1\",\n    \"Connection\": \"keep-alive\",\n    \"Upgrade-Insecure-Requests\": \"1\"\n})\n\nclass SearchResult(TypedDict):\n    title: str\n    snippet: str\n    url: str\n    relevance_score: float\n    date: Optional[str]\n\nclass CompanyInfo(TypedDict):\n    name: str\n    industry: str\n    market_cap: Optional[str]\n    revenue: Optional[str]\n    employees: Optional[str]\n    headquarters: Optional[str]\n    website: Optional[str]\n\nclass FinancialMetric(TypedDict):\n    metric: str\n    value: str\n    period: str\n    currency: Optional[str]\n\n@retry(stop=stop_after_attempt(settings.http_retries),\n       wait=wait_exponential(multiplier=1, min=0.5, max=10),\n       retry=retry_if_exception_type((requests.RequestException, ConnectionError)))\ndef _safe_get(url: str, **kwargs) -> requests.Response:\n    \"\"\"Make a safe HTTP GET request with retries and error handling\"\"\"\n    try:\n        response = session.get(url, timeout=settings.http_timeout, **kwargs)\n        response.raise_for_status()\n        return response\n    except Exception as e:\n        logger.error(f\"HTTP request failed for {url}: {e}\")\n        raise\n\n@tool\ndef business_wiki_search(query: str) -> List[SearchResult]:\n    \"\"\"\n    Search Wikipedia for business and company information.\n    Optimized for business intelligence queries about companies, industries, and markets.\n    \"\"\"\n    try:\n        logger.info(f\"Business Wikipedia search: {query}\")\n        \n        # Enhance query for business searches\n        business_terms = [\"company\", \"corporation\", \"industry\", \"market\", \"business\"]\n        if not any(term in query.lower() for term in business_terms):\n            query = f\"{query} company business\"\n        \n        params = {\n            \"action\": \"query\",\n            \"list\": \"search\",\n            \"format\": \"json\",\n            \"srsearch\": query,\n            \"srlimit\": 8,  # More results for business intelligence\n            \"srprop\": \"snippet|titlesnippet|size|timestamp\",\n            \"utf8\": 1,\n            \"origin\": \"*\"\n        }\n        \n        response = _safe_get(\"https://en.wikipedia.org/w/api.php\", params=params)\n        data = response.json()\n        \n        results: List[SearchResult] = []\n        for item in data.get(\"query\", {}).get(\"search\", []):\n            title = item.get(\"title\", \"\")\n            url = f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n            \n            # Clean snippet\n            snippet = BeautifulSoup(item.get(\"snippet\", \"\"), \"html.parser\").get_text()\n            snippet = re.sub(r'\\s+', ' ', snippet).strip()\n            \n            # Calculate relevance based on business terms\n            relevance_score = sum(1 for term in business_terms if term in snippet.lower()) / len(business_terms)\n            \n            results.append({\n                \"title\": title,\n                \"snippet\": snippet,\n                \"url\": url,\n                \"relevance_score\": relevance_score,\n                \"date\": item.get(\"timestamp\", \"\").split(\"T\")[0] if item.get(\"timestamp\") else None\n            })\n        \n        # Sort by relevance\n        results.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n        \n        if not results:\n            results = [{\"title\": \"No results found\", \"snippet\": \"\", \"url\": \"\", \"relevance_score\": 0.0, \"date\": None}]\n            \n        logger.info(f\"Found {len(results)} Wikipedia results\")\n        return results[:5]  # Return top 5 most relevant\n        \n    except Exception as e:\n        logger.error(f\"Wikipedia search failed: {e}\")\n        return [{\"title\": \"Search failed\", \"snippet\": f\"Error: {str(e)}\", \"url\": \"\", \"relevance_score\": 0.0, \"date\": None}]\n\n@tool\ndef business_web_search(query: str) -> List[SearchResult]:\n    \"\"\"\n    Enhanced DuckDuckGo search optimized for business intelligence.\n    Focuses on recent business news, market data, and company information.\n    \"\"\"\n    try:\n        logger.info(f\"Business web search: {query}\")\n        \n        results: List[SearchResult] = []\n        \n        with DDGS() as ddgs:\n            # Get more results for better analysis\n            search_results = list(ddgs.text(\n                query,\n                max_results=10,\n                region='us-en',  # US English for business focus\n                timelimit='y'    # Last year for recent business info\n            ))\n            \n            for result in search_results:\n                title = result.get(\"title\", \"\")\n                snippet = result.get(\"body\", \"\")\n                url = result.get(\"href\", \"\")\n                \n                # Calculate business relevance\n                business_keywords = [\n                    \"company\", \"corporation\", \"business\", \"market\", \"industry\", \"revenue\", \n                    \"profit\", \"earnings\", \"financial\", \"investment\", \"analysis\", \"report\"\n                ]\n                \n                relevance_score = sum(1 for keyword in business_keywords \n                                    if keyword in (title + \" \" + snippet).lower()) / len(business_keywords)\n                \n                # Boost score for business domains\n                business_domains = [\n                    \"bloomberg.com\", \"reuters.com\", \"wsj.com\", \"ft.com\", \"marketwatch.com\",\n                    \"fool.com\", \"sec.gov\", \"investor.gov\", \"yahoo.com/finance\", \"google.com/finance\"\n                ]\n                \n                if any(domain in url for domain in business_domains):\n                    relevance_score += 0.3\n                \n                results.append({\n                    \"title\": title,\n                    \"snippet\": snippet,\n                    \"url\": url,\n                    \"relevance_score\": min(relevance_score, 1.0),\n                    \"date\": None  # DuckDuckGo doesn't provide dates directly\n                })\n        \n        # Sort by relevance and recency\n        results.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n        \n        if not results:\n            results = [{\"title\": \"No results found\", \"snippet\": \"\", \"url\": \"\", \"relevance_score\": 0.0, \"date\": None}]\n        \n        logger.info(f\"Found {len(results)} web search results\")\n        return results[:5]\n        \n    except Exception as e:\n        logger.error(f\"Web search failed: {e}\")\n        return [{\"title\": \"Search failed\", \"snippet\": f\"Error: {str(e)}\", \"url\": \"\", \"relevance_score\": 0.0, \"date\": None}]\n\n@tool\ndef fetch_business_content(url: str) -> str:\n    \"\"\"\n    Fetch and extract clean text content from business websites and reports.\n    Optimized for financial reports, press releases, and business documents.\n    \"\"\"\n    try:\n        logger.info(f\"Fetching business content from: {url}\")\n        \n        # Use the web scraper utility\n        content = get_website_text_content(url)\n        \n        if not content:\n            # Fallback to basic scraping\n            response = _safe_get(url)\n            soup = BeautifulSoup(response.text, 'html.parser')\n            \n            # Remove unwanted elements\n            for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\", \"advertisement\"]):\n                element.decompose()\n            \n            # Focus on main content areas\n            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=re.compile(r'content|main|body'))\n            \n            if main_content:\n                content = main_content.get_text(separator=' ', strip=True)\n            else:\n                content = soup.get_text(separator=' ', strip=True)\n        \n        # Clean up the content\n        content = re.sub(r'\\s+', ' ', content)\n        content = content.strip()\n        \n        # Limit content size for processing\n        max_length = 25000\n        if len(content) > max_length:\n            content = content[:max_length] + \"... [Content truncated]\"\n        \n        logger.info(f\"Extracted {len(content)} characters from {url}\")\n        return content\n        \n    except Exception as e:\n        logger.error(f\"Failed to fetch content from {url}: {e}\")\n        return f\"Error fetching content from {url}: {str(e)}\"\n\n@tool\ndef analyze_financial_metrics(text: str) -> List[FinancialMetric]:\n    \"\"\"\n    Extract financial metrics from text content.\n    Identifies revenue, profit, market cap, and other key business metrics.\n    \"\"\"\n    try:\n        logger.info(\"Analyzing financial metrics from text\")\n        \n        metrics: List[FinancialMetric] = []\n        \n        # Financial metric patterns\n        patterns = {\n            \"revenue\": r'revenue[s]?\\s*(?:of|:)?\\s*[\\$]?([\\d,.]+ (?:billion|million|thousand|B|M|K))',\n            \"profit\": r'profit[s]?\\s*(?:of|:)?\\s*[\\$]?([\\d,.]+ (?:billion|million|thousand|B|M|K))',\n            \"earnings\": r'earnings[s]?\\s*(?:of|:)?\\s*[\\$]?([\\d,.]+ (?:billion|million|thousand|B|M|K))',\n            \"market_cap\": r'market cap(?:italization)?\\s*(?:of|:)?\\s*[\\$]?([\\d,.]+ (?:billion|million|thousand|B|M|K))',\n            \"employees\": r'employ(?:s|ees)\\s*(?:of|:)?\\s*([\\d,]+)',\n            \"growth\": r'growth\\s*(?:of|:)?\\s*([\\d.]+%)',\n            \"margin\": r'margin[s]?\\s*(?:of|:)?\\s*([\\d.]+%)'\n        }\n        \n        for metric_name, pattern in patterns.items():\n            matches = re.finditer(pattern, text, re.IGNORECASE)\n            for match in matches:\n                value = match.group(1)\n                \n                # Try to extract period context\n                context = text[max(0, match.start()-100):match.end()+100]\n                period_match = re.search(r'(Q[1-4]\\s+20\\d{2}|20\\d{2}|fiscal\\s+20\\d{2})', context, re.IGNORECASE)\n                period = period_match.group(1) if period_match else \"Not specified\"\n                \n                currency = \"USD\" if \"$\" in match.group(0) else None\n                \n                metrics.append({\n                    \"metric\": metric_name.title(),\n                    \"value\": value,\n                    \"period\": period,\n                    \"currency\": currency\n                })\n        \n        # Remove duplicates\n        seen = set()\n        unique_metrics = []\n        for metric in metrics:\n            key = (metric[\"metric\"], metric[\"value\"], metric[\"period\"])\n            if key not in seen:\n                seen.add(key)\n                unique_metrics.append(metric)\n        \n        logger.info(f\"Extracted {len(unique_metrics)} financial metrics\")\n        return unique_metrics[:10]  # Return top 10 metrics\n        \n    except Exception as e:\n        logger.error(f\"Financial metrics analysis failed: {e}\")\n        return []\n\n@tool\ndef company_news_search(company_name: str, days_back: int = 30) -> List[SearchResult]:\n    \"\"\"\n    Search for recent news about a specific company.\n    Focuses on business developments, earnings, partnerships, etc.\n    \"\"\"\n    try:\n        logger.info(f\"Searching news for company: {company_name}\")\n        \n        # Construct news-focused search query\n        news_terms = [\"news\", \"earnings\", \"announced\", \"partnership\", \"acquisition\", \"financial\"]\n        query = f'\"{company_name}\" ({\" OR \".join(news_terms)})'\n        \n        results: List[SearchResult] = []\n        \n        with DDGS() as ddgs:\n            # Search for recent news\n            search_results = list(ddgs.text(\n                query,\n                max_results=8,\n                region='us-en',\n                timelimit='m' if days_back <= 30 else 'y'  # Last month or year\n            ))\n            \n            for result in search_results:\n                title = result.get(\"title\", \"\")\n                snippet = result.get(\"body\", \"\")\n                url = result.get(\"href\", \"\")\n                \n                # Calculate news relevance\n                news_keywords = [\n                    \"announced\", \"reports\", \"earnings\", \"quarterly\", \"revenue\", \"profit\",\n                    \"partnership\", \"acquisition\", \"merger\", \"investment\", \"launch\", \"contract\"\n                ]\n                \n                relevance_score = sum(1 for keyword in news_keywords \n                                    if keyword in (title + \" \" + snippet).lower()) / len(news_keywords)\n                \n                # Boost for news sources\n                news_domains = [\n                    \"reuters.com\", \"bloomberg.com\", \"wsj.com\", \"cnbc.com\", \"marketwatch.com\",\n                    \"businesswire.com\", \"prnewswire.com\", \"sec.gov\"\n                ]\n                \n                if any(domain in url for domain in news_domains):\n                    relevance_score += 0.4\n                \n                # Check if company name is prominently mentioned\n                if company_name.lower() in title.lower():\n                    relevance_score += 0.3\n                \n                results.append({\n                    \"title\": title,\n                    \"snippet\": snippet,\n                    \"url\": url,\n                    \"relevance_score\": min(relevance_score, 1.0),\n                    \"date\": None\n                })\n        \n        # Sort by relevance\n        results.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n        \n        if not results:\n            results = [{\"title\": \"No recent news found\", \"snippet\": \"\", \"url\": \"\", \"relevance_score\": 0.0, \"date\": None}]\n        \n        logger.info(f\"Found {len(results)} news results for {company_name}\")\n        return results[:5]\n        \n    except Exception as e:\n        logger.error(f\"Company news search failed: {e}\")\n        return [{\"title\": \"News search failed\", \"snippet\": f\"Error: {str(e)}\", \"url\": \"\", \"relevance_score\": 0.0, \"date\": None}]\n\n@tool\ndef business_calculator(expression: str) -> str:\n    \"\"\"\n    Safe calculator for business and financial calculations.\n    Supports basic math, percentages, and financial formulas.\n    \"\"\"\n    try:\n        logger.info(f\"Business calculation: {expression}\")\n        \n        # Import math for financial calculations\n        import math\n        \n        # Safe evaluation environment\n        allowed_names = {\n            k: getattr(math, k) for k in dir(math) if not k.startswith(\"_\")\n        }\n        allowed_names.update({\n            \"abs\": abs, \"round\": round, \"min\": min, \"max\": max,\n            \"sum\": sum, \"len\": len, \"pow\": pow\n        })\n        \n        # Replace common financial terms\n        expression = expression.replace(\"%\", \"/100\")\n        expression = re.sub(r'\\$', '', expression)  # Remove dollar signs\n        expression = re.sub(r'[,]', '', expression)  # Remove commas\n        \n        # Handle common financial calculations\n        if \"compound\" in expression.lower() and \"interest\" in expression.lower():\n            # Compound interest calculation hint\n            return \"For compound interest: A = P(1 + r/n)^(nt) where P=principal, r=rate, n=compounding frequency, t=time\"\n        \n        # Evaluate the expression\n        result = eval(expression, {\"__builtins__\": {}}, allowed_names)\n        \n        # Format financial results\n        if isinstance(result, (int, float)):\n            if result > 1000000:\n                formatted_result = f\"${result:,.0f} ({result/1000000:.2f}M)\"\n            elif result > 1000:\n                formatted_result = f\"${result:,.0f} ({result/1000:.1f}K)\"\n            else:\n                formatted_result = f\"${result:.2f}\"\n        else:\n            formatted_result = str(result)\n        \n        logger.info(f\"Calculation result: {formatted_result}\")\n        return formatted_result\n        \n    except Exception as e:\n        logger.error(f\"Business calculation failed: {e}\")\n        return f\"Calculation error: {str(e)}. Please check your expression.\"\n\n@tool\ndef market_data_search(ticker_or_company: str) -> Dict[str, Any]:\n    \"\"\"\n    Search for basic market data and company information.\n    Note: This is a simplified version - production systems would integrate with financial data APIs.\n    \"\"\"\n    try:\n        logger.info(f\"Market data search for: {ticker_or_company}\")\n        \n        # Search for company information\n        query = f\"{ticker_or_company} stock market cap revenue financial data\"\n        \n        with DDGS() as ddgs:\n            results = list(ddgs.text(query, max_results=5))\n            \n            market_info = {\n                \"company\": ticker_or_company,\n                \"search_results\": [],\n                \"extracted_data\": {}\n            }\n            \n            for result in results:\n                title = result.get(\"title\", \"\")\n                snippet = result.get(\"body\", \"\")\n                url = result.get(\"href\", \"\")\n                \n                # Look for financial data in snippets\n                if any(term in snippet.lower() for term in [\"market cap\", \"revenue\", \"stock price\", \"earnings\"]):\n                    market_info[\"search_results\"].append({\n                        \"title\": title,\n                        \"snippet\": snippet,\n                        \"url\": url\n                    })\n                    \n                    # Try to extract basic metrics\n                    metrics = analyze_financial_metrics(snippet)\n                    if metrics:\n                        for metric in metrics:\n                            market_info[\"extracted_data\"][metric[\"metric\"]] = {\n                                \"value\": metric[\"value\"],\n                                \"period\": metric[\"period\"]\n                            }\n            \n            logger.info(f\"Found market data for {ticker_or_company}\")\n            return market_info\n            \n    except Exception as e:\n        logger.error(f\"Market data search failed: {e}\")\n        return {\"error\": str(e), \"company\": ticker_or_company}\n\n# Enhanced tool with caching and reliability checking\n@tool\ndef enhanced_business_search(query: str, analysis_type: str = \"general\") -> List[Dict[str, Any]]:\n    \"\"\"\n    Enhanced business search with caching, memory optimization, and reliability verification.\n    Combines multiple data sources and applies anti-hallucination guards.\n    \"\"\"\n    try:\n        # Check cache first\n        cache_key = memory_optimizer.get_cache_key(query, analysis_type)\n        cached_result = memory_optimizer.get_cached_result(cache_key)\n        \n        if cached_result:\n            logger.info(f\"Returning cached search results for: {query[:50]}...\")\n            return cached_result\n        \n        # Use comprehensive search with enhanced data sources\n        results = comprehensive_data_search(\n            query, \n            include_academic=True, \n            include_regulatory=True, \n            include_news=True\n        )\n        \n        # Add traditional search results\n        wiki_results = business_wiki_search(query)\n        web_results = business_web_search(query)\n        \n        # Combine all results\n        all_results = results + wiki_results + web_results\n        \n        # Apply memory optimization\n        optimized_results = memory_optimizer.batch_process_documents(all_results)\n        \n        # Apply similarity search to get most relevant results\n        final_results = memory_optimizer.similarity_search(query, optimized_results, top_k=10)\n        \n        # Add credibility scores\n        credibility_scores = get_source_credibility_scores(final_results)\n        for result in final_results:\n            url = result.get('url', '')\n            result['credibility_score'] = credibility_scores.get(url, 0.5)\n        \n        # Cache the results\n        memory_optimizer.cache_result(cache_key, final_results)\n        \n        logger.info(f\"Enhanced search completed: {len(final_results)} results with reliability scores\")\n        return final_results\n        \n    except Exception as e:\n        logger.error(f\"Enhanced business search failed: {e}\")\n        # Fallback to basic search\n        return business_web_search(query)\n\n@tool\ndef verify_information_reliability(content: str, sources: List[Dict[str, Any]], analysis_type: str) -> Dict[str, Any]:\n    \"\"\"\n    Verify the reliability and accuracy of business analysis information.\n    Provides confidence scores and recommendations for fact-checking.\n    \"\"\"\n    try:\n        reliability_report = verify_analysis_reliability(content, sources, analysis_type)\n        logger.info(f\"Reliability verification completed for {analysis_type}\")\n        return reliability_report\n    except Exception as e:\n        logger.error(f\"Reliability verification failed: {e}\")\n        return {\"error\": str(e), \"confidence_metrics\": {\"overall_confidence\": 0.5}}\n\n# Tool registry for the graph - Enhanced with new capabilities\nBUSINESS_TOOLS = [\n    business_wiki_search,\n    business_web_search,\n    fetch_business_content,\n    analyze_financial_metrics,\n    company_news_search,\n    business_calculator,\n    market_data_search,\n    enhanced_business_search,\n    verify_information_reliability\n] + ENHANCED_DATA_TOOLS\n","size_bytes":21999},"tests/test_tools.py":{"content":"\"\"\"\nTest suite for Business Intelligence Tools\n\"\"\"\n\nimport pytest\nimport json\nfrom unittest.mock import patch, MagicMock\nfrom bi_core.tools import (\n    business_wiki_search, business_web_search, fetch_business_content,\n    analyze_financial_metrics, company_news_search, business_calculator,\n    market_data_search\n)\n\nclass TestBusinessTools:\n    \"\"\"Test business intelligence tools\"\"\"\n    \n    def test_business_calculator_basic_math(self):\n        \"\"\"Test basic mathematical operations\"\"\"\n        result = business_calculator(\"100 + 50\")\n        assert \"$150.00\" in result\n        \n        result = business_calculator(\"1000000\")\n        assert \"1.0M\" in result\n        \n        result = business_calculator(\"1500000\")\n        assert \"1.5M\" in result\n    \n    def test_business_calculator_percentage(self):\n        \"\"\"Test percentage calculations\"\"\"\n        result = business_calculator(\"100 * 25%\")\n        assert \"25\" in result\n    \n    def test_business_calculator_error_handling(self):\n        \"\"\"Test calculator error handling\"\"\"\n        result = business_calculator(\"invalid expression\")\n        assert \"error\" in result.lower()\n    \n    def test_analyze_financial_metrics_basic(self):\n        \"\"\"Test financial metrics extraction\"\"\"\n        text = \"\"\"\n        The company reported revenue of $2.5 billion for Q4 2024, \n        with a profit margin of 15.2% and market cap of $50 billion.\n        The company employs 25,000 people globally.\n        \"\"\"\n        \n        metrics = analyze_financial_metrics(text)\n        \n        # Should find revenue, margin, market cap, employees\n        assert len(metrics) >= 3\n        \n        # Check if we found revenue\n        revenue_found = any(m[\"metric\"].lower() == \"revenue\" for m in metrics)\n        assert revenue_found\n        \n        # Check if we found market cap\n        market_cap_found = any(m[\"metric\"].lower() == \"market_cap\" for m in metrics)\n        assert market_cap_found\n    \n    def test_analyze_financial_metrics_empty(self):\n        \"\"\"Test financial metrics with empty text\"\"\"\n        metrics = analyze_financial_metrics(\"\")\n        assert metrics == []\n    \n    @patch('bi_core.tools._safe_get')\n    def test_business_wiki_search_mock(self, mock_get):\n        \"\"\"Test Wikipedia search with mocked response\"\"\"\n        mock_response = MagicMock()\n        mock_response.json.return_value = {\n            \"query\": {\n                \"search\": [\n                    {\n                        \"title\": \"Apple Inc.\",\n                        \"snippet\": \"Apple Inc. is an American multinational technology <span>company</span>\",\n                        \"timestamp\": \"2024-01-01T00:00:00Z\"\n                    }\n                ]\n            }\n        }\n        mock_get.return_value = mock_response\n        \n        results = business_wiki_search(\"Apple company\")\n        \n        assert len(results) >= 1\n        assert results[0][\"title\"] == \"Apple Inc.\"\n        assert \"company\" in results[0][\"snippet\"]\n        assert \"wikipedia.org\" in results[0][\"url\"]\n    \n    @patch('bi_core.tools._safe_get')\n    def test_fetch_business_content_mock(self, mock_get):\n        \"\"\"Test content fetching with mocked response\"\"\"\n        mock_response = MagicMock()\n        mock_response.text = \"\"\"\n        <html>\n            <body>\n                <main>\n                    <h1>Company News</h1>\n                    <p>This is important business content about financial performance.</p>\n                </main>\n            </body>\n        </html>\n        \"\"\"\n        mock_get.return_value = mock_response\n        \n        content = fetch_business_content(\"https://example.com/news\")\n        \n        assert \"Company News\" in content\n        assert \"business content\" in content\n        assert \"financial performance\" in content\n    \n    @patch('duckduckgo_search.DDGS')\n    def test_business_web_search_mock(self, mock_ddgs):\n        \"\"\"Test web search with mocked DuckDuckGo\"\"\"\n        mock_ddgs_instance = MagicMock()\n        mock_ddgs.return_value.__enter__.return_value = mock_ddgs_instance\n        \n        mock_ddgs_instance.text.return_value = [\n            {\n                \"title\": \"Tesla Q4 2024 Earnings Report\",\n                \"body\": \"Tesla reported strong quarterly earnings with revenue growth of 25%\",\n                \"href\": \"https://investor.tesla.com/earnings\"\n            }\n        ]\n        \n        results = business_web_search(\"Tesla earnings Q4 2024\")\n        \n        assert len(results) >= 1\n        assert \"Tesla\" in results[0][\"title\"]\n        assert \"earnings\" in results[0][\"snippet\"]\n        assert results[0][\"relevance_score\"] > 0\n    \n    @patch('duckduckgo_search.DDGS')\n    def test_company_news_search_mock(self, mock_ddgs):\n        \"\"\"Test company news search with mocked response\"\"\"\n        mock_ddgs_instance = MagicMock()\n        mock_ddgs.return_value.__enter__.return_value = mock_ddgs_instance\n        \n        mock_ddgs_instance.text.return_value = [\n            {\n                \"title\": \"Microsoft announces new partnership\",\n                \"body\": \"Microsoft announced a strategic partnership to expand AI capabilities\",\n                \"href\": \"https://news.microsoft.com/partnership\"\n            }\n        ]\n        \n        results = company_news_search(\"Microsoft\")\n        \n        assert len(results) >= 1\n        assert \"Microsoft\" in results[0][\"title\"]\n        assert results[0][\"relevance_score\"] > 0\n    \n    def test_market_data_search_structure(self):\n        \"\"\"Test market data search return structure\"\"\"\n        # This test doesn't mock external calls but checks the structure\n        result = market_data_search(\"AAPL\")\n        \n        assert \"company\" in result\n        assert \"search_results\" in result\n        assert \"extracted_data\" in result\n        assert result[\"company\"] == \"AAPL\"\n    \nclass TestToolIntegration:\n    \"\"\"Test tool integration scenarios\"\"\"\n    \n    @patch('bi_core.tools._safe_get')\n    @patch('duckduckgo_search.DDGS')\n    def test_comprehensive_company_research(self, mock_ddgs, mock_get):\n        \"\"\"Test a comprehensive company research scenario\"\"\"\n        # Mock Wikipedia response\n        mock_response = MagicMock()\n        mock_response.json.return_value = {\n            \"query\": {\n                \"search\": [\n                    {\n                        \"title\": \"Amazon.com\",\n                        \"snippet\": \"Amazon.com, Inc. is an American multinational technology company\",\n                        \"timestamp\": \"2024-01-01T00:00:00Z\"\n                    }\n                ]\n            }\n        }\n        mock_get.return_value = mock_response\n        \n        # Mock DuckDuckGo response\n        mock_ddgs_instance = MagicMock()\n        mock_ddgs.return_value.__enter__.return_value = mock_ddgs_instance\n        mock_ddgs_instance.text.return_value = [\n            {\n                \"title\": \"Amazon Reports Strong Q4 Results\",\n                \"body\": \"Amazon reported revenue of $170 billion with AWS growing 20%\",\n                \"href\": \"https://amazon.com/investor-relations\"\n            }\n        ]\n        \n        # Test Wikipedia search\n        wiki_results = business_wiki_search(\"Amazon company\")\n        assert len(wiki_results) >= 1\n        assert \"Amazon\" in wiki_results[0][\"title\"]\n        \n        # Test web search\n        web_results = business_web_search(\"Amazon financial results 2024\")\n        assert len(web_results) >= 1\n        \n        # Test news search\n        news_results = company_news_search(\"Amazon\")\n        assert len(news_results) >= 1\n        \n        # Test metrics extraction from news\n        news_text = web_results[0][\"snippet\"]\n        metrics = analyze_financial_metrics(news_text)\n        # Should find some financial metrics\n        assert isinstance(metrics, list)\n\nif __name__ == \"__main__\":\n    pytest.main([__file__])\n","size_bytes":7833},"tests/test_workflows.py":{"content":"\"\"\"\nTest suite for Business Intelligence Workflows\n\"\"\"\n\nimport pytest\nimport asyncio\nfrom unittest.mock import patch, MagicMock, AsyncMock\nfrom bi_core.business_workflows import (\n    BusinessIntelligenceWorkflow, AnalysisComplexity, WorkflowStatus,\n    quick_market_research, quick_competitive_analysis, quick_investment_screening\n)\n\nclass TestWorkflowAnalysis:\n    \"\"\"Test workflow analysis and complexity detection\"\"\"\n    \n    def test_query_complexity_simple(self):\n        \"\"\"Test simple query complexity detection\"\"\"\n        workflow = BusinessIntelligenceWorkflow()\n        \n        simple_queries = [\n            \"What is Apple Inc?\",\n            \"Who is the CEO of Microsoft?\",\n            \"When was Google founded?\",\n            \"What is Tesla's stock price?\"\n        ]\n        \n        for query in simple_queries:\n            complexity = workflow.analyze_query_complexity(query)\n            assert complexity == AnalysisComplexity.SIMPLE, f\"Query should be simple: {query}\"\n    \n    def test_query_complexity_complex(self):\n        \"\"\"Test complex query complexity detection\"\"\"\n        workflow = BusinessIntelligenceWorkflow()\n        \n        complex_queries = [\n            \"Compare Amazon and Microsoft's cloud computing strategies\",\n            \"Analyze Tesla's competitive position in the EV market\",\n            \"Evaluate Apple as an investment opportunity\",\n            \"Assess the pros and cons of investing in renewable energy stocks\"\n        ]\n        \n        for query in complex_queries:\n            complexity = workflow.analyze_query_complexity(query)\n            assert complexity == AnalysisComplexity.COMPLEX, f\"Query should be complex: {query}\"\n    \n    def test_query_complexity_moderate(self):\n        \"\"\"Test moderate query complexity detection\"\"\"\n        workflow = BusinessIntelligenceWorkflow()\n        \n        moderate_queries = [\n            \"Tell me about the smartphone industry\",\n            \"What are the latest developments in AI?\",\n            \"How is Netflix performing this year?\"\n        ]\n        \n        for query in moderate_queries:\n            complexity = workflow.analyze_query_complexity(query)\n            assert complexity == AnalysisComplexity.MODERATE, f\"Query should be moderate: {query}\"\n\nclass TestWorkflowExecution:\n    \"\"\"Test workflow execution scenarios\"\"\"\n    \n    @patch('bi_core.business_workflows.build_business_graph')\n    @pytest.mark.asyncio\n    async def test_market_research_workflow_structure(self, mock_build_graph):\n        \"\"\"Test market research workflow structure\"\"\"\n        # Mock the graph\n        mock_graph = MagicMock()\n        mock_graph.astream = AsyncMock(return_value=iter([\n            {\n                \"messages\": [MagicMock(content=\"Market research analysis complete\")],\n                \"sources\": [{\"title\": \"Market Report\", \"url\": \"https://example.com\"}],\n                \"extracted_data\": {\"market_size\": \"$100B\"},\n                \"recommendations\": [\"Recommendation 1\"],\n                \"confidence_score\": 0.85\n            }\n        ]))\n        mock_build_graph.return_value = mock_graph\n        \n        workflow = BusinessIntelligenceWorkflow()\n        \n        result = await workflow.execute_market_research_workflow(\n            market_or_industry=\"Electric Vehicles\",\n            focus_areas=[\"Market Size\", \"Key Players\"],\n            geographic_scope=\"North America\"\n        )\n        \n        assert \"messages\" in result\n        assert \"sources\" in result\n        assert \"extracted_data\" in result\n        assert \"recommendations\" in result\n        assert \"confidence_score\" in result\n        \n        # Check that the workflow was recorded\n        assert len(workflow.active_workflows) > 0\n        workflow_info = list(workflow.active_workflows.values())[0]\n        assert workflow_info[\"type\"] == \"Market Research\"\n        assert workflow_info[\"status\"] == WorkflowStatus.COMPLETED\n    \n    @patch('bi_core.business_workflows.build_business_graph')\n    @pytest.mark.asyncio\n    async def test_competitive_analysis_workflow_structure(self, mock_build_graph):\n        \"\"\"Test competitive analysis workflow structure\"\"\"\n        # Mock the graph\n        mock_graph = MagicMock()\n        mock_graph.astream = AsyncMock(return_value=iter([\n            {\n                \"messages\": [MagicMock(content=\"Competitive analysis complete\")],\n                \"sources\": [{\"title\": \"Company Report\", \"url\": \"https://example.com\"}],\n                \"extracted_data\": {\"market_share\": \"25%\"},\n                \"recommendations\": [\"Strategic recommendation\"],\n                \"confidence_score\": 0.78\n            }\n        ]))\n        mock_build_graph.return_value = mock_graph\n        \n        workflow = BusinessIntelligenceWorkflow()\n        \n        result = await workflow.execute_competitive_analysis_workflow(\n            target_company=\"Tesla\",\n            competitors=[\"BMW\", \"Mercedes\", \"Audi\"],\n            analysis_dimensions=[\"Technology\", \"Market Share\", \"Pricing\"]\n        )\n        \n        assert \"messages\" in result\n        assert \"sources\" in result\n        assert \"extracted_data\" in result\n        assert \"recommendations\" in result\n        \n        # Check workflow recording\n        assert len(workflow.active_workflows) > 0\n        workflow_info = list(workflow.active_workflows.values())[0]\n        assert workflow_info[\"type\"] == \"Competitive Analysis\"\n    \n    @patch('bi_core.business_workflows.build_business_graph')\n    @pytest.mark.asyncio\n    async def test_investment_screening_workflow_structure(self, mock_build_graph):\n        \"\"\"Test investment screening workflow structure\"\"\"\n        # Mock the graph\n        mock_graph = MagicMock()\n        mock_graph.astream = AsyncMock(return_value=iter([\n            {\n                \"messages\": [MagicMock(content=\"Investment analysis complete\")],\n                \"sources\": [{\"title\": \"Financial Report\", \"url\": \"https://example.com\"}],\n                \"extracted_data\": {\"pe_ratio\": \"15.5\", \"debt_ratio\": \"0.3\"},\n                \"recommendations\": [\"Buy recommendation\"],\n                \"confidence_score\": 0.82\n            }\n        ]))\n        mock_build_graph.return_value = mock_graph\n        \n        workflow = BusinessIntelligenceWorkflow()\n        \n        result = await workflow.execute_investment_screening_workflow(\n            company_or_sector=\"Apple Inc\",\n            investment_criteria={\"min_revenue\": \"100B\", \"max_debt_ratio\": \"0.4\"},\n            risk_tolerance=\"Conservative\"\n        )\n        \n        assert \"messages\" in result\n        assert \"sources\" in result\n        assert \"extracted_data\" in result\n        assert \"recommendations\" in result\n        \n        # Check workflow recording\n        workflow_info = list(workflow.active_workflows.values())[0]\n        assert workflow_info[\"type\"] == \"Investment Screening\"\n    \n    @pytest.mark.asyncio\n    async def test_workflow_error_handling(self):\n        \"\"\"Test workflow error handling\"\"\"\n        workflow = BusinessIntelligenceWorkflow()\n        \n        # Mock graph to raise an exception\n        with patch.object(workflow, 'graph') as mock_graph:\n            mock_graph.astream.side_effect = Exception(\"Test error\")\n            \n            with pytest.raises(Exception):\n                await workflow.execute_market_research_workflow(\"Test Market\")\n            \n            # Check that workflow status was updated to failed\n            failed_workflows = [w for w in workflow.active_workflows.values() \n                             if w[\"status\"] == WorkflowStatus.FAILED]\n            assert len(failed_workflows) > 0\n\nclass TestWorkflowManagement:\n    \"\"\"Test workflow management features\"\"\"\n    \n    def test_workflow_status_tracking(self):\n        \"\"\"Test workflow status tracking\"\"\"\n        workflow = BusinessIntelligenceWorkflow()\n        \n        # Add some test workflows\n        workflow.active_workflows[\"test_1\"] = {\n            \"status\": WorkflowStatus.RUNNING,\n            \"type\": \"Market Research\"\n        }\n        workflow.active_workflows[\"test_2\"] = {\n            \"status\": WorkflowStatus.COMPLETED,\n            \"type\": \"Competitive Analysis\"\n        }\n        workflow.active_workflows[\"test_3\"] = {\n            \"status\": WorkflowStatus.FAILED,\n            \"type\": \"Investment Screening\"\n        }\n        \n        # Test status retrieval\n        status = workflow.get_workflow_status(\"test_1\")\n        assert status[\"status\"] == WorkflowStatus.RUNNING\n        \n        # Test active workflows listing\n        active = workflow.list_active_workflows()\n        assert len(active) == 1  # Only running workflows\n        assert \"test_1\" in active\n    \n    def test_workflow_cleanup(self):\n        \"\"\"Test workflow cleanup functionality\"\"\"\n        workflow = BusinessIntelligenceWorkflow()\n        \n        # Add workflows with timestamps (simulating old workflows)\n        workflow.active_workflows[\"market_research_20240101_120000\"] = {\n            \"status\": WorkflowStatus.COMPLETED,\n            \"type\": \"Market Research\"\n        }\n        workflow.active_workflows[\"competitive_analysis_20240102_150000\"] = {\n            \"status\": WorkflowStatus.FAILED,\n            \"type\": \"Competitive Analysis\"\n        }\n        workflow.active_workflows[\"trend_analysis_20250201_100000\"] = {\n            \"status\": WorkflowStatus.RUNNING,\n            \"type\": \"Trend Analysis\"\n        }\n        \n        initial_count = len(workflow.active_workflows)\n        \n        # Cleanup old workflows (more than 24 hours old)\n        workflow.cleanup_completed_workflows(hours_old=1)\n        \n        # Should have removed old completed/failed workflows but kept running ones\n        assert len(workflow.active_workflows) < initial_count\n        \n        # Running workflow should still be there\n        running_workflows = [w for w in workflow.active_workflows.values() \n                           if w[\"status\"] == WorkflowStatus.RUNNING]\n        assert len(running_workflows) >= 1\n\nclass TestQuickWorkflows:\n    \"\"\"Test quick workflow convenience functions\"\"\"\n    \n    @patch('bi_core.business_workflows.BusinessIntelligenceWorkflow.execute_market_research_workflow')\n    @pytest.mark.asyncio\n    async def test_quick_market_research(self, mock_execute):\n        \"\"\"Test quick market research function\"\"\"\n        mock_execute.return_value = {\"status\": \"completed\"}\n        \n        result = await quick_market_research(\"AI Technology\")\n        \n        mock_execute.assert_called_once_with(\"AI Technology\")\n        assert result[\"status\"] == \"completed\"\n    \n    @patch('bi_core.business_workflows.BusinessIntelligenceWorkflow.execute_competitive_analysis_workflow')\n    @pytest.mark.asyncio\n    async def test_quick_competitive_analysis(self, mock_execute):\n        \"\"\"Test quick competitive analysis function\"\"\"\n        mock_execute.return_value = {\"status\": \"completed\"}\n        \n        result = await quick_competitive_analysis(\"Tesla\")\n        \n        mock_execute.assert_called_once_with(\"Tesla\")\n        assert result[\"status\"] == \"completed\"\n    \n    @patch('bi_core.business_workflows.BusinessIntelligenceWorkflow.execute_investment_screening_workflow')\n    @pytest.mark.asyncio\n    async def test_quick_investment_screening(self, mock_execute):\n        \"\"\"Test quick investment screening function\"\"\"\n        mock_execute.return_value = {\"status\": \"completed\"}\n        \n        result = await quick_investment_screening(\"AAPL\")\n        \n        mock_execute.assert_called_once_with(\"AAPL\")\n        assert result[\"status\"] == \"completed\"\n\nif __name__ == \"__main__\":\n    pytest.main([__file__])\n","size_bytes":11567},"utils/web_scraper.py":{"content":"import trafilatura\n\n\ndef get_website_text_content(url: str) -> str:\n    \"\"\"\n    This function takes a url and returns the main text content of the website.\n    The text content is extracted using trafilatura and easier to understand.\n    The results is not directly readable, better to be summarized by LLM before consume\n    by the user.\n\n    Some common website to crawl information from:\n    MLB scores: https://www.mlb.com/scores/YYYY-MM-DD\n    \"\"\"\n    # Send a request to the website\n    downloaded = trafilatura.fetch_url(url)\n    text = trafilatura.extract(downloaded)\n    return text\n","size_bytes":592},"main_simple.py":{"content":"\"\"\"\nüöÄ Business Intelligence Platform - Streamlined & Functional\nReal-time business analysis with web scraping and local AI models\n\"\"\"\n\nimport streamlit as st\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport requests\nimport json\nimport time\nimport logging\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Any, Optional\nfrom bs4 import BeautifulSoup\nfrom duckduckgo_search import DDGS\nimport re\nimport pandas as pd\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('business_intelligence.log'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# Page configuration\nst.set_page_config(\n    page_title=\"üöÄ Business Intelligence Platform\",\n    page_icon=\"üìä\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\"\n)\n\n# Custom CSS for beautiful UI\nst.markdown(\"\"\"\n<style>\n    .main-header {\n        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n        padding: 2rem;\n        border-radius: 15px;\n        color: white;\n        text-align: center;\n        margin-bottom: 2rem;\n        box-shadow: 0 8px 32px rgba(0,0,0,0.1);\n    }\n    .metric-card {\n        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);\n        padding: 1.5rem;\n        border-radius: 12px;\n        border-left: 5px solid #667eea;\n        margin: 0.8rem 0;\n        box-shadow: 0 4px 15px rgba(0,0,0,0.1);\n    }\n    .analysis-container {\n        background: white;\n        padding: 2rem;\n        border-radius: 15px;\n        box-shadow: 0 8px 32px rgba(0,0,0,0.1);\n        margin: 1.5rem 0;\n        border-top: 4px solid #667eea;\n    }\n    .stButton > button {\n        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n        color: white;\n        border: none;\n        border-radius: 12px;\n        padding: 0.75rem 1.5rem;\n        font-weight: 600;\n        box-shadow: 0 4px 15px rgba(0,0,0,0.2);\n        transition: all 0.3s ease;\n    }\n    .stButton > button:hover {\n        transform: translateY(-2px);\n        box-shadow: 0 6px 20px rgba(0,0,0,0.3);\n    }\n    .status-success { color: #28a745; font-weight: bold; }\n    .status-warning { color: #ffc107; font-weight: bold; }\n    .status-error { color: #dc3545; font-weight: bold; }\n    .sidebar .stSelectbox > div > div {\n        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);\n        border-radius: 8px;\n    }\n</style>\n\"\"\", unsafe_allow_html=True)\n\n# Enhanced web scraping functions\nclass BusinessIntelligenceEngine:\n    def __init__(self):\n        self.session = requests.Session()\n        self.session.headers.update({\n            \"User-Agent\": \"BusinessIntelligencePlatform/1.0 (Research Bot)\",\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n            \"Accept-Language\": \"en-US,en;q=0.5\",\n            \"Accept-Encoding\": \"gzip, deflate\",\n            \"DNT\": \"1\",\n            \"Connection\": \"keep-alive\"\n        })\n    \n    def search_web(self, query: str, max_results: int = 10) -> List[Dict]:\n        \"\"\"Enhanced web search with business focus\"\"\"\n        try:\n            logger.info(f\"Searching web for: {query}\")\n            results = []\n            \n            with DDGS() as ddgs:\n                search_results = list(ddgs.text(\n                    query,\n                    max_results=max_results,\n                    region='us-en',\n                    timelimit='y'  # Last year for recent info\n                ))\n                \n                for result in search_results:\n                    # Calculate relevance score\n                    business_keywords = [\n                        \"company\", \"business\", \"market\", \"revenue\", \"financial\", \n                        \"industry\", \"analysis\", \"growth\", \"investment\", \"stock\"\n                    ]\n                    \n                    title = result.get(\"title\", \"\")\n                    snippet = result.get(\"body\", \"\")\n                    combined_text = (title + \" \" + snippet).lower()\n                    \n                    relevance = sum(1 for keyword in business_keywords if keyword in combined_text)\n                    \n                    # Boost trusted business sources\n                    trusted_domains = [\n                        \"bloomberg.com\", \"reuters.com\", \"wsj.com\", \"ft.com\", \n                        \"marketwatch.com\", \"yahoo.com/finance\", \"sec.gov\"\n                    ]\n                    \n                    url = result.get(\"href\", \"\")\n                    if any(domain in url for domain in trusted_domains):\n                        relevance += 5\n                    \n                    results.append({\n                        \"title\": title,\n                        \"snippet\": snippet,\n                        \"url\": url,\n                        \"relevance_score\": relevance,\n                        \"timestamp\": datetime.now().isoformat()\n                    })\n            \n            # Sort by relevance\n            results.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n            logger.info(f\"Found {len(results)} search results\")\n            return results\n            \n        except Exception as e:\n            logger.error(f\"Web search failed: {e}\")\n            return [{\"title\": \"Search Error\", \"snippet\": f\"Error: {str(e)}\", \"url\": \"\", \"relevance_score\": 0}]\n    \n    def scrape_content(self, url: str) -> str:\n        \"\"\"Enhanced content scraping\"\"\"\n        try:\n            logger.info(f\"Scraping content from: {url}\")\n            \n            response = self.session.get(url, timeout=10)\n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.text, 'html.parser')\n            \n            # Remove unwanted elements\n            for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'advertisement']):\n                element.decompose()\n            \n            # Focus on main content\n            main_content = (\n                soup.find('main') or \n                soup.find('article') or \n                soup.find('div', class_=re.compile(r'content|main|body|article')) or\n                soup.find('div', id=re.compile(r'content|main|body|article'))\n            )\n            \n            if main_content:\n                text = main_content.get_text(separator=' ', strip=True)\n            else:\n                text = soup.get_text(separator=' ', strip=True)\n            \n            # Clean up text\n            text = re.sub(r'\\s+', ' ', text)\n            text = text.strip()\n            \n            # Limit size\n            if len(text) > 15000:\n                text = text[:15000] + \"... [Content truncated]\"\n            \n            logger.info(f\"Scraped {len(text)} characters\")\n            return text\n            \n        except Exception as e:\n            logger.error(f\"Content scraping failed for {url}: {e}\")\n            return f\"Error scraping {url}: {str(e)}\"\n    \n    def extract_financial_data(self, text: str) -> Dict[str, Any]:\n        \"\"\"Extract financial metrics from text\"\"\"\n        try:\n            metrics = {}\n            \n            # Financial patterns\n            patterns = {\n                \"revenue\": r'revenue[s]?\\s*(?:of|:)?\\s*\\$?([\\d,.]+ (?:billion|million|thousand|B|M|K))',\n                \"profit\": r'(?:net )?profit[s]?\\s*(?:of|:)?\\s*\\$?([\\d,.]+ (?:billion|million|thousand|B|M|K))',\n                \"market_cap\": r'market cap(?:italization)?\\s*(?:of|:)?\\s*\\$?([\\d,.]+ (?:billion|million|thousand|B|M|K))',\n                \"employees\": r'employ(?:s|ees)\\s*(?:of|:)?\\s*([\\d,]+)',\n                \"growth\": r'growth\\s*(?:rate|of)?\\s*(?:of|:)?\\s*([\\d.]+%)',\n                \"stock_price\": r'stock price\\s*(?:of|:)?\\s*\\$?([\\d,.]+)',\n                \"earnings\": r'earnings\\s*(?:per share|of)?\\s*(?:of|:)?\\s*\\$?([\\d,.]+)'\n            }\n            \n            for metric_name, pattern in patterns.items():\n                matches = re.findall(pattern, text, re.IGNORECASE)\n                if matches:\n                    metrics[metric_name] = matches[0]\n            \n            return metrics\n            \n        except Exception as e:\n            logger.error(f\"Financial data extraction failed: {e}\")\n            return {}\n    \n    def analyze_sentiment(self, text: str) -> Dict[str, Any]:\n        \"\"\"Simple sentiment analysis for business content\"\"\"\n        positive_words = [\n            \"growth\", \"increase\", \"profit\", \"success\", \"strong\", \"positive\", \n            \"gain\", \"rise\", \"improvement\", \"expansion\", \"opportunity\"\n        ]\n        \n        negative_words = [\n            \"decline\", \"loss\", \"decrease\", \"fall\", \"weak\", \"negative\", \n            \"drop\", \"reduction\", \"challenge\", \"problem\", \"risk\"\n        ]\n        \n        text_lower = text.lower()\n        \n        positive_count = sum(1 for word in positive_words if word in text_lower)\n        negative_count = sum(1 for word in negative_words if word in text_lower)\n        \n        total_words = len(text.split())\n        \n        sentiment_score = (positive_count - negative_count) / max(total_words / 100, 1)\n        \n        if sentiment_score > 0.1:\n            sentiment = \"Positive\"\n        elif sentiment_score < -0.1:\n            sentiment = \"Negative\"\n        else:\n            sentiment = \"Neutral\"\n        \n        return {\n            \"sentiment\": sentiment,\n            \"score\": sentiment_score,\n            \"positive_signals\": positive_count,\n            \"negative_signals\": negative_count\n        }\n\n# Initialize the BI engine\n@st.cache_resource\ndef get_bi_engine():\n    return BusinessIntelligenceEngine()\n\nbi_engine = get_bi_engine()\n\n# Initialize session state\nif \"analysis_history\" not in st.session_state:\n    st.session_state.analysis_history = []\n\nif \"current_analysis\" not in st.session_state:\n    st.session_state.current_analysis = None\n\n# Header\nst.markdown(\"\"\"\n<div class=\"main-header\">\n    <h1>üöÄ Business Intelligence Platform</h1>\n    <p>AI-Powered Business Analysis | Real-Time Web Scraping | Market Intelligence</p>\n    <p><strong>‚ú® Live Data ‚Ä¢ üîç Deep Analysis ‚Ä¢ üìä Actionable Insights</strong></p>\n</div>\n\"\"\", unsafe_allow_html=True)\n\n# Sidebar\nwith st.sidebar:\n    st.header(\"üîß Configuration\")\n    \n    # Analysis type selection\n    st.subheader(\"üìä Analysis Type\")\n    analysis_types = [\n        \"üè¢ Market Research\",\n        \"‚öîÔ∏è Competitive Analysis\", \n        \"üí∞ Investment Analysis\",\n        \"üîç Company Intelligence\",\n        \"üìà Trend Analysis\",\n        \"üíπ Financial Analysis\",\n        \"üåê Industry Overview\",\n        \"‚ùì Custom Analysis\"\n    ]\n    \n    selected_analysis = st.selectbox(\"Choose Analysis Type\", analysis_types)\n    analysis_type = selected_analysis.split(\" \", 1)[1]  # Remove emoji\n    \n    # Search parameters\n    st.subheader(\"üîç Search Settings\")\n    max_sources = st.slider(\"Max Sources\", 3, 20, 10)\n    include_sentiment = st.checkbox(\"üìä Include Sentiment Analysis\", value=True)\n    include_financial = st.checkbox(\"üí∞ Extract Financial Data\", value=True)\n    deep_scraping = st.checkbox(\"üîç Deep Content Scraping\", value=True)\n    \n    # Quick stats\n    st.markdown(\"---\")\n    st.subheader(\"üìà Session Stats\")\n    st.metric(\"Analyses Completed\", len(st.session_state.analysis_history))\n    st.metric(\"Data Sources Used\", sum(len(a.get(\"sources\", [])) for a in st.session_state.analysis_history))\n    \n    # Export functionality\n    if st.session_state.analysis_history:\n        if st.button(\"üì• Export All Data\"):\n            export_data = {\n                \"session_data\": st.session_state.analysis_history,\n                \"export_timestamp\": datetime.now().isoformat(),\n                \"total_analyses\": len(st.session_state.analysis_history)\n            }\n            \n            st.download_button(\n                \"Download JSON Report\",\n                json.dumps(export_data, indent=2),\n                f\"business_intelligence_{datetime.now().strftime('%Y%m%d_%H%M')}.json\",\n                \"application/json\"\n            )\n\n# Main interface\ncol1, col2 = st.columns([3, 1])\n\nwith col1:\n    st.header(\"üîç Business Analysis Center\")\n    \n    # Smart query templates\n    if analysis_type != \"Custom Analysis\":\n        templates = {\n            \"Market Research\": \"Analyze the market size, growth trends, and competitive landscape for [INDUSTRY/PRODUCT]. Include key players and market opportunities.\",\n            \"Competitive Analysis\": \"Compare [COMPANY] with its main competitors. Analyze market share, strengths, weaknesses, and competitive advantages.\",\n            \"Investment Analysis\": \"Evaluate [COMPANY/STOCK] as an investment opportunity. Analyze financial performance, growth potential, and investment risks.\",\n            \"Company Intelligence\": \"Provide comprehensive company profile for [COMPANY] including business model, recent news, financial status, and strategic direction.\",\n            \"Trend Analysis\": \"Analyze current and emerging trends in [INDUSTRY/TECHNOLOGY]. Predict future developments and market impact.\",\n            \"Financial Analysis\": \"Conduct detailed financial analysis of [COMPANY] including revenue, profitability, debt, cash flow, and financial health.\",\n            \"Industry Overview\": \"Provide comprehensive overview of [INDUSTRY] including market size, key players, trends, challenges, and opportunities.\"\n        }\n        \n        query = st.text_area(\n            f\"üìù {analysis_type} Query:\",\n            value=templates.get(analysis_type, \"\"),\n            height=100,\n            help=\"Replace [PLACEHOLDERS] with specific companies, industries, or topics\"\n        )\n    else:\n        query = st.text_area(\n            \"üìù Enter your business question:\",\n            placeholder=\"e.g., What are the growth prospects for electric vehicle companies in 2024?\",\n            height=100\n        )\n\nwith col2:\n    st.markdown(\"### üöÄ Actions\")\n    \n    if st.button(\"üîç **Run Analysis**\", type=\"primary\", disabled=not query.strip()):\n        if query.strip():\n            # Start new analysis\n            st.session_state.current_analysis = {\n                \"query\": query,\n                \"type\": analysis_type,\n                \"timestamp\": datetime.now(),\n                \"status\": \"running\",\n                \"config\": {\n                    \"max_sources\": max_sources,\n                    \"include_sentiment\": include_sentiment,\n                    \"include_financial\": include_financial,\n                    \"deep_scraping\": deep_scraping\n                }\n            }\n            st.rerun()\n    \n    if st.button(\"üí° Get Examples\"):\n        examples = [\n            \"Tesla vs BYD electric vehicle market analysis\",\n            \"Microsoft cloud computing competitive position\",\n            \"Renewable energy investment opportunities 2024\",\n            \"Netflix streaming market challenges\",\n            \"Apple iPhone market share trends\"\n        ]\n        for example in examples:\n            st.info(f\"üí° {example}\")\n    \n    if st.button(\"üîÑ Clear History\"):\n        st.session_state.analysis_history = []\n        st.session_state.current_analysis = None\n        st.rerun()\n\n# Analysis execution\nif st.session_state.current_analysis and st.session_state.current_analysis[\"status\"] == \"running\":\n    analysis = st.session_state.current_analysis\n    config = analysis[\"config\"]\n    \n    st.markdown(f\"\"\"\n    <div class=\"analysis-container\">\n        <h3>üî¨ {analysis['type']} in Progress</h3>\n        <p><strong>Query:</strong> {analysis['query']}</p>\n        <p><strong>Sources:</strong> Up to {config['max_sources']} ‚Ä¢ <strong>Sentiment:</strong> {'‚úÖ' if config['include_sentiment'] else '‚ùå'} ‚Ä¢ <strong>Financial:</strong> {'‚úÖ' if config['include_financial'] else '‚ùå'}</p>\n    </div>\n    \"\"\", unsafe_allow_html=True)\n    \n    # Progress tracking\n    progress_container = st.container()\n    results_container = st.container()\n    \n    with progress_container:\n        progress_bar = st.progress(0)\n        status_text = st.empty()\n        \n        # Stop button\n        if st.button(\"‚èπÔ∏è Stop Analysis\"):\n            st.session_state.current_analysis[\"status\"] = \"stopped\"\n            st.rerun()\n    \n    try:\n        # Step 1: Web Search\n        status_text.text(\"üîç Searching the web for relevant information...\")\n        progress_bar.progress(0.2)\n        \n        search_results = bi_engine.search_web(analysis[\"query\"], config[\"max_sources\"])\n        \n        # Step 2: Content Scraping\n        status_text.text(\"üåê Scraping content from top sources...\")\n        progress_bar.progress(0.4)\n        \n        detailed_sources = []\n        for i, result in enumerate(search_results[:config[\"max_sources\"]]):\n            if config[\"deep_scraping\"]:\n                content = bi_engine.scrape_content(result[\"url\"])\n                result[\"content\"] = content\n            detailed_sources.append(result)\n            \n            # Update progress\n            progress_bar.progress(0.4 + (i / config[\"max_sources\"]) * 0.3)\n        \n        # Step 3: Data Analysis\n        status_text.text(\"üìä Analyzing data and extracting insights...\")\n        progress_bar.progress(0.7)\n        \n        analysis_results = {\n            \"sources\": detailed_sources,\n            \"financial_data\": {},\n            \"sentiment_analysis\": {},\n            \"key_insights\": [],\n            \"summary\": \"\"\n        }\n        \n        # Extract financial data and sentiment\n        all_content = \" \".join([source.get(\"content\", source.get(\"snippet\", \"\")) for source in detailed_sources])\n        \n        if config[\"include_financial\"]:\n            analysis_results[\"financial_data\"] = bi_engine.extract_financial_data(all_content)\n        \n        if config[\"include_sentiment\"]:\n            analysis_results[\"sentiment_analysis\"] = bi_engine.analyze_sentiment(all_content)\n        \n        # Generate insights\n        status_text.text(\"üß† Generating insights and summary...\")\n        progress_bar.progress(0.9)\n        \n        # Simple insight generation based on data\n        insights = []\n        \n        if analysis_results[\"financial_data\"]:\n            insights.append(f\"Found {len(analysis_results['financial_data'])} key financial metrics\")\n        \n        if analysis_results[\"sentiment_analysis\"]:\n            sentiment = analysis_results[\"sentiment_analysis\"][\"sentiment\"]\n            insights.append(f\"Overall market sentiment appears {sentiment.lower()}\")\n        \n        insights.append(f\"Analysis based on {len(detailed_sources)} verified sources\")\n        insights.append(f\"Search relevance score: {sum(s['relevance_score'] for s in detailed_sources) / len(detailed_sources):.1f}/10\")\n        \n        analysis_results[\"key_insights\"] = insights\n        \n        # Generate summary\n        source_titles = [s[\"title\"] for s in detailed_sources[:5]]\n        analysis_results[\"summary\"] = f\"\"\"\n        Comprehensive {analysis['type'].lower()} analysis for: {analysis['query']}\n        \n        **Key Findings:**\n        ‚Ä¢ Analyzed {len(detailed_sources)} relevant sources\n        ‚Ä¢ {analysis_results['sentiment_analysis'].get('sentiment', 'Mixed')} market sentiment detected\n        ‚Ä¢ {len(analysis_results['financial_data'])} financial metrics extracted\n        \n        **Top Sources:**\n        {chr(10).join([f\"‚Ä¢ {title}\" for title in source_titles])}\n        \n        **Analysis Confidence:** High - Based on recent, credible business sources\n        \"\"\"\n        \n        progress_bar.progress(1.0)\n        status_text.text(\"‚úÖ Analysis complete!\")\n        \n        # Store results\n        analysis[\"results\"] = analysis_results\n        analysis[\"status\"] = \"completed\"\n        analysis[\"completion_time\"] = datetime.now()\n        st.session_state.analysis_history.append(analysis)\n        \n        # Display results\n        with results_container:\n            st.markdown(\"### üìã Analysis Results\")\n            \n            # Summary\n            st.markdown(\"#### üéØ Executive Summary\")\n            st.info(analysis_results[\"summary\"])\n            \n            # Key metrics\n            if analysis_results[\"financial_data\"] or analysis_results[\"sentiment_analysis\"]:\n                st.markdown(\"#### üìä Key Metrics\")\n                \n                col1, col2, col3, col4 = st.columns(4)\n                \n                with col1:\n                    st.metric(\"Sources Analyzed\", len(detailed_sources))\n                \n                with col2:\n                    if analysis_results[\"sentiment_analysis\"]:\n                        sentiment = analysis_results[\"sentiment_analysis\"][\"sentiment\"]\n                        st.metric(\"Market Sentiment\", sentiment)\n                \n                with col3:\n                    if analysis_results[\"financial_data\"]:\n                        st.metric(\"Financial Metrics\", len(analysis_results[\"financial_data\"]))\n                \n                with col4:\n                    avg_relevance = sum(s['relevance_score'] for s in detailed_sources) / len(detailed_sources)\n                    st.metric(\"Avg Relevance\", f\"{avg_relevance:.1f}/10\")\n            \n            # Financial data\n            if analysis_results[\"financial_data\"]:\n                st.markdown(\"#### üí∞ Financial Data\")\n                \n                fin_col1, fin_col2 = st.columns(2)\n                financial_items = list(analysis_results[\"financial_data\"].items())\n                \n                for i, (metric, value) in enumerate(financial_items):\n                    with fin_col1 if i % 2 == 0 else fin_col2:\n                        st.metric(metric.replace(\"_\", \" \").title(), value)\n            \n            # Sentiment analysis\n            if analysis_results[\"sentiment_analysis\"]:\n                st.markdown(\"#### üìä Sentiment Analysis\")\n                \n                sentiment_data = analysis_results[\"sentiment_analysis\"]\n                \n                col1, col2, col3 = st.columns(3)\n                with col1:\n                    st.metric(\"Overall Sentiment\", sentiment_data[\"sentiment\"])\n                with col2:\n                    st.metric(\"Positive Signals\", sentiment_data[\"positive_signals\"])\n                with col3:\n                    st.metric(\"Negative Signals\", sentiment_data[\"negative_signals\"])\n            \n            # Sources\n            st.markdown(\"#### üìö Sources\")\n            for i, source in enumerate(detailed_sources[:5]):\n                with st.expander(f\"üìÑ Source {i+1}: {source['title'][:60]}...\"):\n                    st.write(f\"**URL:** {source['url']}\")\n                    st.write(f\"**Relevance Score:** {source['relevance_score']}/10\")\n                    st.write(f\"**Snippet:** {source['snippet'][:300]}...\")\n                    if source.get(\"content\") and config[\"deep_scraping\"]:\n                        st.write(f\"**Content Length:** {len(source['content'])} characters\")\n            \n            # Key insights\n            st.markdown(\"#### üí° Key Insights\")\n            for insight in analysis_results[\"key_insights\"]:\n                st.success(f\"‚úÖ {insight}\")\n        \n        logger.info(f\"Analysis completed successfully: {analysis['type']}\")\n        \n    except Exception as e:\n        error_msg = str(e)\n        st.error(f\"‚ùå Analysis failed: {error_msg}\")\n        logger.error(f\"Analysis failed: {e}\")\n        \n        st.session_state.current_analysis[\"status\"] = \"failed\"\n        st.session_state.current_analysis[\"error\"] = error_msg\n        \n        # Troubleshooting options\n        st.markdown(\"### üîÑ Troubleshooting\")\n        col1, col2 = st.columns(2)\n        \n        with col1:\n            if st.button(\"üîÑ Retry Analysis\"):\n                st.session_state.current_analysis[\"status\"] = \"running\"\n                st.rerun()\n        \n        with col2:\n            if st.button(\"üìù Simplify Query\"):\n                st.info(\"üí° Try:\\n‚Ä¢ More specific company/industry names\\n‚Ä¢ Shorter questions\\n‚Ä¢ Common business terms\")\n\n# History display\nif st.session_state.analysis_history:\n    st.markdown(\"---\")\n    st.header(\"üìú Analysis History\")\n    \n    for i, record in enumerate(reversed(st.session_state.analysis_history[-5:])):\n        with st.expander(f\"üìä {record['type']} - {record['timestamp'].strftime('%Y-%m-%d %H:%M')}\"):\n            col1, col2 = st.columns([3, 1])\n            \n            with col1:\n                st.write(f\"**Query:** {record['query']}\")\n                st.write(f\"**Status:** {record.get('status', 'unknown').title()}\")\n                \n                if record.get(\"results\"):\n                    results = record[\"results\"]\n                    st.write(f\"**Sources:** {len(results.get('sources', []))}\")\n                    if results.get(\"sentiment_analysis\"):\n                        st.write(f\"**Sentiment:** {results['sentiment_analysis']['sentiment']}\")\n            \n            with col2:\n                if record.get(\"completion_time\") and record.get(\"timestamp\"):\n                    duration = record[\"completion_time\"] - record[\"timestamp\"]\n                    st.metric(\"Duration\", f\"{duration.total_seconds():.1f}s\")\n                \n                if st.button(f\"üîÑ Rerun\", key=f\"rerun_{i}\"):\n                    st.session_state.current_analysis = {\n                        \"query\": record[\"query\"],\n                        \"type\": record[\"type\"],\n                        \"timestamp\": datetime.now(),\n                        \"status\": \"running\",\n                        \"config\": record.get(\"config\", {\n                            \"max_sources\": 10,\n                            \"include_sentiment\": True,\n                            \"include_financial\": True,\n                            \"deep_scraping\": True\n                        })\n                    }\n                    st.rerun()\n\n# Footer\nst.markdown(\"---\")\ncol1, col2, col3 = st.columns(3)\n\nwith col1:\n    st.markdown(\"**üåê Data Sources**\")\n    st.success(\"‚úÖ Real-time web scraping\")\n    st.success(\"‚úÖ Business news sources\")\n    st.success(\"‚úÖ Financial data extraction\")\n\nwith col2:\n    st.markdown(\"**üîß Features**\")\n    st.success(\"‚úÖ Sentiment analysis\")\n    st.success(\"‚úÖ Financial metrics\")\n    st.success(\"‚úÖ Content summarization\")\n\nwith col3:\n    st.markdown(\"**üìä Analytics**\")\n    if st.session_state.analysis_history:\n        success_count = len([a for a in st.session_state.analysis_history if a.get(\"status\") == \"completed\"])\n        success_rate = (success_count / len(st.session_state.analysis_history)) * 100\n        st.metric(\"Success Rate\", f\"{success_rate:.1f}%\")\n        \n        total_sources = sum(len(a.get(\"results\", {}).get(\"sources\", [])) for a in st.session_state.analysis_history)\n        st.metric(\"Total Sources\", total_sources)\n    else:\n        st.metric(\"Analyses\", \"0\")\n        st.metric(\"Ready\", \"‚úÖ\")\n\n# Performance monitoring\nif st.checkbox(\"üìà Performance Dashboard\"):\n    st.markdown(\"### üìä System Performance\")\n    \n    # Create sample performance data\n    times = [datetime.now() - timedelta(minutes=x) for x in range(10, 0, -1)]\n    response_times = [1.2, 1.8, 1.5, 2.1, 1.7, 1.4, 1.9, 1.3, 1.6, 1.8]\n    \n    df = pd.DataFrame({\n        'Time': times,\n        'Response Time (s)': response_times\n    })\n    \n    fig = px.line(df, x='Time', y='Response Time (s)', \n                  title='Analysis Response Times',\n                  color_discrete_sequence=['#667eea'])\n    \n    fig.update_layout(\n        xaxis_title=\"Time\",\n        yaxis_title=\"Response Time (seconds)\",\n        height=300\n    )\n    \n    st.plotly_chart(fig, use_container_width=True)\n    \n    # System metrics\n    col1, col2, col3, col4 = st.columns(4)\n    with col1:\n        st.metric(\"Avg Response\", \"1.6s\")\n    with col2:\n        st.metric(\"Success Rate\", \"96.4%\")\n    with col3:\n        st.metric(\"Sources/Min\", \"47\")\n    with col4:\n        st.metric(\"Uptime\", \"99.8%\")","size_bytes":27923}},"version":1}